{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  male    age   debt married bankcustomer educationlevel ethnicity  \\\n",
      "0    b  30.83  0.000       u            g              w         v   \n",
      "1    a  58.67  4.460       u            g              q         h   \n",
      "2    a  24.50  0.500       u            g              q         h   \n",
      "3    b  27.83  1.540       u            g              w         v   \n",
      "4    b  20.17  5.625       u            g              w         v   \n",
      "\n",
      "   yearsemployed priordefault employed  creditscore driverslicense citizen  \\\n",
      "0           1.25            t        t            1              f       g   \n",
      "1           3.04            t        t            6              f       g   \n",
      "2           1.50            t        f            0              f       g   \n",
      "3           3.75            t        t            5              t       g   \n",
      "4           1.71            t        f            0              f       s   \n",
      "\n",
      "     zip  income approved  \n",
      "0  00202       0        +  \n",
      "1  00043     560        +  \n",
      "2  00280     824        +  \n",
      "3  00100       3        +  \n",
      "4  00120       0        +  \n",
      "678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age --> yearsemployed',\n",
       " 'priordefault --> age',\n",
       " 'creditscore --> debt',\n",
       " 'ethnicity --> age',\n",
       " 'yearsemployed --> creditscore',\n",
       " 'zip --> debt',\n",
       " 'married --> approved',\n",
       " 'yearsemployed --> debt',\n",
       " 'employed --> creditscore',\n",
       " 'bankcustomer --> approved',\n",
       " 'employed --> approved',\n",
       " 'priordefault --> yearsemployed',\n",
       " 'bankcustomer --> employed',\n",
       " 'bankcustomer --- married',\n",
       " 'priordefault --> approved',\n",
       " 'yearsemployed --> driverslicense',\n",
       " 'citizen --> employed',\n",
       " 'creditscore --> zip',\n",
       " 'employed --> priordefault']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "column_names = ['male', 'age', 'debt', 'married', 'bankcustomer', 'educationlevel', 'ethnicity', 'yearsemployed',\n",
    "               'priordefault', 'employed', 'creditscore', 'driverslicense', 'citizen', 'zip', 'income', 'approved']\n",
    "\n",
    "data = pd.read_csv('data/crx.data', header=None,  names=column_names)\n",
    "data.reset_index(drop=True, inplace=True) \n",
    "\n",
    "\n",
    "data = data.dropna(how = 'all')\n",
    "data = data[data.age != '?']\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "from sklearn import preprocessing\n",
    "for feat in ['male', 'married','bankcustomer', 'educationlevel', 'ethnicity','priordefault', 'employed', 'driverslicense', 'citizen', 'zip', 'approved']:\n",
    "    data[feat] = preprocessing.LabelEncoder().fit_transform(data[feat])\n",
    "\n",
    "\n",
    "#####################################################\n",
    "#### For this experiment, we uniquely drop the default variable (prior default)\n",
    "###################################################\n",
    "#data = data.drop(['educationlevel'], axis=1)\n",
    "    \n",
    "print(len(data))\n",
    "\n",
    "from pycausal.pycausal import pycausal as pc\n",
    "pc = pc()\n",
    "pc.start_vm()\n",
    "\n",
    "from pycausal import prior as p\n",
    "prior = p.knowledge(addtemporal = [['male', 'age','ethnicity'],[ 'debt', 'married', 'bankcustomer', 'educationlevel', 'yearsemployed',\n",
    "                'employed', 'creditscore', 'driverslicense', 'citizen', 'zip', 'income'],['approved']])\n",
    "\n",
    "\n",
    "from pycausal import search as s\n",
    "tetrad = s.tetradrunner()\n",
    "tetrad.run(algoId = 'fges', scoreId = 'cg-bic-score', dfs = data, priorKnowledge = prior,\n",
    "           maxDegree = -1, faithfulnessAssumed = True, verbose = False)\n",
    "tetrad.getEdges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 7], [8, 1], [10, 2], [6, 1], [7, 10], [13, 2], [3, 15], [7, 2], [9, 10], [4, 15], [9, 15], [8, 7], [4, 9], [4, 3], [8, 15], [7, 11], [12, 9], [10, 13], [9, 8]]\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "for edge in tetrad.getEdges():\n",
    "    edges.append(list([column_names.index(edge.split(' ')[0]), column_names.index(edge.split(' ')[-1])]))\n",
    "print(edges )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the above edge list \n",
    "column_names.index('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>debt</th>\n",
       "      <th>married</th>\n",
       "      <th>bankcustomer</th>\n",
       "      <th>educationlevel</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>yearsemployed</th>\n",
       "      <th>priordefault</th>\n",
       "      <th>employed</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>driverslicense</th>\n",
       "      <th>citizen</th>\n",
       "      <th>zip</th>\n",
       "      <th>income</th>\n",
       "      <th>approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male    age   debt  married  bankcustomer  educationlevel  ethnicity  \\\n",
       "0     2  30.83  0.000        2             1              13          8   \n",
       "1     1  58.67  4.460        2             1              11          4   \n",
       "2     1  24.50  0.500        2             1              11          4   \n",
       "3     2  27.83  1.540        2             1              13          8   \n",
       "4     2  20.17  5.625        2             1              13          8   \n",
       "\n",
       "   yearsemployed  priordefault  employed  creditscore  driverslicense  \\\n",
       "0           1.25             1         1            1               0   \n",
       "1           3.04             1         1            6               0   \n",
       "2           1.50             1         0            0               0   \n",
       "3           3.75             1         1            5               1   \n",
       "4           1.71             1         0            0               0   \n",
       "\n",
       "   citizen  zip  income  approved  \n",
       "0        0   68       0         0  \n",
       "1        0   11     560         0  \n",
       "2        0   96     824         0  \n",
       "3        0   31       3         0  \n",
       "4        2   37       0         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  male    age   debt married bankcustomer educationlevel ethnicity  \\\n",
      "0    b  30.83  0.000       u            g              w         v   \n",
      "1    a  58.67  4.460       u            g              q         h   \n",
      "2    a  24.50  0.500       u            g              q         h   \n",
      "3    b  27.83  1.540       u            g              w         v   \n",
      "4    b  20.17  5.625       u            g              w         v   \n",
      "\n",
      "   yearsemployed priordefault employed  creditscore driverslicense citizen  \\\n",
      "0           1.25            t        t            1              f       g   \n",
      "1           3.04            t        t            6              f       g   \n",
      "2           1.50            t        f            0              f       g   \n",
      "3           3.75            t        t            5              t       g   \n",
      "4           1.71            t        f            0              f       s   \n",
      "\n",
      "     zip  income approved  \n",
      "0  00202       0        +  \n",
      "1  00043     560        +  \n",
      "2  00280     824        +  \n",
      "3  00100       3        +  \n",
      "4  00120       0        +  \n",
      "690\n",
      "678\n",
      "0.0    539\n",
      "1.0    139\n",
      "Name: approved, dtype: int64 1.0    413\n",
      "0.0    265\n",
      "Name: ethnicity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from models.DECAF_credit import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "            \n",
    "column_names = ['male', 'age', 'debt', 'married', 'bankcustomer', 'educationlevel', 'ethnicity', 'yearsemployed',\n",
    "               'priordefault', 'employed', 'creditscore', 'driverslicense', 'citizen', 'zip', 'income', 'approved']\n",
    "\n",
    "data = pd.read_csv('data/crx.data', header=None,  names=column_names)\n",
    "print(data.head(5))\n",
    "data.reset_index(drop=True, inplace=True) \n",
    "print(len(data))\n",
    "\n",
    "data = data.dropna(how = 'all')\n",
    "\n",
    "data = data[data.age != '?']\n",
    "data.reset_index(drop=True, inplace = True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "from sklearn import preprocessing\n",
    "for feat in ['male', 'married','bankcustomer', 'educationlevel', 'ethnicity','priordefault', 'employed', 'driverslicense', 'citizen', 'zip', 'approved']:\n",
    "    data[feat] = preprocessing.LabelEncoder().fit_transform(data[feat])\n",
    "\n",
    "data['age'] = pd.to_numeric(data['age'],errors='coerce')\n",
    "\n",
    "#####################################################\n",
    "#### For this experiment, we uniquely drop the default variable (prior default)\n",
    "###################################################\n",
    "#data = data.drop(['priordefault'], axis=1)\n",
    "\n",
    "# binarize the protected variable\n",
    "data.loc[data['ethnicity'] <= 4, 'ethnicity'] = 0\n",
    "data.loc[data['ethnicity'] > 4, 'ethnicity']= 1\n",
    "data.loc[data['ethnicity'] == 1 , 'employed'] =  1\n",
    "\n",
    "biased_data = data.copy()\n",
    "biased_data.loc[biased_data['ethnicity'] == 1, 'approved'] = 0\n",
    "\n",
    "thresh = 0.8\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data[data.columns] = scaler.fit_transform(data)\n",
    "biased_data[biased_data.columns] = scaler.transform(biased_data)\n",
    "print(biased_data.approved.value_counts(), biased_data.ethnicity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairGAN_code.fairGAN import Medgan\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# from DECAF.data import DataModule\n",
    "import pickle\n",
    "\n",
    "\n",
    "def train_fairgan(datapath):\n",
    "\n",
    "    #data = np.load(datapath, allow_pickle = True)\n",
    "    inputDim = data.shape[1]-1\n",
    "    inputNum = data.shape[0]\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    mg = Medgan(dataType='count',\n",
    "                inputDim=inputDim,\n",
    "                embeddingDim=128,\n",
    "                randomDim=128,\n",
    "                generatorDims=(128,128),\n",
    "                discriminatorDims=(256,128),\n",
    "                compressDims=(),\n",
    "                decompressDims=(),\n",
    "                bnDecay=0.99,\n",
    "                l2scale=0.001)\n",
    "\n",
    "    model_file = ''\n",
    "    out_file = 'fair'\n",
    "    batch_size = 32\n",
    "    \n",
    "    mg.train(dataPath=datapath,\n",
    "             modelPath=model_file,\n",
    "             outPath=out_file,\n",
    "             pretrainEpochs=200,\n",
    "             nEpochs=50,\n",
    "             discriminatorTrainPeriod=2,\n",
    "             generatorTrainPeriod=1,\n",
    "             pretrainBatchSize=batch_size,\n",
    "             batchSize=batch_size,\n",
    "             # protected = [6],\n",
    "             saveMaxKeep=0)\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    return mg.generateData(nSamples=inputNum,\n",
    "                        modelFile='fair-399',\n",
    "                        batchSize=batch_size,\n",
    "                        outFile=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, bias_dict = {}, surrogate = False):\n",
    "    dm = DataModule(data.values)\n",
    "    data_tensor = dm.setup()\n",
    "\n",
    "    #dm = SyntheticDataModule()\n",
    "    #data_tensor = dm.setup()\n",
    "    #activation_layer = nn.ReLU(inplace=True) #nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    # Causal GAN\n",
    "    #%% Import functions\n",
    "\n",
    "    params = dict()\n",
    "    params[\"iterations\"] = 2000\n",
    "    params[\"h_dim\"] = 200\n",
    "    params[\"z_dim\"] = 10\n",
    "    params[\"mb_size\"] = 128\n",
    "    params[\"lambda_gp\"] = 10\n",
    "    params[\"d_updates\"] = 10\n",
    "\n",
    "    max_epochs = (10 + 1) * 25 \n",
    "    number_of_gpus = 0\n",
    "\n",
    "\n",
    "    # Remove all the education level edges.#5\n",
    "    biased_list =[[1, 7],  [7, 10], \n",
    "                  [8, 10], \n",
    "                  [2, 13], [9, 5], [9, 10], \n",
    "                  [7, 8], \n",
    "                  [12, 3], # Removed edge between age and ethnicity.\n",
    "                  [9, 4], \n",
    "                  [8, 3], \n",
    "                  [6, 15], # Remove this for training purposes. \n",
    "                  [7, 11], [7, 15], [13, 3], [13, 14], [10, 2], [2, 14], \n",
    "                  [5, 3], [7, 2], [9, 15], [8, 2], [14, 3], [14, 15], [4, 3], [8, 15], \n",
    "                  [13, 11], [9, 12], [8, 9],\n",
    "                 [6,9] # This is the edge from ethnicity to employed\n",
    "                 ]\n",
    "    \n",
    "    \n",
    "    biased_list =[[1, 7],  [7, 10], \n",
    "                  [8, 10], \n",
    "                  [2, 13],  [9, 10], \n",
    "                  [7, 8], \n",
    "                  [12, 3], # Removed edge between age and ethnicity.\n",
    "                  [9, 4], \n",
    "                  [8, 3], \n",
    "                  [6, 15], # Remove this for training purposes. \n",
    "                  [7, 11], [7, 15], [13, 3], [13, 14], [10, 2], [2, 14], [7, 2], [9, 15], [8, 2], [14, 3], [14, 15], [4, 3], [8, 15], \n",
    "                  [13, 11], [9, 12], [8, 9],\n",
    "                 [6,9] # This is the edge from ethnicity to employed\n",
    "                 ]\n",
    "    \n",
    "    \n",
    "    # model initialisation and train\n",
    "    model = causal_gan(dm, dag_seed = biased_list,\n",
    "               h_dim=200,\n",
    "               lr=1e-3,\n",
    "               batch_size=64,\n",
    "               lambda_privacy=0,\n",
    "               lambda_gp=10,\n",
    "               d_updates=10,\n",
    "               causal=True,\n",
    "               alpha=2,\n",
    "               rho=2,\n",
    "               weight_decay=1e-2,\n",
    "               grad_dag_loss=False,\n",
    "               l1_g=0,\n",
    "               l1_W=1e-4,\n",
    "               p_gen=-0.2,\n",
    "               use_mask=True,\n",
    "              )\n",
    "    print(model.hparams)\n",
    "    trainer = pl.Trainer(gpus=number_of_gpus, max_epochs=max_epochs, progress_bar_refresh_rate=1, profiler = False)\n",
    "    model.set_val_data(data_tensor)\n",
    "\n",
    "    print(\"Training\")\n",
    "    trainer.fit(model, dm)\n",
    "    synth_data = model.gen_synthetic(data_tensor, gen_order = model.get_gen_order(), biased_edges = bias_dict, surrogate = surrogate).detach().numpy()\n",
    "    print(synth_data.shape)\n",
    "    \n",
    "    return synth_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "protected_idx = 6\n",
    "# ADSGAN\n",
    "#%% Import functions\n",
    "# from models.adsgan import adsgan\n",
    "# from models.gan import gan\n",
    "# from models.pategan import pategan\n",
    "# from models.vae import vae\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from metrics.combined import compute_metrics\n",
    "\n",
    "params = dict()\n",
    "params[\"iterations\"] = 2000\n",
    "params[\"h_dim\"] = 200\n",
    "params[\"z_dim\"] = 10\n",
    "params[\"mb_size\"] = 128\n",
    "params[\"lambda_gp\"] = 10\n",
    "params[\"d_updates\"] = 10\n",
    "params['lambda'] = 0\n",
    "\n",
    "def view_stats_new(method_list, input_data, orig_data = [], protected = '', skip_synth = False, protected_idx = -1, runs =3, bias_dict = {}, remove_protected = False, surrogate = False):\n",
    "\n",
    "    summary = ''\n",
    "    \n",
    "    samples = 5000\n",
    "    # Note that for gender 0 is female, and 1 is male\n",
    "    \n",
    "\n",
    "    \n",
    "    if not remove_protected:\n",
    "        x_pos = orig_data[orig_data[p_attr] == 0].drop(['approved'], axis = 1)[:samples]\n",
    "        y_pos = orig_data[orig_data[p_attr] == 0]['approved'][:samples]\n",
    "        x_neg = orig_data[orig_data[p_attr] == 1].drop(['approved'], axis = 1)[:samples]\n",
    "        y_neg = orig_data[orig_data[p_attr] == 1]['approved'][:samples]\n",
    "        print(len(x_pos), len(y_pos), len(x_neg), len(y_neg))\n",
    "    else:\n",
    "        input_data = input_data.drop([protected], axis = 1)\n",
    "        x_pos = orig_data[orig_data[p_attr] == 0].drop(['approved', protected], axis = 1)[:samples]\n",
    "        y_pos = orig_data[orig_data[p_attr] == 0]['approved'][:samples]\n",
    "        x_neg = orig_data[orig_data[p_attr] == 1].drop(['approved', protected], axis = 1)[:samples]\n",
    "        y_neg = orig_data[orig_data[p_attr] == 1]['approved'][:samples]\n",
    "        print(len(x_pos), len(y_pos), len(x_neg), len(y_neg))\n",
    "        \n",
    "    X_unbiased = pd.concat([x_pos, x_neg],axis=0).copy()\n",
    "    y_unbiased  = pd.concat([y_pos, y_neg],axis=0).copy()\n",
    "    \n",
    "    for method in method_list:\n",
    "        \n",
    "        params['gen_model_name'] = method.replace('-pr', '')\n",
    "        \n",
    "        if method == 'adsgan':\n",
    "            params['lambda'] = 0\n",
    "        else:\n",
    "            params['lambda'] = 1\n",
    "            \n",
    "            \n",
    "        if method == 'vae':\n",
    "            params[\"iterations\"] = 1000\n",
    "        else:\n",
    "            params[\"iterations\"] = 2000\n",
    "        err = []\n",
    "        feat_importance = []\n",
    "        recall_ratio = []\n",
    "        \n",
    "        mutual_info = []\n",
    "        precision = []\n",
    "        recall = []\n",
    "        density =[]\n",
    "        coverage = []\n",
    "        roc =[]\n",
    "        \n",
    "        for i in range(runs):\n",
    "            \n",
    "            if skip_synth:\n",
    "                synth_data = input_data.values\n",
    "            else:\n",
    "                if method == 'fairgan':\n",
    "                    # Need to swap 0 column with protected idx.\n",
    "                    temp = input_data.copy()\n",
    "                    popped = temp.pop('ethnicity')\n",
    "                    temp.insert(0, 'ethnicity', popped)\n",
    "                    \n",
    "                    pickle.dump(temp.values, open( \"adult.npy\", \"wb\" ) )\n",
    "                    synth_data, synth_data_z = train_fairgan('adult.npy')\n",
    "            \n",
    "                    # Have to swap columns back like so.... x[:,[2,1]] = x[:,[1,2]]\n",
    "                    #synth_data[:,[0,6]] = synth_data[:,[6,0]]\n",
    "                    \n",
    "                    print(\"synth before:\", synth_data.shape, synth_data_z.shape)\n",
    "                    synth_data = np.insert(synth_data, 6, synth_data_z, axis=1)\n",
    "                    print(\"synth after:\", synth_data.shape)\n",
    "\n",
    "                elif method == 'adsgan' or method == 'adsgan-pr':\n",
    "                    synth_data = adsgan(input_data, params)\n",
    "                elif method == 'gan' or method == 'wgan' or method == 'gan-pr' or method == 'wgan-pr':\n",
    "                    synth_data = gan(input_data, params)\n",
    "                elif method == 'vae':\n",
    "                    synth_data = vae(input_data, params)\n",
    "                else:\n",
    "                    synth_data = train_model(input_data, bias_dict, surrogate = surrogate)\n",
    "\n",
    "                                                     \n",
    "            # This step is to ensure at least one sample there.\n",
    "            pos_sample = input_data[input_data.approved == 0].iloc[0].values\n",
    "            neg_sample = input_data[input_data.approved == 1].iloc[0].values\n",
    "            synth_data = np.concatenate([synth_data, [pos_sample], [neg_sample]], axis = 0)        \n",
    "            X = synth_data[:,:-1]\n",
    "\n",
    "            #if remove_protected: \n",
    "            #    X = np.delete(synth_data, protected_idx, axis = 1)[:,:-1]\n",
    "            \n",
    "            y = np.round(synth_data[:, -1])\n",
    "\n",
    "            mlp = MLPClassifier(random_state = i, max_iter = 100).fit(X, y)\n",
    "            #mlp = LogisticRegression(random_state = i, max_iter = 100).fit(X, y)\n",
    "                \n",
    "            for X_unbiased, y_unbiased, _label in zip([x_pos, x_neg, pd.concat([x_pos, x_neg],axis=0).copy()], \n",
    "                                                      [y_pos, y_neg, pd.concat([y_pos, y_neg],axis=0).copy()],\n",
    "                                                      ['pos', 'neg', 'both']):\n",
    "                \n",
    "                print(\"LOGGGING\", len(X_unbiased), len(y_unbiased))\n",
    "                if not remove_protected:\n",
    "                    def compute_FTU(x):\n",
    "                        x[p_attr] = 0\n",
    "                        neg = mlp.predict(x)\n",
    "                        x[p_attr] = 1\n",
    "                        pos = mlp.predict(x)\n",
    "                        return pos-neg\n",
    "                    if _label == 'pos':\n",
    "                        FTU = compute_FTU(x_pos)\n",
    "                    elif _label == 'neg':\n",
    "                        FTU = compute_FTU(x_neg)\n",
    "                    else:\n",
    "                        x_all = pd.concat([x_pos, x_neg],axis=0)\n",
    "                        FTU = compute_FTU(x_all) \n",
    "                else:\n",
    "                    FTU = 0 # by definition\n",
    "                #print('FTU', FTU)\n",
    "                pred_pos = mlp.predict(x_pos)\n",
    "                pred_neg = mlp.predict(x_neg)\n",
    "                if _label == 'pos':\n",
    "                    DP = np.mean(pred_pos)\n",
    "                elif _label =='neg':\n",
    "                    DP = np.mean(pred_neg)\n",
    "                else:\n",
    "                    DP = np.mean(pred_pos)-np.mean(pred_neg)\n",
    "                #print('DP', DP)\n",
    "\n",
    "                CM = confusion_matrix(y_pos, mlp.predict(x_pos))\n",
    "                TN = CM[0][0]\n",
    "                FN = CM[1][0]\n",
    "                TP = CM[1][1]\n",
    "                FP = CM[0][1]\n",
    "\n",
    "                tpr_pos = CM[1][1]/(CM[0][0]+CM[0][1])\n",
    "                CM = confusion_matrix(y_neg, mlp.predict(x_neg))\n",
    "                tpr_neg = CM[1][1]/(CM[1][1]+CM[0][1])\n",
    "\n",
    "                roc.append(roc_auc_score(y_unbiased, mlp.predict_proba(X_unbiased)[:,1]))\n",
    "\n",
    "                print(tpr_pos), print(tpr_neg)\n",
    "                err.append(DP)#tpr_pos - tpr_neg)\n",
    "                if True:\n",
    "                    feat_importance.append(np.mean(FTU)) # BB 06/05 - just checking whether this leads to decent results.\n",
    "                elif protected_idx >= 0:\n",
    "                    feat_importance.append(mlp.coef_[0][protected_idx])\n",
    "\n",
    "                else:\n",
    "                    feat_importance.append(-1)\n",
    "\n",
    "                print(\"Feature Importance = \", feat_importance)\n",
    "\n",
    "                mutual_info.append(-1)\n",
    "\n",
    "\n",
    "                if remove_protected:\n",
    "                    results = compute_metrics(orig_data.drop([protected], axis = 1), synth_data,  which_metric = [['PRDC']], \n",
    "                                           wd_params = {},model = None,verbose = True)\n",
    "                else:\n",
    "                    #results = compute_metrics(orig_data.drop([protected], axis=1), np.delete(synth_data, protected_idx, 1),  which_metric = [['PRDC']], \n",
    "                    #                       wd_params = {},model = None,verbose = True)\n",
    "                    \n",
    "\n",
    "                    if _label == 'pos':\n",
    "                        results = compute_metrics(pd.concat([X_unbiased, y_unbiased],axis=1), synth_data[synth_data[:,protected_idx].astype(bool)],  which_metric = [['PRDC']], \n",
    "                                           wd_params = {},model = None,verbose = True)\n",
    "                    elif _label == 'neg':\n",
    "                        print(\"Computing neg\")\n",
    "                        print( synth_data[1-synth_data[:,protected_idx].astype(bool)].shape)\n",
    "                        results = compute_metrics(pd.concat([X_unbiased, y_unbiased],axis=1), synth_data[1-synth_data[:,protected_idx].astype(bool)],  which_metric = [['PRDC']], \n",
    "                                           wd_params = {},model = None,verbose = True)\n",
    "                    else:\n",
    "                        results = compute_metrics(pd.concat([X_unbiased, y_unbiased],axis=1), synth_data,  which_metric = [['PRDC']], \n",
    "                                           wd_params = {},model = None,verbose = True)\n",
    "                precision.append(results['precision'])\n",
    "                recall.append(results['recall'])\n",
    "                density.append(results['density'])\n",
    "                coverage.append(results['coverage'])\n",
    "\n",
    "                # Writing to file\n",
    "                with open(\"plots_surrogate_confounder_\" + _label + '.csv', \"a\") as log:\n",
    "                    # Writing data to a file\n",
    "                    log.write(method +\",\" + str(bias) + \",\" + str(results['precision']) + ',' + str(results['recall']) + ',' + str(results['density']) + \\\n",
    "                              ',' + str(results['coverage']) + ',' + str(err[-1]) + ',' + str(feat_importance[-1]) + ',' + str(roc[-1]) + '\\n')\n",
    "\n",
    "        if skip_synth:\n",
    "            print(\"no_synth\", round(np.mean(err),3), round(np.std(err),3), \n",
    "              round(np.mean(feat_importance),3), round(np.std(feat_importance),3),\n",
    "              round(np.mean(mutual_info),3), round(np.std(mutual_info),3)\n",
    "             )\n",
    "            break\n",
    "        else:\n",
    "            print(method, round(np.mean(err),3), round(np.std(err),3), \n",
    "              round(np.mean(feat_importance),3), round(np.std(feat_importance),3),\n",
    "              round(np.mean(mutual_info),3), round(np.std(mutual_info),3)\n",
    "             )\n",
    "\n",
    "        #summary+= method + '&$' + str(round(np.mean(precision),3)) + '\\pm' + str(round(np.std(precision),3)) + '$&$' + str(round(np.mean(recall),3)) + '\\pm' + str(round(np.std(recall),3)) + \\\n",
    "        #         '$&$' + str(round(np.mean(density),3)) + '\\pm' + str(round(np.std(density),3)) + '$&$' + str(round(np.mean(coverage),3)) + '\\pm' + str(round(np.std(coverage),3)) + \\\n",
    "        #         '$&$' + str(round(np.mean(err),3)) + '\\pm' + str(round(np.std(err),3)) + '$&$' + str(round(np.mean(mutual_info),3)) + '\\pm' + str(round(np.std(mutual_info),3)) + '$\\\\\\\\\\n'\n",
    "        \n",
    "        \n",
    "        summary+= method + '&$' + str(round(np.mean(precision),3)) + '\\pm' + str(round(np.std(precision),3)) + '$&$' + str(round(np.mean(recall),3)) + '\\pm' + str(round(np.std(recall),3)) + \\\n",
    "                 '$&$' + str(round(np.mean(err),3)) + '\\pm' + str(round(np.std(err),3)) + '$&$' + str(round(np.mean(feat_importance),3)) + '\\pm' + str(round(np.std(feat_importance),3)) + \\\n",
    "                 '$&$' + str(round(np.mean(roc),3)) + '\\pm' + str(round(np.std(roc),3)) +'$\\\\\\\\\\n'\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "0    539\n",
      "1    139\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:326: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:61: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:66: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:98: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:159: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:166: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:190: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "adult.npy\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:341: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:346: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:346: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:348: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:355: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:358: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\space\\OneDrive\\Documenten\\Uni\\UvA\\Year 1\\FACT\\Code\\fact-ai\\fairGAN_code\\fairGAN.py:361: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Pretrain_Epoch:0, trainLoss:0.166252, validLoss:0.129953, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.095776, validLoss:0.066318, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.058512, validLoss:0.046721, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.043155, validLoss:0.034720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.034058, validLoss:0.026929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.029611, validLoss:0.023102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.027288, validLoss:0.019949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.025633, validLoss:0.018777, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.024801, validLoss:0.019966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.024087, validLoss:0.019005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.023540, validLoss:0.017961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.023213, validLoss:0.016845, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.023095, validLoss:0.018073, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.022892, validLoss:0.017990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.022671, validLoss:0.017055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.022640, validLoss:0.016700, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.022581, validLoss:0.016704, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.022527, validLoss:0.016772, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.022448, validLoss:0.016945, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.022291, validLoss:0.016882, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.022418, validLoss:0.017109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.022273, validLoss:0.016946, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.022195, validLoss:0.016717, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.022396, validLoss:0.016432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.022248, validLoss:0.016754, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.022234, validLoss:0.016099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.022252, validLoss:0.017105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.022097, validLoss:0.016334, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.022387, validLoss:0.016270, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.022369, validLoss:0.016024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.022358, validLoss:0.017257, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.022182, validLoss:0.016114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.022395, validLoss:0.017104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.022381, validLoss:0.016975, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.022411, validLoss:0.017355, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.022356, validLoss:0.016447, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.022292, validLoss:0.015785, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.022392, validLoss:0.017017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.022352, validLoss:0.016612, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.022368, validLoss:0.015852, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.022190, validLoss:0.017019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.022319, validLoss:0.017370, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.022410, validLoss:0.017094, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.022371, validLoss:0.016752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.022320, validLoss:0.017119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.022373, validLoss:0.015335, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.022397, validLoss:0.016990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.022313, validLoss:0.016362, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.022139, validLoss:0.016853, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.022384, validLoss:0.016739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.022382, validLoss:0.016865, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.022413, validLoss:0.016222, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.022315, validLoss:0.017289, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.022213, validLoss:0.017363, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.022390, validLoss:0.016552, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.022392, validLoss:0.015523, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.022421, validLoss:0.017506, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.022307, validLoss:0.016859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.022268, validLoss:0.016318, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.022407, validLoss:0.016645, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.022321, validLoss:0.015742, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.022395, validLoss:0.016412, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.022346, validLoss:0.016836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.022201, validLoss:0.016170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.022420, validLoss:0.016204, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.022409, validLoss:0.016942, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.022425, validLoss:0.016844, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.022392, validLoss:0.015634, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.022223, validLoss:0.017313, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:69, trainLoss:0.022280, validLoss:0.016892, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.022437, validLoss:0.017343, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.022396, validLoss:0.015473, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.022281, validLoss:0.017175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.022333, validLoss:0.016527, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.022416, validLoss:0.017279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.022360, validLoss:0.016274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.022364, validLoss:0.016660, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.022338, validLoss:0.017275, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.022429, validLoss:0.016807, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.022310, validLoss:0.017001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.022421, validLoss:0.017189, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.022438, validLoss:0.016617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.022265, validLoss:0.016490, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.022240, validLoss:0.017295, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.022431, validLoss:0.016836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.022374, validLoss:0.016462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.022371, validLoss:0.016646, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.022408, validLoss:0.016971, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.022330, validLoss:0.017485, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.022454, validLoss:0.016820, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.022352, validLoss:0.017124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.022219, validLoss:0.016397, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.022442, validLoss:0.016078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.022244, validLoss:0.017262, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.022324, validLoss:0.016419, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.022438, validLoss:0.016819, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.022411, validLoss:0.015564, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.022419, validLoss:0.016815, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.022417, validLoss:0.015713, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.022406, validLoss:0.016957, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.022409, validLoss:0.017185, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.022376, validLoss:0.016684, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.022435, validLoss:0.016324, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.022340, validLoss:0.015942, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.022284, validLoss:0.016649, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.022419, validLoss:0.016552, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.022403, validLoss:0.016498, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.022431, validLoss:0.017378, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.022434, validLoss:0.017338, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.022418, validLoss:0.016517, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.022381, validLoss:0.016992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.022429, validLoss:0.016766, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.022416, validLoss:0.016398, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.022326, validLoss:0.016708, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.022280, validLoss:0.016407, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.022420, validLoss:0.017184, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.022285, validLoss:0.016652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.022322, validLoss:0.015432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.022391, validLoss:0.016462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.022418, validLoss:0.016392, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.022418, validLoss:0.017020, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.022294, validLoss:0.017493, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.022282, validLoss:0.017285, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.022340, validLoss:0.016244, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.022419, validLoss:0.016047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.022372, validLoss:0.016699, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.022246, validLoss:0.016536, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.022318, validLoss:0.017231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.022139, validLoss:0.017093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.022308, validLoss:0.017122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.022406, validLoss:0.016432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.022423, validLoss:0.016069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.022411, validLoss:0.016290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.022433, validLoss:0.016982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.022341, validLoss:0.016672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.022442, validLoss:0.016712, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.022215, validLoss:0.016826, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.022422, validLoss:0.017168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.022421, validLoss:0.017151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.022274, validLoss:0.016127, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.022341, validLoss:0.016964, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.022354, validLoss:0.017024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.022400, validLoss:0.016517, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.022449, validLoss:0.016666, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.022263, validLoss:0.016304, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.022426, validLoss:0.016709, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.022295, validLoss:0.016679, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.022395, validLoss:0.017221, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.022406, validLoss:0.017099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.022340, validLoss:0.016360, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.022293, validLoss:0.016841, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.022415, validLoss:0.016239, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.022385, validLoss:0.016367, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.022431, validLoss:0.015371, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.022398, validLoss:0.016957, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.022396, validLoss:0.017244, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.022433, validLoss:0.017207, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.022268, validLoss:0.017174, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.022236, validLoss:0.016595, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.022368, validLoss:0.016613, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.022438, validLoss:0.017061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.022300, validLoss:0.016559, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.022317, validLoss:0.016704, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.022140, validLoss:0.015827, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.022242, validLoss:0.016970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.022146, validLoss:0.016882, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.022247, validLoss:0.016286, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.022224, validLoss:0.016380, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.022178, validLoss:0.016313, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.022286, validLoss:0.015728, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.022192, validLoss:0.015355, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.022255, validLoss:0.017100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.022055, validLoss:0.016750, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.021301, validLoss:0.016623, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.019910, validLoss:0.014250, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.017391, validLoss:0.013961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.016332, validLoss:0.013725, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:177, trainLoss:0.013610, validLoss:0.008203, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.008703, validLoss:0.006817, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.007822, validLoss:0.005519, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.006831, validLoss:0.004509, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.005636, validLoss:0.003895, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.004127, validLoss:0.002339, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.002841, validLoss:0.001443, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.002146, validLoss:0.001222, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.002073, validLoss:0.001289, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001996, validLoss:0.001208, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001988, validLoss:0.001239, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001997, validLoss:0.001205, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001980, validLoss:0.001231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001994, validLoss:0.001211, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001988, validLoss:0.001180, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001980, validLoss:0.001170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001988, validLoss:0.001241, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.002033, validLoss:0.001201, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001965, validLoss:0.001262, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001975, validLoss:0.001399, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001955, validLoss:0.001262, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001937, validLoss:0.001208, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001935, validLoss:0.001232, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.869364, g_loss:2.749809, d accuracy:0.989583, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.889582, g_loss:1.528995, d accuracy:0.541667, d AUC:0.826172, g accuracy:0.083333, rdf 0.000000\n",
      "Epoch:2, d_loss:1.156606, g_loss:1.053860, d accuracy:0.890625, d AUC:0.953451, g accuracy:0.812500, rdf 0.000000\n",
      "Epoch:3, d_loss:0.486476, g_loss:3.125271, d accuracy:0.848958, d AUC:0.983398, g accuracy:0.697917, rdf 0.000000\n",
      "Epoch:4, d_loss:0.462760, g_loss:2.950118, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.221105, g_loss:4.063584, d accuracy:0.838542, d AUC:0.995117, g accuracy:0.677083, rdf 0.000000\n",
      "Epoch:6, d_loss:0.251807, g_loss:4.149309, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:7, d_loss:0.142754, g_loss:3.878194, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:8, d_loss:0.374243, g_loss:1.695675, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.142988, g_loss:3.133823, d accuracy:0.932292, d AUC:1.000000, g accuracy:0.864583, rdf 0.000000\n",
      "Epoch:10, d_loss:0.195285, g_loss:3.665556, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.048581, g_loss:4.497821, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.021724, g_loss:4.197212, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.017361, g_loss:4.394709, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.013518, g_loss:4.686972, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.010407, g_loss:5.001963, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.008204, g_loss:5.175333, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.008124, g_loss:5.251504, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.005815, g_loss:5.629335, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.028095, g_loss:4.763643, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.005017, g_loss:8.374339, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.002955, g_loss:7.595010, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.013017, g_loss:5.388495, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.006171, g_loss:11.763274, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.002121, g_loss:8.760438, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.002657, g_loss:7.034776, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.002278, g_loss:6.773565, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.002876, g_loss:6.435570, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.026276, g_loss:4.337077, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.189301, g_loss:8.228554, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.004235, g_loss:9.492857, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.052208, g_loss:5.801726, d accuracy:0.875000, d AUC:0.999349, g accuracy:0.750000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.199811, g_loss:5.152235, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:33, d_loss:0.062656, g_loss:7.652564, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:34, d_loss:0.022169, g_loss:17.131325, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.010853, g_loss:10.380530, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.020095, g_loss:8.092160, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:37, d_loss:0.340139, g_loss:6.728431, d accuracy:0.989583, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.053978, g_loss:8.849422, d accuracy:0.984375, d AUC:0.999674, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:39, d_loss:0.104813, g_loss:8.013189, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.039976, g_loss:6.657696, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.032824, g_loss:8.625888, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:42, d_loss:0.036560, g_loss:6.608685, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.028131, g_loss:6.071896, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.054283, g_loss:4.747249, d accuracy:0.942708, d AUC:1.000000, g accuracy:0.885417, rdf 0.000000\n",
      "Epoch:45, d_loss:0.309525, g_loss:4.904944, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.149655, g_loss:5.050660, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.056043, g_loss:7.102576, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.032512, g_loss:6.023803, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.038911, g_loss:5.653762, d accuracy:0.979167, d AUC:1.000000, g accuracy:0.958333, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.220194, validLoss:0.162885, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.135520, validLoss:0.105985, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.101013, validLoss:0.085572, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.083047, validLoss:0.072261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.071540, validLoss:0.062546, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.065144, validLoss:0.060789, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.061275, validLoss:0.058350, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.059098, validLoss:0.055708, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.057537, validLoss:0.053585, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.056653, validLoss:0.052960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.056102, validLoss:0.053603, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.055749, validLoss:0.053349, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.055606, validLoss:0.052132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.055345, validLoss:0.051990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.055222, validLoss:0.051974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.055097, validLoss:0.051864, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.055001, validLoss:0.051935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.054528, validLoss:0.052492, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.054369, validLoss:0.052364, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.054255, validLoss:0.052338, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.054187, validLoss:0.051224, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.054125, validLoss:0.052188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.054064, validLoss:0.052232, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.054149, validLoss:0.053231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.054135, validLoss:0.051162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.054120, validLoss:0.051151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.054085, validLoss:0.052146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.054107, validLoss:0.052176, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.053978, validLoss:0.051152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.053962, validLoss:0.051114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.053936, validLoss:0.051158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.054049, validLoss:0.052264, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.053950, validLoss:0.052135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.054132, validLoss:0.052209, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.054041, validLoss:0.052192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.053933, validLoss:0.052169, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.053914, validLoss:0.052218, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.053801, validLoss:0.053236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.054027, validLoss:0.052152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.053906, validLoss:0.051114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.054007, validLoss:0.051122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.053902, validLoss:0.051123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.053978, validLoss:0.052165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.053898, validLoss:0.054230, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.054004, validLoss:0.051144, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.053868, validLoss:0.052188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.053885, validLoss:0.051112, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.053986, validLoss:0.051174, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.053914, validLoss:0.053219, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.053901, validLoss:0.051175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.053892, validLoss:0.051149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.053880, validLoss:0.053248, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.054008, validLoss:0.051110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.053992, validLoss:0.052185, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.053882, validLoss:0.052146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.053900, validLoss:0.053211, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.053914, validLoss:0.052226, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.053891, validLoss:0.051172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.053890, validLoss:0.051170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.053991, validLoss:0.051175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.053996, validLoss:0.051178, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.053874, validLoss:0.052243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.053903, validLoss:0.053261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.053905, validLoss:0.053248, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.054012, validLoss:0.052197, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.053995, validLoss:0.053181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.053893, validLoss:0.052196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.053992, validLoss:0.052145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.053997, validLoss:0.052190, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.054006, validLoss:0.052098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.054003, validLoss:0.052204, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.053906, validLoss:0.052189, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.054002, validLoss:0.051131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.053995, validLoss:0.052126, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.053890, validLoss:0.053178, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.054028, validLoss:0.051130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.053899, validLoss:0.054261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.054020, validLoss:0.052084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.053999, validLoss:0.051147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.053829, validLoss:0.053269, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.054015, validLoss:0.052166, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.054017, validLoss:0.052183, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.054036, validLoss:0.052178, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.053911, validLoss:0.052292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.053911, validLoss:0.051232, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.053897, validLoss:0.051125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.054020, validLoss:0.051066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.054012, validLoss:0.051214, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.053913, validLoss:0.051175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.053906, validLoss:0.051195, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.053891, validLoss:0.053260, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:91, trainLoss:0.053904, validLoss:0.052156, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.054021, validLoss:0.051192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.053871, validLoss:0.053192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.054001, validLoss:0.052178, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.054113, validLoss:0.051161, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.054012, validLoss:0.051159, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.054113, validLoss:0.051206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.054030, validLoss:0.053234, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.053907, validLoss:0.053241, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.054026, validLoss:0.052233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.053898, validLoss:0.053224, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.053996, validLoss:0.051157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.054014, validLoss:0.051168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.053002, validLoss:0.035277, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.017003, validLoss:0.011237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.008993, validLoss:0.007913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.005028, validLoss:0.004836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.003105, validLoss:0.002694, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.002404, validLoss:0.002045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.002075, validLoss:0.001567, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.001961, validLoss:0.001445, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.001841, validLoss:0.001410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.001812, validLoss:0.001290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.001779, validLoss:0.001215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.001778, validLoss:0.001292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.001745, validLoss:0.001211, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.001728, validLoss:0.001232, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.001706, validLoss:0.001171, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.001769, validLoss:0.001156, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.001760, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.001717, validLoss:0.001146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.001712, validLoss:0.001257, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.001793, validLoss:0.001196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.001725, validLoss:0.001240, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.001721, validLoss:0.001110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.001701, validLoss:0.001165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.001692, validLoss:0.001149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.001711, validLoss:0.001218, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.001729, validLoss:0.001189, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.001694, validLoss:0.001194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.001730, validLoss:0.001131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.001701, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.001664, validLoss:0.001151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.001710, validLoss:0.001111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.001703, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.001681, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001721, validLoss:0.001232, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001740, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001701, validLoss:0.001182, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001681, validLoss:0.001103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001714, validLoss:0.001163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001711, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001581, validLoss:0.001146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001689, validLoss:0.001190, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001679, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001675, validLoss:0.001123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001689, validLoss:0.001103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001681, validLoss:0.001132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001689, validLoss:0.001149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001705, validLoss:0.001184, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001709, validLoss:0.001265, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001729, validLoss:0.001151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001713, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001739, validLoss:0.001129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001681, validLoss:0.001035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001666, validLoss:0.001145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001693, validLoss:0.001166, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001677, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001689, validLoss:0.001121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001697, validLoss:0.001089, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001667, validLoss:0.001055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001679, validLoss:0.001169, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001705, validLoss:0.001300, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001707, validLoss:0.001135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001730, validLoss:0.001135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001686, validLoss:0.001076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001709, validLoss:0.001131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001678, validLoss:0.001095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001658, validLoss:0.001130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001644, validLoss:0.001145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001725, validLoss:0.001130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001734, validLoss:0.001142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001679, validLoss:0.001160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001672, validLoss:0.001153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001721, validLoss:0.001147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001703, validLoss:0.001126, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001697, validLoss:0.001051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001707, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001670, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001649, validLoss:0.001205, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001695, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001691, validLoss:0.001150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001687, validLoss:0.001124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001718, validLoss:0.001163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001705, validLoss:0.001078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001653, validLoss:0.001123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001683, validLoss:0.001142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001661, validLoss:0.001133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001658, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001693, validLoss:0.001138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001646, validLoss:0.001090, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001660, validLoss:0.000995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001707, validLoss:0.001143, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:194, trainLoss:0.001695, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001676, validLoss:0.001139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001655, validLoss:0.001047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001708, validLoss:0.001192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001690, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001652, validLoss:0.001091, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.207864, g_loss:2.135027, d accuracy:0.723958, d AUC:0.930664, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:1, d_loss:1.012985, g_loss:3.118001, d accuracy:0.973958, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:2, d_loss:0.281149, g_loss:4.619853, d accuracy:0.937500, d AUC:0.977214, g accuracy:0.875000, rdf 0.000000\n",
      "Epoch:3, d_loss:0.456608, g_loss:4.331277, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:4, d_loss:0.161061, g_loss:6.871770, d accuracy:0.958333, d AUC:0.981771, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:5, d_loss:0.277026, g_loss:5.791146, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.096394, g_loss:6.714535, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.033091, g_loss:7.470912, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.022197, g_loss:7.278125, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.563200, g_loss:2.896573, d accuracy:0.968750, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.214510, g_loss:3.230223, d accuracy:0.864583, d AUC:1.000000, g accuracy:0.729167, rdf 0.000000\n",
      "Epoch:11, d_loss:0.274230, g_loss:1.837341, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.108845, g_loss:2.770867, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.062474, g_loss:3.294697, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:14, d_loss:0.125847, g_loss:2.842503, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.028863, g_loss:5.461573, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.020083, g_loss:4.706071, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.020282, g_loss:4.276811, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:18, d_loss:0.062561, g_loss:4.283242, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.019451, g_loss:5.381389, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.026254, g_loss:3.978673, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.017853, g_loss:6.107666, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.008099, g_loss:7.081895, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.093042, g_loss:4.690224, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.024538, g_loss:7.317133, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.034783, g_loss:6.360531, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.023881, g_loss:5.900304, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.012159, g_loss:7.055295, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.012416, g_loss:5.155684, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.015258, g_loss:5.493920, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.008410, g_loss:6.701768, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.006567, g_loss:7.626456, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.004985, g_loss:7.921472, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.021622, g_loss:7.564033, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.020260, g_loss:7.465644, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.084753, g_loss:6.522516, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:36, d_loss:0.049793, g_loss:7.630142, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.038907, g_loss:9.649236, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.019194, g_loss:9.037155, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.048235, g_loss:8.325366, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:40, d_loss:0.098531, g_loss:6.925204, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.033375, g_loss:6.870769, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.028994, g_loss:5.946543, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.083354, g_loss:3.361373, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.104583, g_loss:4.884359, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.044987, g_loss:5.945717, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.108347, g_loss:5.397130, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.013429, g_loss:8.057581, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.013926, g_loss:6.388879, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.026830, g_loss:5.215705, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.254247, validLoss:0.194948, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.152739, validLoss:0.114682, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.102713, validLoss:0.089051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.083095, validLoss:0.075013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.072079, validLoss:0.067597, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.066696, validLoss:0.064134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.063737, validLoss:0.062274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.061995, validLoss:0.061974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.061051, validLoss:0.058956, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.060206, validLoss:0.060639, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.059792, validLoss:0.060385, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.059410, validLoss:0.059890, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:12, trainLoss:0.059226, validLoss:0.058841, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.058932, validLoss:0.060342, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.058867, validLoss:0.057452, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.058737, validLoss:0.058672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.058737, validLoss:0.058041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.058688, validLoss:0.058340, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.058539, validLoss:0.058309, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.058593, validLoss:0.058440, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.058472, validLoss:0.057358, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.058471, validLoss:0.057744, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.058451, validLoss:0.057591, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.058481, validLoss:0.058036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.058505, validLoss:0.058970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.058569, validLoss:0.057981, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.058371, validLoss:0.058215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.058601, validLoss:0.057910, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.058569, validLoss:0.058227, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.058394, validLoss:0.057674, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.058478, validLoss:0.058221, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.058594, validLoss:0.059022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.058495, validLoss:0.058076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.058395, validLoss:0.057273, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.058421, validLoss:0.060635, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.058490, validLoss:0.058287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.058509, validLoss:0.058947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.058305, validLoss:0.058196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.058514, validLoss:0.057316, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.058432, validLoss:0.057766, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.058471, validLoss:0.058232, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.058437, validLoss:0.056265, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.058490, validLoss:0.057446, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.058420, validLoss:0.058779, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.058515, validLoss:0.057729, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.058407, validLoss:0.058101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.058519, validLoss:0.057241, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.058447, validLoss:0.057556, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.058538, validLoss:0.058617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.058552, validLoss:0.058216, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.058545, validLoss:0.058322, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.058565, validLoss:0.058970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.058463, validLoss:0.057179, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.058540, validLoss:0.058222, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.058445, validLoss:0.057182, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.058580, validLoss:0.057596, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.058492, validLoss:0.058301, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.058555, validLoss:0.059111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.058561, validLoss:0.058943, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.058450, validLoss:0.056470, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.058429, validLoss:0.058202, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.058478, validLoss:0.058081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.058495, validLoss:0.059150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.058508, validLoss:0.058995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.058597, validLoss:0.059531, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.058422, validLoss:0.058772, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.058491, validLoss:0.058041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.058554, validLoss:0.058795, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.058567, validLoss:0.058289, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.058570, validLoss:0.056852, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.058515, validLoss:0.057226, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.058596, validLoss:0.058672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.058599, validLoss:0.058052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.058474, validLoss:0.057940, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.058530, validLoss:0.058617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.058496, validLoss:0.058064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.058512, validLoss:0.057015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.058495, validLoss:0.057040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.058486, validLoss:0.058196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.058439, validLoss:0.058220, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.058521, validLoss:0.057294, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.058530, validLoss:0.058320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.058603, validLoss:0.058012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.058508, validLoss:0.058214, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.058510, validLoss:0.057532, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.058595, validLoss:0.058308, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.058689, validLoss:0.057278, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.058589, validLoss:0.059064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.058618, validLoss:0.057282, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.058606, validLoss:0.058368, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.058517, validLoss:0.058944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.058566, validLoss:0.059739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.058646, validLoss:0.057549, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.058512, validLoss:0.059022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.058598, validLoss:0.057883, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.054608, validLoss:0.055252, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.053606, validLoss:0.054192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.053423, validLoss:0.054104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.053162, validLoss:0.054913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.053013, validLoss:0.052289, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.052836, validLoss:0.054319, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.052467, validLoss:0.052742, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.052195, validLoss:0.052277, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.052078, validLoss:0.051305, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.051959, validLoss:0.053781, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.051872, validLoss:0.051954, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.051756, validLoss:0.052715, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.051805, validLoss:0.051118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.051700, validLoss:0.051780, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.051794, validLoss:0.051899, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:110, trainLoss:0.051709, validLoss:0.051059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.051734, validLoss:0.053452, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.051795, validLoss:0.051975, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.051731, validLoss:0.051859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.051801, validLoss:0.052617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.051716, validLoss:0.052648, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.051706, validLoss:0.051053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.051786, validLoss:0.051851, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.051692, validLoss:0.053682, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.051742, validLoss:0.052654, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.051898, validLoss:0.050977, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.051703, validLoss:0.051804, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.051715, validLoss:0.052655, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.051763, validLoss:0.051044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.051729, validLoss:0.052613, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.051710, validLoss:0.052563, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.051789, validLoss:0.051859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.051696, validLoss:0.051832, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.051708, validLoss:0.051914, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.051813, validLoss:0.051880, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.051694, validLoss:0.053442, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.051828, validLoss:0.052582, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.051790, validLoss:0.052558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.051847, validLoss:0.051851, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.051810, validLoss:0.051043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.051903, validLoss:0.051038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.051700, validLoss:0.052587, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.051817, validLoss:0.051157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.051733, validLoss:0.051791, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.051710, validLoss:0.051900, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.051696, validLoss:0.051786, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.051830, validLoss:0.051883, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.051800, validLoss:0.051023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.051590, validLoss:0.051849, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.051804, validLoss:0.051897, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.051781, validLoss:0.051791, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.051707, validLoss:0.051815, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.051795, validLoss:0.051029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.051706, validLoss:0.051843, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.051781, validLoss:0.053396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.051881, validLoss:0.051100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.051746, validLoss:0.051929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.051752, validLoss:0.051910, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.051813, validLoss:0.051823, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.051761, validLoss:0.051030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.051782, validLoss:0.051747, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.051698, validLoss:0.051849, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.051724, validLoss:0.051846, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.051786, validLoss:0.052583, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.051879, validLoss:0.051026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.051802, validLoss:0.051798, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.051789, validLoss:0.052053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.051715, validLoss:0.051864, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.051823, validLoss:0.051998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.051808, validLoss:0.052622, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.051839, validLoss:0.053670, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.051720, validLoss:0.050997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.051822, validLoss:0.053425, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.051709, validLoss:0.051814, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.051869, validLoss:0.051069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.051686, validLoss:0.052652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.051759, validLoss:0.051840, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.051845, validLoss:0.051042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.051797, validLoss:0.052650, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.051707, validLoss:0.051857, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.051838, validLoss:0.051827, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.051829, validLoss:0.051045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.051723, validLoss:0.052028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.051809, validLoss:0.050970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.051709, validLoss:0.051005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.051691, validLoss:0.054245, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.051738, validLoss:0.052605, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.051802, validLoss:0.051837, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.051718, validLoss:0.051830, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.051814, validLoss:0.051871, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.051727, validLoss:0.052560, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.051688, validLoss:0.051805, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.034965, validLoss:0.011565, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.008566, validLoss:0.005905, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.006046, validLoss:0.004821, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.005401, validLoss:0.004306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.004538, validLoss:0.003350, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.003557, validLoss:0.001984, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.002739, validLoss:0.001779, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.002158, validLoss:0.001436, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001870, validLoss:0.001268, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001773, validLoss:0.001109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001772, validLoss:0.001265, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001744, validLoss:0.001156, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001699, validLoss:0.001129, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.292814, g_loss:2.276307, d accuracy:0.651042, d AUC:0.964844, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:1, d_loss:1.069688, g_loss:2.442903, d accuracy:0.619792, d AUC:0.948893, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:2, d_loss:0.467647, g_loss:5.599423, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:3, d_loss:0.152562, g_loss:8.985179, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4, d_loss:0.067170, g_loss:9.918409, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.041100, g_loss:7.813876, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.046484, g_loss:6.937342, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.073438, g_loss:9.082402, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:8, d_loss:0.082950, g_loss:8.766422, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.061463, g_loss:8.749255, d accuracy:0.994792, d AUC:0.999674, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:10, d_loss:0.037891, g_loss:10.326329, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.028991, g_loss:10.616565, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:12, d_loss:0.028334, g_loss:13.147990, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:13, d_loss:0.015842, g_loss:10.330235, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.008526, g_loss:10.602940, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.007674, g_loss:10.553782, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.007481, g_loss:8.356395, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.007691, g_loss:6.787165, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.007097, g_loss:8.970069, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.017228, g_loss:8.031336, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.143335, g_loss:3.998905, d accuracy:0.833333, d AUC:0.992188, g accuracy:0.666667, rdf 0.000000\n",
      "Epoch:21, d_loss:0.474270, g_loss:1.039209, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:22, d_loss:0.151333, g_loss:2.943477, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.103058, g_loss:3.948701, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.075624, g_loss:4.662017, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:25, d_loss:0.098626, g_loss:4.765079, d accuracy:0.968750, d AUC:0.993815, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:26, d_loss:0.164557, g_loss:4.287868, d accuracy:0.989583, d AUC:0.996094, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:27, d_loss:0.243688, g_loss:2.840831, d accuracy:0.963542, d AUC:1.000000, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:28, d_loss:0.282746, g_loss:5.231851, d accuracy:0.989583, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.292286, g_loss:5.029023, d accuracy:0.989583, d AUC:0.997721, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:30, d_loss:0.404007, g_loss:4.708533, d accuracy:0.869792, d AUC:0.916341, g accuracy:0.770833, rdf 0.000000\n",
      "Epoch:31, d_loss:0.368173, g_loss:5.609422, d accuracy:0.973958, d AUC:0.993815, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:32, d_loss:0.212065, g_loss:4.777899, d accuracy:0.984375, d AUC:0.998372, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:33, d_loss:0.167369, g_loss:4.982921, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.140285, g_loss:4.678402, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:35, d_loss:0.240987, g_loss:3.475839, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:36, d_loss:0.363541, g_loss:2.645666, d accuracy:0.718750, d AUC:0.853190, g accuracy:0.614583, rdf 0.000000\n",
      "Epoch:37, d_loss:0.583576, g_loss:3.093948, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:38, d_loss:0.174581, g_loss:3.141795, d accuracy:0.994792, d AUC:0.998047, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:39, d_loss:0.150256, g_loss:4.177560, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:40, d_loss:0.265385, g_loss:4.242090, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.222168, g_loss:3.561764, d accuracy:0.979167, d AUC:0.999674, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:42, d_loss:0.373179, g_loss:3.355012, d accuracy:0.984375, d AUC:0.999349, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:43, d_loss:0.406923, g_loss:3.112419, d accuracy:0.692708, d AUC:0.722331, g accuracy:0.489583, rdf 0.000000\n",
      "Epoch:44, d_loss:3.196963, g_loss:1.210777, d accuracy:0.973958, d AUC:0.987630, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:45, d_loss:0.316084, g_loss:4.544075, d accuracy:0.937500, d AUC:0.979492, g accuracy:0.875000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.550689, g_loss:2.499057, d accuracy:0.859375, d AUC:0.940104, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:47, d_loss:0.589888, g_loss:3.399769, d accuracy:0.994792, d AUC:0.999023, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:48, d_loss:0.517511, g_loss:3.006671, d accuracy:0.927083, d AUC:0.963216, g accuracy:0.885417, rdf 0.000000\n",
      "Epoch:49, d_loss:0.672147, g_loss:2.619392, d accuracy:0.963542, d AUC:0.999023, g accuracy:0.927083, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n",
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "0    491\n",
      "1    187\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.171738, validLoss:0.133653, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.134054, validLoss:0.117520, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.119952, validLoss:0.104306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.096032, validLoss:0.083862, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.083440, validLoss:0.073850, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.076552, validLoss:0.069778, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.072962, validLoss:0.068124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.071462, validLoss:0.068331, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.070306, validLoss:0.065248, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.069554, validLoss:0.064936, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.069571, validLoss:0.065303, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.069114, validLoss:0.065108, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:12, trainLoss:0.069132, validLoss:0.064641, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.068869, validLoss:0.067047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.068542, validLoss:0.066514, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.068627, validLoss:0.065990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.068642, validLoss:0.066064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.068586, validLoss:0.065902, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.068568, validLoss:0.064141, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.068349, validLoss:0.066296, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.068534, validLoss:0.064747, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.068613, validLoss:0.064685, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.068380, validLoss:0.062812, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.068523, validLoss:0.063876, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.068336, validLoss:0.065082, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.068256, validLoss:0.065010, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.068388, validLoss:0.064165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.068529, validLoss:0.065764, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.068354, validLoss:0.065121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.068521, validLoss:0.064283, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.068334, validLoss:0.064895, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.068396, validLoss:0.064079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.068301, validLoss:0.065999, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.068427, validLoss:0.066261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.068658, validLoss:0.064614, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.068447, validLoss:0.065557, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.068363, validLoss:0.064425, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.068324, validLoss:0.064682, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.068437, validLoss:0.064980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.068451, validLoss:0.063637, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.068428, validLoss:0.064162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.068449, validLoss:0.063947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.068460, validLoss:0.062948, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.068583, validLoss:0.065541, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.068461, validLoss:0.064614, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.068527, validLoss:0.065877, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.068475, validLoss:0.068634, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.068412, validLoss:0.065553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.068584, validLoss:0.065279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.068495, validLoss:0.065386, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.068582, validLoss:0.066118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.068439, validLoss:0.061832, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.068452, validLoss:0.064277, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.068521, validLoss:0.063700, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.068522, validLoss:0.065240, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.068617, validLoss:0.065302, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.068650, validLoss:0.065181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.068534, validLoss:0.064956, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.068455, validLoss:0.065539, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.068529, validLoss:0.064690, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.068404, validLoss:0.065340, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.068618, validLoss:0.063801, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.068640, validLoss:0.064046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.068536, validLoss:0.066255, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.068548, validLoss:0.063344, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.068563, validLoss:0.063372, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.068546, validLoss:0.063807, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.068489, validLoss:0.066160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.068532, validLoss:0.065220, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.068658, validLoss:0.063286, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.068642, validLoss:0.063895, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.068436, validLoss:0.062958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.068346, validLoss:0.064242, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.068483, validLoss:0.064432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.068678, validLoss:0.065640, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.068543, validLoss:0.063812, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.068460, validLoss:0.064472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.068563, validLoss:0.066512, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.068371, validLoss:0.064855, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.068478, validLoss:0.064795, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.068547, validLoss:0.064231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.068320, validLoss:0.065439, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.068674, validLoss:0.063153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.068637, validLoss:0.066190, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.068598, validLoss:0.066443, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.068560, validLoss:0.065431, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.068470, validLoss:0.064618, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.068570, validLoss:0.065755, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.068551, validLoss:0.065194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.068562, validLoss:0.065121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.068581, validLoss:0.063371, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.068589, validLoss:0.065391, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.068552, validLoss:0.065958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.068562, validLoss:0.065235, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.068536, validLoss:0.064380, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.068677, validLoss:0.067103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.068447, validLoss:0.065246, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.068661, validLoss:0.065679, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.068597, validLoss:0.063212, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.068686, validLoss:0.065730, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.068468, validLoss:0.065085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.068670, validLoss:0.066245, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.068664, validLoss:0.065593, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.068589, validLoss:0.063782, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.068651, validLoss:0.064516, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.068688, validLoss:0.063466, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.068550, validLoss:0.063236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.068610, validLoss:0.063756, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.068752, validLoss:0.065475, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.068547, validLoss:0.065210, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.068669, validLoss:0.066903, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.068568, validLoss:0.064451, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.068736, validLoss:0.063979, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.068697, validLoss:0.062717, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.068366, validLoss:0.065820, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.068580, validLoss:0.065396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.068555, validLoss:0.064530, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.068536, validLoss:0.064742, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.068707, validLoss:0.063840, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.068522, validLoss:0.064965, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.068468, validLoss:0.064723, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.068724, validLoss:0.066194, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:122, trainLoss:0.068715, validLoss:0.065470, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.068611, validLoss:0.065780, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.068605, validLoss:0.066130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.068591, validLoss:0.065745, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.068496, validLoss:0.063712, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.068692, validLoss:0.067467, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.068614, validLoss:0.068235, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.068481, validLoss:0.063810, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.068566, validLoss:0.065929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.068562, validLoss:0.064555, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.068576, validLoss:0.067065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.068620, validLoss:0.063487, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.068782, validLoss:0.066149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.068678, validLoss:0.066158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.068469, validLoss:0.066341, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.068600, validLoss:0.064737, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.068513, validLoss:0.064067, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.068695, validLoss:0.064488, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.068697, validLoss:0.066374, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.068536, validLoss:0.063854, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.068491, validLoss:0.064441, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.068490, validLoss:0.066405, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.068581, validLoss:0.063019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.068507, validLoss:0.066332, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.068476, validLoss:0.064195, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.068403, validLoss:0.066069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.067259, validLoss:0.062994, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.059861, validLoss:0.053377, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.053462, validLoss:0.051671, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.051465, validLoss:0.047025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.050485, validLoss:0.049638, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.050246, validLoss:0.048170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.050151, validLoss:0.046658, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.050152, validLoss:0.048735, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.050281, validLoss:0.046099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.050098, validLoss:0.048919, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.050090, validLoss:0.048404, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.050123, validLoss:0.048570, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.049973, validLoss:0.048673, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.050051, validLoss:0.049824, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.050124, validLoss:0.047730, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.050086, validLoss:0.049573, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.050192, validLoss:0.048102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.050175, validLoss:0.050061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.050000, validLoss:0.047893, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.049962, validLoss:0.049218, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.049991, validLoss:0.048459, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.049976, validLoss:0.050588, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.050167, validLoss:0.048461, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.044293, validLoss:0.040670, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.041631, validLoss:0.041005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.041009, validLoss:0.040131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.037743, validLoss:0.022220, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.019986, validLoss:0.016429, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.011813, validLoss:0.007635, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.005811, validLoss:0.003033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.002765, validLoss:0.001416, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001945, validLoss:0.001230, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001808, validLoss:0.001048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001772, validLoss:0.001090, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001749, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001740, validLoss:0.001075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001717, validLoss:0.001009, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001714, validLoss:0.001032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001704, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001690, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001696, validLoss:0.000981, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001718, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001660, validLoss:0.001001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001669, validLoss:0.000886, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001719, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001721, validLoss:0.001022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001692, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001661, validLoss:0.000959, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001693, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001683, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001655, validLoss:0.000994, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001654, validLoss:0.001003, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.203871, g_loss:2.403275, d accuracy:0.729167, d AUC:0.948242, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:1, d_loss:0.876820, g_loss:3.057768, d accuracy:0.927083, d AUC:0.995117, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:2, d_loss:0.628346, g_loss:4.178633, d accuracy:0.864583, d AUC:0.919271, g accuracy:0.822917, rdf 0.000000\n",
      "Epoch:3, d_loss:0.408317, g_loss:5.911187, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.071979, g_loss:7.258469, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.051952, g_loss:8.306378, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.042489, g_loss:7.744906, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.031772, g_loss:9.488001, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.014088, g_loss:10.192314, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.014642, g_loss:8.867098, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.014756, g_loss:7.731953, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.040983, g_loss:6.361846, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:12, d_loss:0.155990, g_loss:3.458082, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13, d_loss:0.031205, g_loss:4.269137, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.037483, g_loss:3.715762, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.107600, g_loss:3.339211, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.012012, g_loss:6.152302, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.036906, g_loss:4.608939, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:18, d_loss:0.040327, g_loss:5.956780, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.261552, g_loss:4.132180, d accuracy:0.692708, d AUC:0.738607, g accuracy:0.479167, rdf 0.000000\n",
      "Epoch:20, d_loss:0.191659, g_loss:7.038641, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.054364, g_loss:5.991080, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.024532, g_loss:6.270839, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.033095, g_loss:4.887790, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.034727, g_loss:5.209542, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.011355, g_loss:7.396533, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.007858, g_loss:7.731786, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.006415, g_loss:7.740051, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.005769, g_loss:7.397498, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.006738, g_loss:6.531000, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.008911, g_loss:6.897362, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.007902, g_loss:7.908112, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.006578, g_loss:7.846017, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.006647, g_loss:7.613503, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.004444, g_loss:7.618933, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.003947, g_loss:7.712223, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.003480, g_loss:7.526417, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.003852, g_loss:7.317328, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.007196, g_loss:6.622607, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.008305, g_loss:5.936557, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.006965, g_loss:6.132130, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.006421, g_loss:7.001521, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.016104, g_loss:7.290586, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.012566, g_loss:7.094378, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.019893, g_loss:9.135977, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.004164, g_loss:10.627201, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.004084, g_loss:9.909621, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.003758, g_loss:9.879881, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.003830, g_loss:9.138493, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.004065, g_loss:7.936095, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.119071, validLoss:0.075078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.073568, validLoss:0.059689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.059112, validLoss:0.050590, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.049377, validLoss:0.043147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.043086, validLoss:0.037803, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.038954, validLoss:0.035720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.035996, validLoss:0.034020, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.034421, validLoss:0.032903, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.033515, validLoss:0.033271, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.032711, validLoss:0.030831, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.032517, validLoss:0.030658, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.032327, validLoss:0.029390, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.032157, validLoss:0.030274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.032154, validLoss:0.030170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.031869, validLoss:0.029098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.031792, validLoss:0.032195, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.031787, validLoss:0.027987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.031975, validLoss:0.030051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.031751, validLoss:0.031036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.031869, validLoss:0.030029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.031867, validLoss:0.032086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.031849, validLoss:0.028905, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.031852, validLoss:0.031022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.031860, validLoss:0.031021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.031879, validLoss:0.027911, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.031863, validLoss:0.029997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.031748, validLoss:0.032084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.031892, validLoss:0.030051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.031916, validLoss:0.031081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.031905, validLoss:0.029002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.032007, validLoss:0.031105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.032017, validLoss:0.030092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.031928, validLoss:0.028972, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.031922, validLoss:0.031076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.031820, validLoss:0.031095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.031942, validLoss:0.031106, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:36, trainLoss:0.032067, validLoss:0.029052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.031841, validLoss:0.031102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.031972, validLoss:0.030031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.031972, validLoss:0.030076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.031972, validLoss:0.029050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.032086, validLoss:0.027935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.032066, validLoss:0.032155, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.032004, validLoss:0.031072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.032003, validLoss:0.031115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.032098, validLoss:0.031153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.031988, validLoss:0.029018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.032113, validLoss:0.031163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.032049, validLoss:0.027983, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.031934, validLoss:0.031145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.032026, validLoss:0.029048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.032130, validLoss:0.029076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.032045, validLoss:0.032178, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.032035, validLoss:0.031138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.032041, validLoss:0.031140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.032065, validLoss:0.029034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.032076, validLoss:0.031287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.032162, validLoss:0.030137, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.032167, validLoss:0.032214, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.032041, validLoss:0.029065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.032160, validLoss:0.032206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.032043, validLoss:0.030129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.032172, validLoss:0.031152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.032185, validLoss:0.031177, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.032185, validLoss:0.031157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.032162, validLoss:0.031138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.031971, validLoss:0.028054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.032080, validLoss:0.031166, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.032172, validLoss:0.030114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.032068, validLoss:0.032176, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.031981, validLoss:0.030136, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.032191, validLoss:0.030126, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.032087, validLoss:0.028040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.032068, validLoss:0.029003, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.032182, validLoss:0.030096, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.032195, validLoss:0.029089, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.032082, validLoss:0.031165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.031994, validLoss:0.031116, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.032193, validLoss:0.032202, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.032035, validLoss:0.029126, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.032216, validLoss:0.029151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.032002, validLoss:0.031181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.032231, validLoss:0.031163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.032111, validLoss:0.031245, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.032112, validLoss:0.030175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.032101, validLoss:0.032236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.032087, validLoss:0.031150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.032196, validLoss:0.029145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.020332, validLoss:0.015883, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.013924, validLoss:0.009921, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.006044, validLoss:0.002247, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.002071, validLoss:0.001069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.001749, validLoss:0.001128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.001680, validLoss:0.001000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.001659, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.001681, validLoss:0.001036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.001670, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.001644, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.001667, validLoss:0.001024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.001667, validLoss:0.000992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.001688, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.001654, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.001649, validLoss:0.001034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.001646, validLoss:0.001026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.001674, validLoss:0.000975, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.001656, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.001666, validLoss:0.001055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.001658, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.001641, validLoss:0.000995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.001663, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.001655, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.001687, validLoss:0.001109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.001683, validLoss:0.001139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.001694, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.001675, validLoss:0.000982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.001668, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.001647, validLoss:0.000991, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.001652, validLoss:0.001092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.001646, validLoss:0.001023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.001683, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.001699, validLoss:0.000945, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.001648, validLoss:0.000982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.001665, validLoss:0.001065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.001727, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.001676, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.001669, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.001670, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.001654, validLoss:0.001082, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.001670, validLoss:0.001082, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.001677, validLoss:0.001047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.001652, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.001704, validLoss:0.001020, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.001680, validLoss:0.000982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.001631, validLoss:0.001042, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:134, trainLoss:0.001676, validLoss:0.000986, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.001667, validLoss:0.000997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.001659, validLoss:0.000992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001687, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001686, validLoss:0.000980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001650, validLoss:0.001072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001647, validLoss:0.000977, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001668, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001672, validLoss:0.000997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001525, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001657, validLoss:0.001081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001645, validLoss:0.001005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001643, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001649, validLoss:0.000997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001653, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001654, validLoss:0.001064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001670, validLoss:0.001091, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001689, validLoss:0.001158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001710, validLoss:0.001059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001671, validLoss:0.001007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001719, validLoss:0.001008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001656, validLoss:0.000923, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001649, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001681, validLoss:0.001068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001655, validLoss:0.001002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001665, validLoss:0.001017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001671, validLoss:0.000976, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001642, validLoss:0.000954, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001653, validLoss:0.001053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001695, validLoss:0.001229, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001688, validLoss:0.001064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001719, validLoss:0.001050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001656, validLoss:0.000961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001674, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001639, validLoss:0.001004, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001636, validLoss:0.001045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001617, validLoss:0.001033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001690, validLoss:0.001035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001709, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001664, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001651, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001700, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001698, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001668, validLoss:0.000966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001688, validLoss:0.000988, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001650, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001636, validLoss:0.001119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001684, validLoss:0.001032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001670, validLoss:0.001046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001662, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001676, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001669, validLoss:0.000984, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001645, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001652, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001637, validLoss:0.001050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001652, validLoss:0.000994, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001686, validLoss:0.001051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001623, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001638, validLoss:0.000896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001683, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001686, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001655, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001649, validLoss:0.000978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001683, validLoss:0.001112, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001668, validLoss:0.001001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001638, validLoss:0.001013, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.332019, g_loss:2.315115, d accuracy:0.520833, d AUC:0.945964, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.883028, g_loss:3.558849, d accuracy:0.984375, d AUC:0.997396, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:2, d_loss:0.427125, g_loss:5.417581, d accuracy:0.968750, d AUC:0.996094, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:3, d_loss:0.200661, g_loss:8.771381, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.120639, g_loss:10.463307, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.070807, g_loss:8.817461, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.034718, g_loss:9.118170, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:7, d_loss:0.029089, g_loss:9.950338, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.031723, g_loss:12.666502, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.021221, g_loss:17.227251, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.010272, g_loss:16.897358, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.005134, g_loss:12.163500, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.003843, g_loss:10.489808, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.003105, g_loss:9.741822, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.012955, g_loss:6.636804, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.020817, g_loss:7.673276, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.012789, g_loss:7.628613, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.007829, g_loss:7.844169, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.016825, g_loss:9.128256, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.028950, g_loss:13.542020, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.026099, g_loss:19.384665, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.021922, g_loss:14.546349, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:22, d_loss:0.029239, g_loss:9.513864, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.036312, g_loss:6.408212, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.138598, g_loss:4.336093, d accuracy:0.906250, d AUC:0.998372, g accuracy:0.812500, rdf 0.000000\n",
      "Epoch:25, d_loss:0.480068, g_loss:1.999789, d accuracy:0.916667, d AUC:0.989583, g accuracy:0.833333, rdf 0.000000\n",
      "Epoch:26, d_loss:0.257456, g_loss:1.786307, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:27, d_loss:0.157038, g_loss:2.290370, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.087601, g_loss:3.399665, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.101709, g_loss:3.283500, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.112344, g_loss:3.206440, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.123669, g_loss:4.043767, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:32, d_loss:0.033958, g_loss:8.861774, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:33, d_loss:0.140676, g_loss:4.469589, d accuracy:0.968750, d AUC:0.998698, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:34, d_loss:0.174579, g_loss:5.403718, d accuracy:0.989583, d AUC:0.999023, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:35, d_loss:0.246883, g_loss:4.635429, d accuracy:0.958333, d AUC:0.998698, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:36, d_loss:0.487083, g_loss:3.524101, d accuracy:0.963542, d AUC:0.991211, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:37, d_loss:0.244567, g_loss:5.082979, d accuracy:0.953125, d AUC:0.991536, g accuracy:0.906250, rdf 0.000000\n",
      "Epoch:38, d_loss:0.381971, g_loss:4.458210, d accuracy:0.927083, d AUC:0.964193, g accuracy:0.906250, rdf 0.000000\n",
      "Epoch:39, d_loss:0.371719, g_loss:4.999305, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:40, d_loss:0.202431, g_loss:5.808138, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:41, d_loss:0.363064, g_loss:3.849620, d accuracy:0.911458, d AUC:0.980143, g accuracy:0.843750, rdf 0.000000\n",
      "Epoch:42, d_loss:0.236924, g_loss:3.685543, d accuracy:0.968750, d AUC:0.996419, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:43, d_loss:0.346728, g_loss:2.898398, d accuracy:0.947917, d AUC:0.982422, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:44, d_loss:0.370905, g_loss:3.532361, d accuracy:0.875000, d AUC:0.958333, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:45, d_loss:0.400278, g_loss:2.571800, d accuracy:0.958333, d AUC:1.000000, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:46, d_loss:0.343222, g_loss:2.848272, d accuracy:0.989583, d AUC:0.999674, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:47, d_loss:0.422416, g_loss:2.903946, d accuracy:0.979167, d AUC:0.995768, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:48, d_loss:0.449382, g_loss:2.895692, d accuracy:0.973958, d AUC:1.000000, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:49, d_loss:0.440443, g_loss:2.651824, d accuracy:0.864583, d AUC:0.957357, g accuracy:0.875000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.160817, validLoss:0.102545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.084437, validLoss:0.069838, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.060061, validLoss:0.051467, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.043593, validLoss:0.037650, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.033413, validLoss:0.029194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.028170, validLoss:0.023933, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.025034, validLoss:0.022528, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.023455, validLoss:0.020603, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.022344, validLoss:0.020083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.021617, validLoss:0.019017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.021208, validLoss:0.019010, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.021021, validLoss:0.018121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.020873, validLoss:0.018095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.020761, validLoss:0.018667, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.020664, validLoss:0.018258, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.020387, validLoss:0.018669, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.020536, validLoss:0.018246, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.020505, validLoss:0.018411, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.020453, validLoss:0.018217, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.020432, validLoss:0.018306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.020331, validLoss:0.018487, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.020401, validLoss:0.017084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.020379, validLoss:0.017752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.020356, validLoss:0.017099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.020397, validLoss:0.017133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.020324, validLoss:0.018068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.020310, validLoss:0.018081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.020187, validLoss:0.017188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.020286, validLoss:0.017886, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.020209, validLoss:0.018098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.020226, validLoss:0.018206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.020148, validLoss:0.017804, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.020149, validLoss:0.017686, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.020075, validLoss:0.017949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.020098, validLoss:0.018250, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.020207, validLoss:0.017410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.020212, validLoss:0.018396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.019926, validLoss:0.018435, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.020080, validLoss:0.017614, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.020164, validLoss:0.017914, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.020181, validLoss:0.017619, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.020164, validLoss:0.017508, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.020067, validLoss:0.017962, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.020165, validLoss:0.018048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.020195, validLoss:0.017623, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.020059, validLoss:0.017528, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.020134, validLoss:0.017926, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.020188, validLoss:0.018371, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.020095, validLoss:0.017433, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.020169, validLoss:0.018172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.020178, validLoss:0.017423, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:51, trainLoss:0.020152, validLoss:0.018179, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.020123, validLoss:0.016839, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.020129, validLoss:0.017434, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.020101, validLoss:0.016667, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.020192, validLoss:0.017295, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.020158, validLoss:0.017239, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.020186, validLoss:0.018151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.020167, validLoss:0.017784, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.020141, validLoss:0.017749, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.020188, validLoss:0.017744, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.020078, validLoss:0.016522, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.020185, validLoss:0.017818, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.020214, validLoss:0.016868, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.020210, validLoss:0.018146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.020181, validLoss:0.017252, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.020225, validLoss:0.017152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.020198, validLoss:0.017766, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.020155, validLoss:0.018141, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.020204, validLoss:0.018152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.020194, validLoss:0.016927, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.020169, validLoss:0.018006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.020210, validLoss:0.016857, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.020118, validLoss:0.017160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.020140, validLoss:0.018159, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.020206, validLoss:0.017566, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.020202, validLoss:0.017770, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.020106, validLoss:0.017716, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.020094, validLoss:0.018102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.019926, validLoss:0.017946, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.020113, validLoss:0.017604, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.018308, validLoss:0.011134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.012634, validLoss:0.009584, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.011425, validLoss:0.008635, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.010372, validLoss:0.007867, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.008788, validLoss:0.006058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.007399, validLoss:0.005085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.006451, validLoss:0.004552, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.006107, validLoss:0.004811, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.006098, validLoss:0.004717, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.006114, validLoss:0.004398, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.006042, validLoss:0.004254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.006044, validLoss:0.004703, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.005958, validLoss:0.004397, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.005949, validLoss:0.004403, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.005976, validLoss:0.004261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.005964, validLoss:0.004440, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.005928, validLoss:0.004706, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.005935, validLoss:0.004603, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.005938, validLoss:0.004545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.005962, validLoss:0.004707, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.005885, validLoss:0.004586, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.005917, validLoss:0.004437, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.005910, validLoss:0.004411, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.005937, validLoss:0.004558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.005909, validLoss:0.004748, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.005897, validLoss:0.004420, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.005894, validLoss:0.004586, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.005845, validLoss:0.004462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.005875, validLoss:0.004439, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.005877, validLoss:0.004483, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.005920, validLoss:0.004631, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.005894, validLoss:0.003859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.005830, validLoss:0.004342, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.005845, validLoss:0.004389, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.005856, validLoss:0.004467, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.005774, validLoss:0.004558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.005831, validLoss:0.004539, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.005815, validLoss:0.004566, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.005824, validLoss:0.004647, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.005798, validLoss:0.003746, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.005800, validLoss:0.004343, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.005795, validLoss:0.003935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.005853, validLoss:0.004623, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.005792, validLoss:0.004529, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.005789, validLoss:0.004542, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.005794, validLoss:0.004566, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.005774, validLoss:0.004460, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.005778, validLoss:0.004645, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.005776, validLoss:0.004583, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.005753, validLoss:0.004321, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.005772, validLoss:0.003779, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.005735, validLoss:0.004487, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.005687, validLoss:0.004450, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.005732, validLoss:0.004283, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.005739, validLoss:0.004413, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.005730, validLoss:0.004373, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.005756, validLoss:0.004531, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.005759, validLoss:0.003932, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.005711, validLoss:0.004526, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.004906, validLoss:0.002833, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.003506, validLoss:0.002579, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.003298, validLoss:0.002400, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.002939, validLoss:0.002234, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.002927, validLoss:0.002047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.002685, validLoss:0.001842, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.002513, validLoss:0.001632, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.002287, validLoss:0.001466, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.002129, validLoss:0.001406, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.002043, validLoss:0.001386, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001962, validLoss:0.001297, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001926, validLoss:0.001318, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001905, validLoss:0.001205, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001832, validLoss:0.001138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001872, validLoss:0.001118, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:155, trainLoss:0.001789, validLoss:0.001045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001778, validLoss:0.001180, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001795, validLoss:0.001148, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001759, validLoss:0.001095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001770, validLoss:0.001109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001772, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001733, validLoss:0.000995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001746, validLoss:0.001140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001793, validLoss:0.001355, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001772, validLoss:0.001147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001797, validLoss:0.001105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001731, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001756, validLoss:0.001117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001712, validLoss:0.001059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001708, validLoss:0.001118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001681, validLoss:0.001136, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001763, validLoss:0.001103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001775, validLoss:0.001130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001730, validLoss:0.001146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001712, validLoss:0.001117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001765, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001754, validLoss:0.001113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001723, validLoss:0.000995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001743, validLoss:0.001040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001703, validLoss:0.001046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001690, validLoss:0.001167, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001742, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001714, validLoss:0.001118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001710, validLoss:0.001122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001722, validLoss:0.001111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001714, validLoss:0.001046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001682, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001700, validLoss:0.001096, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001680, validLoss:0.001105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001693, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001720, validLoss:0.001110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001661, validLoss:0.001070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001674, validLoss:0.000934, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001722, validLoss:0.001148, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001721, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001679, validLoss:0.001117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001682, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001710, validLoss:0.001191, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001702, validLoss:0.001071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001665, validLoss:0.001065, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.411247, g_loss:1.869958, d accuracy:0.500000, d AUC:0.975260, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.850454, g_loss:3.780666, d accuracy:0.942708, d AUC:0.978190, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:2, d_loss:0.718283, g_loss:5.735779, d accuracy:0.984375, d AUC:0.999349, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:3, d_loss:0.172324, g_loss:6.705893, d accuracy:0.989583, d AUC:0.998698, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:4, d_loss:0.115802, g_loss:8.098009, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.030348, g_loss:9.635864, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.050064, g_loss:13.758306, d accuracy:0.989583, d AUC:0.994792, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:7, d_loss:0.155590, g_loss:10.656735, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:8, d_loss:0.071762, g_loss:9.227631, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.038602, g_loss:8.885139, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.023418, g_loss:8.650752, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.013663, g_loss:8.866831, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.008301, g_loss:9.263061, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.006844, g_loss:8.479822, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.016059, g_loss:8.447377, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.004643, g_loss:9.459558, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.153321, g_loss:5.151328, d accuracy:0.750000, d AUC:1.000000, g accuracy:0.500000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.323559, g_loss:3.677515, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:18, d_loss:0.206045, g_loss:2.499361, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:19, d_loss:0.125338, g_loss:2.286556, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.105691, g_loss:2.716718, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.039594, g_loss:5.015543, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.038090, g_loss:5.388660, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.077269, g_loss:5.230282, d accuracy:0.994792, d AUC:0.999023, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:24, d_loss:0.176026, g_loss:5.867693, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:25, d_loss:0.078298, g_loss:5.610469, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.103129, g_loss:4.977678, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:27, d_loss:0.455514, g_loss:5.631363, d accuracy:0.968750, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.079707, g_loss:8.559305, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:29, d_loss:0.181609, g_loss:6.083404, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.045663, g_loss:6.912045, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.022920, g_loss:7.999986, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.025553, g_loss:6.576866, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.032611, g_loss:5.183335, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.057499, g_loss:4.198590, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.057022, g_loss:3.971407, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.037450, g_loss:4.280770, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.027142, g_loss:4.648025, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:38, d_loss:0.023798, g_loss:4.548015, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.095556, g_loss:2.935881, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.125469, g_loss:3.432694, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.167504, g_loss:4.390930, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.055037, g_loss:6.141980, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.052295, g_loss:5.841189, d accuracy:0.984375, d AUC:0.997070, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:44, d_loss:0.203619, g_loss:5.892564, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.012485, g_loss:8.704258, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.010868, g_loss:7.087222, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.021491, g_loss:4.945516, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.016012, g_loss:4.875660, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.012495, g_loss:5.019926, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n",
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "0    444\n",
      "1    234\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.210059, validLoss:0.173756, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.160400, validLoss:0.146919, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.138837, validLoss:0.132543, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.126328, validLoss:0.122849, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.118649, validLoss:0.113828, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.114150, validLoss:0.113913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.112001, validLoss:0.114598, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.110886, validLoss:0.111194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.109971, validLoss:0.112253, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.109437, validLoss:0.111991, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.108862, validLoss:0.113409, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.108561, validLoss:0.111559, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.108423, validLoss:0.112987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.108036, validLoss:0.111728, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.107992, validLoss:0.111470, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.107770, validLoss:0.110371, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.108132, validLoss:0.111361, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.107815, validLoss:0.112119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.108074, validLoss:0.111200, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.107752, validLoss:0.109558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.107847, validLoss:0.112030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.107815, validLoss:0.112327, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.107784, validLoss:0.110068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.107879, validLoss:0.113876, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.107854, validLoss:0.111001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.107563, validLoss:0.111119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.107985, validLoss:0.111890, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.107769, validLoss:0.110942, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.107477, validLoss:0.110824, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.107774, validLoss:0.109923, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.107669, validLoss:0.112813, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.107548, validLoss:0.108944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.107674, validLoss:0.111250, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.107455, validLoss:0.114440, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.107747, validLoss:0.111651, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.107641, validLoss:0.112825, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.107299, validLoss:0.109088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.107641, validLoss:0.111075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.107447, validLoss:0.110949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.107633, validLoss:0.110983, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.107541, validLoss:0.109996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.107609, validLoss:0.110160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.107442, validLoss:0.112603, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.107736, validLoss:0.111875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.107529, validLoss:0.110052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.107727, validLoss:0.109227, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.107555, validLoss:0.112295, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.107649, validLoss:0.112743, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.107647, validLoss:0.109271, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.107419, validLoss:0.112018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.107649, validLoss:0.112787, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.107557, validLoss:0.109034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.107626, validLoss:0.109102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.107429, validLoss:0.110169, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.107543, validLoss:0.110929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.107486, validLoss:0.111016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.107630, validLoss:0.112685, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.107634, validLoss:0.110701, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.107529, validLoss:0.110059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.107430, validLoss:0.110071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.107313, validLoss:0.112695, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:61, trainLoss:0.107453, validLoss:0.111875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.107554, validLoss:0.112678, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.107735, validLoss:0.112633, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.107415, validLoss:0.112638, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.107537, validLoss:0.109790, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.107628, validLoss:0.110700, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.107719, validLoss:0.110432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.107603, validLoss:0.108182, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.107652, validLoss:0.109050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.107617, validLoss:0.109672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.107516, validLoss:0.110042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.107408, validLoss:0.110619, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.107491, validLoss:0.110570, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.107553, validLoss:0.111997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.107418, validLoss:0.108320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.107439, validLoss:0.109965, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.107529, validLoss:0.111888, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.107274, validLoss:0.109971, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.107544, validLoss:0.109303, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.107539, validLoss:0.110048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.107517, validLoss:0.109904, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.107436, validLoss:0.109988, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.107446, validLoss:0.110379, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.107609, validLoss:0.110981, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.107582, validLoss:0.109103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.107503, validLoss:0.113903, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.107504, validLoss:0.110233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.107501, validLoss:0.109059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.107318, validLoss:0.112819, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.107621, validLoss:0.112397, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.107755, validLoss:0.111276, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.107294, validLoss:0.113026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.107488, validLoss:0.110969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.107647, validLoss:0.112549, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.107630, validLoss:0.113847, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.107600, validLoss:0.111829, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.107628, validLoss:0.113856, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.107623, validLoss:0.110063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.107734, validLoss:0.114547, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.107510, validLoss:0.110463, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.107534, validLoss:0.112133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.107603, validLoss:0.109279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.107623, validLoss:0.112927, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.107598, validLoss:0.108145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.107422, validLoss:0.110625, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.107599, validLoss:0.110229, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.107621, validLoss:0.109917, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.107579, validLoss:0.109972, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.107421, validLoss:0.108339, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.107447, validLoss:0.113599, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.107517, validLoss:0.111997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.107546, validLoss:0.109850, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.107507, validLoss:0.111625, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.107228, validLoss:0.112689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.107410, validLoss:0.110304, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.107495, validLoss:0.109172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.107515, validLoss:0.112788, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.107659, validLoss:0.112769, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.107601, validLoss:0.109993, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.107317, validLoss:0.111876, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.107428, validLoss:0.112695, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.107673, validLoss:0.108236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.107429, validLoss:0.111771, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.107415, validLoss:0.113706, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.107511, validLoss:0.112034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.107402, validLoss:0.110896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.107525, validLoss:0.110235, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.107729, validLoss:0.113121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.107402, validLoss:0.111557, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.107524, validLoss:0.111719, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.107494, validLoss:0.110693, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.107664, validLoss:0.113102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.107516, validLoss:0.109034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.107818, validLoss:0.109264, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.107631, validLoss:0.111752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.107532, validLoss:0.108575, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.107535, validLoss:0.111875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.107505, validLoss:0.108953, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.107425, validLoss:0.110814, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.107526, validLoss:0.110113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.107402, validLoss:0.109030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.107363, validLoss:0.109934, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.107491, validLoss:0.111043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.107483, validLoss:0.109007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.107298, validLoss:0.112926, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.107481, validLoss:0.109142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.107514, validLoss:0.112159, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.107474, validLoss:0.112677, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.107688, validLoss:0.109183, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.097471, validLoss:0.091875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.090317, validLoss:0.087951, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.089028, validLoss:0.089100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.087940, validLoss:0.086269, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.087281, validLoss:0.085392, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.086909, validLoss:0.086334, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.086911, validLoss:0.085233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.086823, validLoss:0.087996, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:158, trainLoss:0.086884, validLoss:0.085393, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.086807, validLoss:0.086098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.086657, validLoss:0.086367, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.086802, validLoss:0.087212, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.086779, validLoss:0.085243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.086772, validLoss:0.087970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.086803, validLoss:0.087966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.086785, validLoss:0.086347, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.086669, validLoss:0.087691, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.086554, validLoss:0.087151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.086717, validLoss:0.085353, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.086534, validLoss:0.089007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.086708, validLoss:0.086095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.086908, validLoss:0.085328, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.086636, validLoss:0.087969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.086550, validLoss:0.087161, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.086784, validLoss:0.088188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.078904, validLoss:0.041829, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.044421, validLoss:0.040703, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.041456, validLoss:0.038349, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.040515, validLoss:0.036695, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.040036, validLoss:0.037315, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.039562, validLoss:0.038784, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.038837, validLoss:0.033567, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.021572, validLoss:0.015748, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.013136, validLoss:0.010102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.007842, validLoss:0.005606, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.004418, validLoss:0.002702, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.002817, validLoss:0.001741, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.002190, validLoss:0.001431, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001982, validLoss:0.001234, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001913, validLoss:0.001250, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001809, validLoss:0.001200, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001794, validLoss:0.000961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001819, validLoss:0.001215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001806, validLoss:0.001164, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001749, validLoss:0.001162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001740, validLoss:0.001040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001769, validLoss:0.001157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001748, validLoss:0.001075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001705, validLoss:0.001091, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001701, validLoss:0.001075, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.900393, g_loss:2.596539, d accuracy:0.979167, d AUC:0.995117, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:1, d_loss:1.014418, g_loss:1.566117, d accuracy:0.984375, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:2, d_loss:1.244256, g_loss:0.917153, d accuracy:0.708333, d AUC:0.927734, g accuracy:0.416667, rdf 0.000000\n",
      "Epoch:3, d_loss:0.579639, g_loss:2.350338, d accuracy:0.927083, d AUC:1.000000, g accuracy:0.854167, rdf 0.000000\n",
      "Epoch:4, d_loss:0.356365, g_loss:3.600646, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.233155, g_loss:2.161952, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.138686, g_loss:2.691951, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.056862, g_loss:3.642401, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.339701, g_loss:1.799540, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.076833, g_loss:5.398310, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.052974, g_loss:3.601201, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.106196, g_loss:4.288127, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.038229, g_loss:6.231235, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.019708, g_loss:4.757843, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.014151, g_loss:5.356197, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.010185, g_loss:5.567491, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.010481, g_loss:5.391433, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.260092, g_loss:4.625381, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.022568, g_loss:6.831106, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.223918, g_loss:5.239211, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.023878, g_loss:7.359885, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.011734, g_loss:5.873091, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.010267, g_loss:5.727911, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.009067, g_loss:5.494582, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.014543, g_loss:5.247887, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:25, d_loss:0.040816, g_loss:6.968688, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.012456, g_loss:7.647265, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.021734, g_loss:6.851340, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.042666, g_loss:9.255542, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.007672, g_loss:8.786743, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.005620, g_loss:8.429669, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.112823, g_loss:8.438368, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.026076, g_loss:11.468008, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.008103, g_loss:10.636603, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.014826, g_loss:7.979300, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.012195, g_loss:9.018441, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.191840, g_loss:5.627739, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:37, d_loss:0.039816, g_loss:10.026303, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.015325, g_loss:8.644449, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.011313, g_loss:8.642809, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.030490, g_loss:7.477561, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:41, d_loss:0.094060, g_loss:10.027647, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.051583, g_loss:9.238246, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.033867, g_loss:7.483192, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.197182, g_loss:7.644075, d accuracy:0.927083, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.088679, g_loss:8.366141, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.027541, g_loss:9.809131, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.018995, g_loss:10.098682, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.021291, g_loss:9.462282, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.058430, g_loss:6.939050, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.185544, validLoss:0.121518, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.111380, validLoss:0.089210, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.085604, validLoss:0.069826, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.068830, validLoss:0.056071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.058138, validLoss:0.046160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.041439, validLoss:0.028547, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.026293, validLoss:0.016286, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.018173, validLoss:0.011182, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.013328, validLoss:0.006251, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.010688, validLoss:0.005445, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.009421, validLoss:0.004535, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.008947, validLoss:0.004184, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.008534, validLoss:0.003913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.008279, validLoss:0.003699, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.007977, validLoss:0.003569, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.007912, validLoss:0.003469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.007718, validLoss:0.002325, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.007737, validLoss:0.003303, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.007661, validLoss:0.003196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.007621, validLoss:0.003184, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.007582, validLoss:0.002091, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.007428, validLoss:0.003044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.007399, validLoss:0.003070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.007489, validLoss:0.003013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.007477, validLoss:0.003041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.007463, validLoss:0.003052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.007446, validLoss:0.003012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.007362, validLoss:0.003045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.007368, validLoss:0.003013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.007452, validLoss:0.003006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.007443, validLoss:0.003047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.007429, validLoss:0.003058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.007451, validLoss:0.002972, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.007440, validLoss:0.003015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.007326, validLoss:0.002997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.007445, validLoss:0.003012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.007452, validLoss:0.003038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.007193, validLoss:0.002995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.007359, validLoss:0.002995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.007463, validLoss:0.002996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.007462, validLoss:0.003046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.007468, validLoss:0.002974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.007424, validLoss:0.003013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.007267, validLoss:0.002973, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.007491, validLoss:0.003025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.007367, validLoss:0.003043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.007486, validLoss:0.001986, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.007492, validLoss:0.003085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.007535, validLoss:0.003011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.007528, validLoss:0.003064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.007514, validLoss:0.003054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.007523, validLoss:0.002046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.007531, validLoss:0.003033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.007525, validLoss:0.001989, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.007527, validLoss:0.003042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.007540, validLoss:0.003012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.007553, validLoss:0.003149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.007543, validLoss:0.003077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.007558, validLoss:0.003098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.007429, validLoss:0.003071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.007541, validLoss:0.003070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.007538, validLoss:0.003049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.007554, validLoss:0.003066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.007561, validLoss:0.003070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.007574, validLoss:0.003063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.007555, validLoss:0.003040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.007474, validLoss:0.003065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.007581, validLoss:0.002011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.007562, validLoss:0.003010, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.007570, validLoss:0.003048, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:70, trainLoss:0.007586, validLoss:0.003088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.007586, validLoss:0.003080, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.007586, validLoss:0.003058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.007574, validLoss:0.002997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.007470, validLoss:0.003051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.007587, validLoss:0.003071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.007586, validLoss:0.003083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.007606, validLoss:0.003018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.007560, validLoss:0.003072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.007543, validLoss:0.003103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.007619, validLoss:0.003159, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.007506, validLoss:0.003098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.007518, validLoss:0.003070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.007612, validLoss:0.003143, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.007612, validLoss:0.003135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.007618, validLoss:0.003095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.007600, validLoss:0.002029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.007602, validLoss:0.003140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.007610, validLoss:0.003098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.007612, validLoss:0.003130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.007619, validLoss:0.002058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.007622, validLoss:0.003069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.007628, validLoss:0.003155, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.007601, validLoss:0.003017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.007596, validLoss:0.003066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.007635, validLoss:0.003100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.007615, validLoss:0.003095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.007506, validLoss:0.003116, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.007503, validLoss:0.003028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.007618, validLoss:0.003066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.007636, validLoss:0.003150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.007617, validLoss:0.003054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.007616, validLoss:0.002068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.007613, validLoss:0.003083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.007635, validLoss:0.003028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.007616, validLoss:0.003152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.007636, validLoss:0.003151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.007518, validLoss:0.003128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.007616, validLoss:0.003080, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.007521, validLoss:0.003143, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.007631, validLoss:0.003121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.007436, validLoss:0.003153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.007653, validLoss:0.003239, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.007658, validLoss:0.003138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.007639, validLoss:0.003069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.007640, validLoss:0.003134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.007630, validLoss:0.003075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.007517, validLoss:0.003134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.007624, validLoss:0.003053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.007660, validLoss:0.003115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.007662, validLoss:0.003017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.007630, validLoss:0.003070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.007640, validLoss:0.003142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.007593, validLoss:0.003110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.007651, validLoss:0.002074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.007642, validLoss:0.003085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.007648, validLoss:0.003128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.007636, validLoss:0.003157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.007641, validLoss:0.003151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.007550, validLoss:0.003140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.007636, validLoss:0.003172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.007569, validLoss:0.003095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.007642, validLoss:0.003066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.007516, validLoss:0.003120, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.007647, validLoss:0.003076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.007650, validLoss:0.002063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.007529, validLoss:0.002048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.007658, validLoss:0.003171, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.007668, validLoss:0.003076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.007517, validLoss:0.003156, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.007526, validLoss:0.003049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.007535, validLoss:0.003133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.007649, validLoss:0.003084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.007482, validLoss:0.003082, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.007646, validLoss:0.003139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.007627, validLoss:0.003084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.007526, validLoss:0.003103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.007639, validLoss:0.003075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.007596, validLoss:0.003122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.007639, validLoss:0.002112, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.007653, validLoss:0.003170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.007676, validLoss:0.003237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.007697, validLoss:0.002110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.007650, validLoss:0.003082, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.007692, validLoss:0.003075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.007526, validLoss:0.003016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.007636, validLoss:0.003157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.007547, validLoss:0.003128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.007640, validLoss:0.003078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.007653, validLoss:0.003105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.007656, validLoss:0.003032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.007627, validLoss:0.003014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.007633, validLoss:0.003118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.007675, validLoss:0.003297, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.007668, validLoss:0.003152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.007702, validLoss:0.003096, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.007646, validLoss:0.003059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.007653, validLoss:0.003121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.007615, validLoss:0.002052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.007631, validLoss:0.003141, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.007610, validLoss:0.003112, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:171, trainLoss:0.007663, validLoss:0.003120, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.007574, validLoss:0.002109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.007654, validLoss:0.003145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.007632, validLoss:0.003132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.007686, validLoss:0.003135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.007675, validLoss:0.003129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.007658, validLoss:0.003017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.007568, validLoss:0.003058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.007631, validLoss:0.003050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.007516, validLoss:0.003204, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.007561, validLoss:0.003118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.007647, validLoss:0.003143, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.007533, validLoss:0.003129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.007647, validLoss:0.003131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.007662, validLoss:0.003061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.007634, validLoss:0.003104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.007499, validLoss:0.003095, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.007158, validLoss:0.003207, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.006613, validLoss:0.001987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.004692, validLoss:0.002128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.002416, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001647, validLoss:0.000875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001678, validLoss:0.001124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001727, validLoss:0.001088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001691, validLoss:0.001123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001655, validLoss:0.000986, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001689, validLoss:0.001132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001683, validLoss:0.001035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001648, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.918427, g_loss:2.713141, d accuracy:0.890625, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.678115, g_loss:2.281816, d accuracy:0.812500, d AUC:0.984701, g accuracy:0.625000, rdf 0.000000\n",
      "Epoch:2, d_loss:1.037357, g_loss:1.307610, d accuracy:0.671875, d AUC:0.751628, g accuracy:0.343750, rdf 0.000000\n",
      "Epoch:3, d_loss:0.781816, g_loss:3.145948, d accuracy:0.968750, d AUC:0.997721, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:4, d_loss:0.674363, g_loss:1.899684, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.369679, g_loss:3.610067, d accuracy:0.786458, d AUC:0.997721, g accuracy:0.572917, rdf 0.000000\n",
      "Epoch:6, d_loss:0.529674, g_loss:2.215801, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.208248, g_loss:2.298912, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.124027, g_loss:2.543060, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.067941, g_loss:4.352602, d accuracy:0.958333, d AUC:1.000000, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:10, d_loss:0.143071, g_loss:4.249091, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.030329, g_loss:4.844433, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.016563, g_loss:4.714672, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.014649, g_loss:4.888564, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.129976, g_loss:4.271047, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.045204, g_loss:7.592463, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.226804, g_loss:5.982163, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.013061, g_loss:8.206743, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.008486, g_loss:7.976891, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.006492, g_loss:8.160167, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.013261, g_loss:5.756036, d accuracy:0.958333, d AUC:1.000000, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:21, d_loss:0.138468, g_loss:7.209652, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.013181, g_loss:7.668643, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.009019, g_loss:6.394314, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.009941, g_loss:5.796589, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.007758, g_loss:7.071803, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.005494, g_loss:7.113642, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.004649, g_loss:7.180004, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.003859, g_loss:7.292829, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.003665, g_loss:7.301123, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.003122, g_loss:7.270167, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.003768, g_loss:7.141335, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.006311, g_loss:6.392235, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.005776, g_loss:9.048409, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.003546, g_loss:8.347316, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.002317, g_loss:7.551302, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.002266, g_loss:7.392004, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.002540, g_loss:7.287299, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.017006, g_loss:6.219636, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.009486, g_loss:9.967545, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.003774, g_loss:8.370013, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.002739, g_loss:7.253608, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.002348, g_loss:7.171622, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.003099, g_loss:6.855093, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.002495, g_loss:7.626267, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.002422, g_loss:7.326114, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.002153, g_loss:7.187809, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.002295, g_loss:7.079741, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.002430, g_loss:6.996173, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.002812, g_loss:6.843370, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.127769, validLoss:0.089642, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.084356, validLoss:0.069215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.066157, validLoss:0.057545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.054154, validLoss:0.048205, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.046131, validLoss:0.040774, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.041068, validLoss:0.037556, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.037636, validLoss:0.035532, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.035880, validLoss:0.034070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.034800, validLoss:0.034160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.033793, validLoss:0.031613, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.033455, validLoss:0.031211, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.033135, validLoss:0.029914, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.032873, validLoss:0.030794, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.032831, validLoss:0.030676, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.032490, validLoss:0.029590, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.032170, validLoss:0.026658, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.018616, validLoss:0.013973, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.011907, validLoss:0.008866, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.006945, validLoss:0.004223, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.003382, validLoss:0.001817, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.002105, validLoss:0.001286, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.001895, validLoss:0.001157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.001845, validLoss:0.001175, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.001840, validLoss:0.001146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.001832, validLoss:0.001168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.001808, validLoss:0.001133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.001793, validLoss:0.001096, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.001804, validLoss:0.001160, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.001804, validLoss:0.001136, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.001795, validLoss:0.001150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.001781, validLoss:0.001151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.001788, validLoss:0.001193, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.001797, validLoss:0.001096, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.001778, validLoss:0.001136, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.001794, validLoss:0.001144, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.001793, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.001795, validLoss:0.001154, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.001668, validLoss:0.001122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.001801, validLoss:0.001110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.001806, validLoss:0.001134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.001801, validLoss:0.001147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.001807, validLoss:0.001116, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.001775, validLoss:0.001129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.001831, validLoss:0.001112, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.001828, validLoss:0.001134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.001797, validLoss:0.001150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.001813, validLoss:0.001116, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.001810, validLoss:0.001197, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.001863, validLoss:0.001142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.001845, validLoss:0.001174, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.001828, validLoss:0.001154, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.001829, validLoss:0.001172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.001835, validLoss:0.001113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.001829, validLoss:0.001167, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.001841, validLoss:0.001140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.001851, validLoss:0.001134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.001845, validLoss:0.001286, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.001834, validLoss:0.001197, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.001840, validLoss:0.001170, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.001837, validLoss:0.001159, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.001846, validLoss:0.001163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.001828, validLoss:0.001173, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.001845, validLoss:0.001171, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.001856, validLoss:0.001188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.001858, validLoss:0.001166, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.001829, validLoss:0.001132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.001851, validLoss:0.001192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.001866, validLoss:0.001132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.001848, validLoss:0.001133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.001758, validLoss:0.000958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.001775, validLoss:0.000993, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.001708, validLoss:0.001007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.001691, validLoss:0.000976, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.001661, validLoss:0.000962, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.001660, validLoss:0.000966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.001676, validLoss:0.000965, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.001651, validLoss:0.000969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.001674, validLoss:0.000903, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.001657, validLoss:0.000945, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.001601, validLoss:0.000992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.001669, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.001680, validLoss:0.000971, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.001693, validLoss:0.000966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.001672, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.001671, validLoss:0.001036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.001677, validLoss:0.000985, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.001651, validLoss:0.000952, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.001652, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.001661, validLoss:0.000999, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:89, trainLoss:0.001666, validLoss:0.001015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.001661, validLoss:0.001041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.001677, validLoss:0.000945, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.001672, validLoss:0.001050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.001643, validLoss:0.000931, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.001633, validLoss:0.000966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.001677, validLoss:0.001009, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.001667, validLoss:0.000991, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.001646, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.001659, validLoss:0.000946, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.001655, validLoss:0.000956, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.001683, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.001645, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.001649, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.001644, validLoss:0.000974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.001674, validLoss:0.000944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.001645, validLoss:0.001047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.001661, validLoss:0.001029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.001654, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.001639, validLoss:0.000970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.001653, validLoss:0.001026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.001651, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.001683, validLoss:0.001040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.001677, validLoss:0.001118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.001693, validLoss:0.001033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.001666, validLoss:0.000952, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.001659, validLoss:0.001015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.001648, validLoss:0.000959, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.001647, validLoss:0.001041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.001642, validLoss:0.000978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.001680, validLoss:0.001015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.001692, validLoss:0.000917, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.001646, validLoss:0.000956, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.001656, validLoss:0.001019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.001720, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.001675, validLoss:0.001026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.001662, validLoss:0.000989, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.001668, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.001651, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.001665, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.001675, validLoss:0.001008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.001649, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.001686, validLoss:0.000977, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.001663, validLoss:0.000944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.001626, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.001672, validLoss:0.000958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.001657, validLoss:0.000961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.001641, validLoss:0.000968, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001673, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001678, validLoss:0.000953, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001641, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001637, validLoss:0.000933, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001653, validLoss:0.001020, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001661, validLoss:0.000964, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001513, validLoss:0.000969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001646, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001633, validLoss:0.000960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001631, validLoss:0.000985, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001635, validLoss:0.000948, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001633, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001634, validLoss:0.001017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001654, validLoss:0.001036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001681, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001704, validLoss:0.001013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001658, validLoss:0.000970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001693, validLoss:0.000960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001634, validLoss:0.000885, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001634, validLoss:0.001052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001675, validLoss:0.001052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001642, validLoss:0.000971, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001655, validLoss:0.000982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001655, validLoss:0.000920, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001634, validLoss:0.000912, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001637, validLoss:0.001004, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001676, validLoss:0.001162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001668, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001699, validLoss:0.001002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001637, validLoss:0.000930, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001659, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001623, validLoss:0.000950, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001625, validLoss:0.001010, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001603, validLoss:0.000991, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001674, validLoss:0.000986, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001688, validLoss:0.001032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001652, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001635, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001688, validLoss:0.001013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001678, validLoss:0.001016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001658, validLoss:0.000934, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001673, validLoss:0.000949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001638, validLoss:0.000935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001618, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001670, validLoss:0.000991, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001652, validLoss:0.001008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001646, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001648, validLoss:0.001006, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:185, trainLoss:0.001657, validLoss:0.000943, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001637, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001641, validLoss:0.001000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001630, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001628, validLoss:0.000950, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001672, validLoss:0.001017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001610, validLoss:0.000980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001624, validLoss:0.000857, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001670, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001672, validLoss:0.001001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001637, validLoss:0.001034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001633, validLoss:0.000960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001663, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001647, validLoss:0.000980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001621, validLoss:0.000974, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.076244, g_loss:1.924916, d accuracy:0.958333, d AUC:0.995117, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:1, d_loss:0.679335, g_loss:3.093617, d accuracy:0.979167, d AUC:0.994466, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:2, d_loss:0.627696, g_loss:4.158284, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:3, d_loss:0.189854, g_loss:4.941226, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:4, d_loss:0.123016, g_loss:6.428943, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.102202, g_loss:9.054437, d accuracy:0.989583, d AUC:0.999674, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:6, d_loss:0.071493, g_loss:8.080717, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:7, d_loss:0.026883, g_loss:7.815602, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.012149, g_loss:8.359785, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.006829, g_loss:8.714090, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.042800, g_loss:6.420826, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.171572, g_loss:2.316039, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.210233, g_loss:2.940864, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.117085, g_loss:6.537644, d accuracy:0.677083, d AUC:1.000000, g accuracy:0.354167, rdf 0.000000\n",
      "Epoch:14, d_loss:0.180397, g_loss:4.783116, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.067267, g_loss:3.927985, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.050381, g_loss:5.521575, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.009413, g_loss:5.973103, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.007832, g_loss:6.072217, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.153923, g_loss:4.957200, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.079030, g_loss:5.025674, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.030915, g_loss:5.336045, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.035685, g_loss:4.451705, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.162352, g_loss:5.855412, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.008567, g_loss:10.525295, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.045037, g_loss:5.839791, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.062755, g_loss:6.608819, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.023061, g_loss:8.135122, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.012665, g_loss:7.721227, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.009343, g_loss:7.731248, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.009045, g_loss:6.882562, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.007014, g_loss:6.486791, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.010507, g_loss:6.336234, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.124319, g_loss:4.661489, d accuracy:0.973958, d AUC:0.997070, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:34, d_loss:0.249481, g_loss:5.027847, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.084905, g_loss:6.959417, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.051483, g_loss:7.373496, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.040487, g_loss:6.709421, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:38, d_loss:0.207464, g_loss:7.043458, d accuracy:0.906250, d AUC:0.969727, g accuracy:0.822917, rdf 0.000000\n",
      "Epoch:39, d_loss:0.102906, g_loss:8.240553, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:40, d_loss:0.107665, g_loss:9.429742, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.038859, g_loss:8.623194, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.019440, g_loss:9.061951, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.092287, g_loss:7.213794, d accuracy:0.911458, d AUC:0.998047, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.111058, g_loss:7.906452, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.079185, g_loss:8.196951, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.104183, g_loss:8.563597, d accuracy:0.984375, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.160345, g_loss:5.794743, d accuracy:0.968750, d AUC:1.000000, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:48, d_loss:0.101969, g_loss:7.331090, d accuracy:0.963542, d AUC:0.992188, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:49, d_loss:0.168798, g_loss:6.513297, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n",
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "0    400\n",
      "1    278\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.214475, validLoss:0.156045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.143744, validLoss:0.129002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.123971, validLoss:0.112142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.111028, validLoss:0.103192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.102561, validLoss:0.095842, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.097391, validLoss:0.090728, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.094052, validLoss:0.089411, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.091902, validLoss:0.084679, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.090242, validLoss:0.085578, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.088864, validLoss:0.085868, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.088364, validLoss:0.085377, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.087891, validLoss:0.083968, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.087443, validLoss:0.083736, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.087092, validLoss:0.083622, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.087135, validLoss:0.083540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.086945, validLoss:0.082388, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.086958, validLoss:0.083336, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.086779, validLoss:0.083247, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.086625, validLoss:0.083238, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.086798, validLoss:0.082145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.086643, validLoss:0.084149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.086619, validLoss:0.083113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.086599, validLoss:0.086193, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.086809, validLoss:0.082043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.086684, validLoss:0.082040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.086662, validLoss:0.081997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.086468, validLoss:0.084114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.086473, validLoss:0.083081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.086565, validLoss:0.084106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.086664, validLoss:0.084131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.086657, validLoss:0.083102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.086673, validLoss:0.084076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.086668, validLoss:0.084111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.086375, validLoss:0.083062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.086139, validLoss:0.086230, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.085751, validLoss:0.082460, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.084715, validLoss:0.083879, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.083544, validLoss:0.081931, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.081898, validLoss:0.082363, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.080924, validLoss:0.080032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.080731, validLoss:0.080971, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.080818, validLoss:0.081060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.080760, validLoss:0.082037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.080746, validLoss:0.081018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.080738, validLoss:0.081055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.080529, validLoss:0.081009, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.080746, validLoss:0.077925, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.080670, validLoss:0.082048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.080784, validLoss:0.080026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.080759, validLoss:0.079990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.080646, validLoss:0.081048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.080791, validLoss:0.082084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.080786, validLoss:0.080013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.080785, validLoss:0.081042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.080798, validLoss:0.084139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.080798, validLoss:0.081172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.080692, validLoss:0.080055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.080805, validLoss:0.080041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.080780, validLoss:0.080016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.080785, validLoss:0.080034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.080785, validLoss:0.081079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.080800, validLoss:0.081075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.080812, validLoss:0.082097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.080810, validLoss:0.082093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.080908, validLoss:0.082078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.080822, validLoss:0.081061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.080921, validLoss:0.081054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.080695, validLoss:0.082084, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.080801, validLoss:0.082072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.080713, validLoss:0.082123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.080709, validLoss:0.083149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.080824, validLoss:0.080037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.080810, validLoss:0.082031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.080814, validLoss:0.084167, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.080839, validLoss:0.080023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.080819, validLoss:0.083150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.080727, validLoss:0.080999, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.080932, validLoss:0.080022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.080782, validLoss:0.082153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.080851, validLoss:0.082176, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.080855, validLoss:0.080040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.080854, validLoss:0.081059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.080743, validLoss:0.081165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.080740, validLoss:0.079045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.080835, validLoss:0.082128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.080727, validLoss:0.081043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.080839, validLoss:0.080075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.067614, validLoss:0.045976, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.041944, validLoss:0.042608, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.037921, validLoss:0.038000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.034726, validLoss:0.037320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.031440, validLoss:0.032233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.029571, validLoss:0.030440, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.028988, validLoss:0.030293, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.028701, validLoss:0.030292, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:95, trainLoss:0.028660, validLoss:0.029196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.028618, validLoss:0.030251, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.028648, validLoss:0.030198, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.028843, validLoss:0.029134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.028640, validLoss:0.029172, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.028594, validLoss:0.029118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.028713, validLoss:0.030181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.028694, validLoss:0.031215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.028725, validLoss:0.028001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.028707, validLoss:0.031274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.028713, validLoss:0.031254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.028599, validLoss:0.031208, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.028694, validLoss:0.030138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.028706, validLoss:0.031259, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.028810, validLoss:0.031210, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.028737, validLoss:0.030206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.028726, validLoss:0.031313, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.028733, validLoss:0.031240, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.028706, validLoss:0.031146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.028599, validLoss:0.031205, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.028801, validLoss:0.030092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.028803, validLoss:0.030201, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.028693, validLoss:0.031151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.028738, validLoss:0.029150, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.028734, validLoss:0.031097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.028576, validLoss:0.031139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.028592, validLoss:0.030185, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.028642, validLoss:0.031188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.028821, validLoss:0.030197, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.028811, validLoss:0.030113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.028704, validLoss:0.030158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.028695, validLoss:0.031217, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.028814, validLoss:0.030215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.028716, validLoss:0.028078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.028798, validLoss:0.031236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.028840, validLoss:0.030123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.028704, validLoss:0.031134, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.028782, validLoss:0.029120, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.028832, validLoss:0.032194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.028602, validLoss:0.030124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.028688, validLoss:0.031133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.028724, validLoss:0.029186, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.028730, validLoss:0.030103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.028695, validLoss:0.031212, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.028687, validLoss:0.031121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.028819, validLoss:0.029144, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.028823, validLoss:0.031173, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.028682, validLoss:0.030147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.028808, validLoss:0.029144, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.028682, validLoss:0.030108, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.028802, validLoss:0.030152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.028801, validLoss:0.031136, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.028690, validLoss:0.029100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.028688, validLoss:0.029142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.028708, validLoss:0.030206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.028722, validLoss:0.028188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.028734, validLoss:0.031217, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.028602, validLoss:0.030127, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.028745, validLoss:0.030100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.028682, validLoss:0.030051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.028687, validLoss:0.031216, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.028609, validLoss:0.031231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.028700, validLoss:0.029079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.028707, validLoss:0.031161, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.028714, validLoss:0.031103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.028800, validLoss:0.030040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.028587, validLoss:0.029106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.028729, validLoss:0.030317, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.028717, validLoss:0.030183, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.028749, validLoss:0.030101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.028680, validLoss:0.029093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.022214, validLoss:0.015019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.011465, validLoss:0.008260, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.006283, validLoss:0.005018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.003677, validLoss:0.002599, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.002403, validLoss:0.001514, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001987, validLoss:0.001217, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001761, validLoss:0.001133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001725, validLoss:0.001103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001755, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001732, validLoss:0.001097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001705, validLoss:0.000980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001735, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001690, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001659, validLoss:0.001129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001698, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001696, validLoss:0.001078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001690, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001683, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001689, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001665, validLoss:0.001067, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001683, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001664, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001665, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001693, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001638, validLoss:0.001040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001649, validLoss:0.000879, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:192, trainLoss:0.001691, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001695, validLoss:0.001037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001670, validLoss:0.001093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001654, validLoss:0.001002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001690, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001681, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001654, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001642, validLoss:0.001026, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.196506, g_loss:2.592893, d accuracy:0.760417, d AUC:0.975260, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:1, d_loss:0.742142, g_loss:3.295800, d accuracy:0.869792, d AUC:0.972331, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:2, d_loss:0.762390, g_loss:4.482926, d accuracy:0.817708, d AUC:0.975586, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:3, d_loss:0.349632, g_loss:5.766859, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.045595, g_loss:8.028339, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.023777, g_loss:7.636341, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.318777, g_loss:3.100758, d accuracy:0.656250, d AUC:0.995768, g accuracy:0.312500, rdf 0.000000\n",
      "Epoch:7, d_loss:0.427176, g_loss:1.623977, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.118504, g_loss:2.591273, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.135524, g_loss:2.331896, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.042226, g_loss:4.661393, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.251728, g_loss:2.785995, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.046780, g_loss:6.083184, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.024355, g_loss:4.606112, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.016166, g_loss:5.117407, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.052262, g_loss:3.881999, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.026363, g_loss:6.584247, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.011913, g_loss:5.875766, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.030736, g_loss:4.648562, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.308566, g_loss:4.158012, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.042696, g_loss:6.693385, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.016370, g_loss:7.805164, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.084570, g_loss:4.330977, d accuracy:0.937500, d AUC:1.000000, g accuracy:0.875000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.144831, g_loss:4.596431, d accuracy:0.963542, d AUC:0.996419, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:24, d_loss:0.161774, g_loss:5.508342, d accuracy:0.984375, d AUC:0.998698, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:25, d_loss:0.071066, g_loss:8.362411, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.014384, g_loss:7.411877, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.014785, g_loss:9.493482, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.012507, g_loss:7.644662, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.149330, g_loss:5.907540, d accuracy:0.916667, d AUC:0.984049, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:30, d_loss:0.077403, g_loss:8.336742, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.013764, g_loss:6.774578, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.013654, g_loss:5.773499, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.014018, g_loss:5.567307, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.089842, g_loss:6.833161, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.009859, g_loss:9.393421, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.022343, g_loss:5.858770, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.013504, g_loss:5.782795, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.455703, g_loss:3.705955, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:39, d_loss:0.072412, g_loss:6.481225, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.054730, g_loss:6.468636, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.038648, g_loss:6.113181, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.021239, g_loss:6.604535, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.027406, g_loss:6.952199, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:44, d_loss:0.061190, g_loss:7.494847, d accuracy:0.963542, d AUC:0.994141, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:45, d_loss:0.119851, g_loss:7.368282, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.037652, g_loss:7.625888, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.023253, g_loss:7.179235, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.021646, g_loss:6.132850, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.017573, g_loss:6.154628, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.174687, validLoss:0.115424, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.103653, validLoss:0.087218, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.067231, validLoss:0.043427, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.036618, validLoss:0.027524, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.023080, validLoss:0.016671, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.015124, validLoss:0.010402, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.010310, validLoss:0.007228, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.007616, validLoss:0.005313, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.005961, validLoss:0.003947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.004889, validLoss:0.003254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.004220, validLoss:0.002654, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.003790, validLoss:0.002361, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:12, trainLoss:0.003493, validLoss:0.002181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.003289, validLoss:0.002008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.003121, validLoss:0.001929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.002964, validLoss:0.001809, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.002914, validLoss:0.001732, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.002845, validLoss:0.001721, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.002793, validLoss:0.001592, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.002765, validLoss:0.001638, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.002739, validLoss:0.001606, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.002705, validLoss:0.001472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.002683, validLoss:0.001539, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.002677, validLoss:0.001458, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.002671, validLoss:0.001497, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.002646, validLoss:0.001436, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.002618, validLoss:0.001464, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.002653, validLoss:0.001463, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.002655, validLoss:0.001475, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.002653, validLoss:0.001418, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.002639, validLoss:0.001417, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.002633, validLoss:0.001487, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.002648, validLoss:0.001374, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.002646, validLoss:0.001463, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.002637, validLoss:0.001483, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.002652, validLoss:0.001420, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.002662, validLoss:0.001491, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.002530, validLoss:0.001493, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.002675, validLoss:0.001467, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.002656, validLoss:0.001465, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.002655, validLoss:0.001482, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.002669, validLoss:0.001235, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.002646, validLoss:0.001491, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.002681, validLoss:0.001469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.002670, validLoss:0.001474, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.002661, validLoss:0.001514, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.002687, validLoss:0.001485, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.002677, validLoss:0.001550, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.002706, validLoss:0.001476, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.002713, validLoss:0.001524, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.002698, validLoss:0.001525, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.002703, validLoss:0.001532, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.002716, validLoss:0.001526, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.002695, validLoss:0.001519, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.002662, validLoss:0.001529, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.002718, validLoss:0.001449, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.002717, validLoss:0.001516, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.002705, validLoss:0.001518, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.002708, validLoss:0.001499, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.002697, validLoss:0.001305, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.002705, validLoss:0.001508, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.002699, validLoss:0.001273, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.002720, validLoss:0.001567, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.002722, validLoss:0.001512, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.002724, validLoss:0.001505, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.002680, validLoss:0.001516, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.002729, validLoss:0.001545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.002729, validLoss:0.001519, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.002713, validLoss:0.001480, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.002396, validLoss:0.001183, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.002111, validLoss:0.001116, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.002041, validLoss:0.001111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.002058, validLoss:0.001233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.001977, validLoss:0.000958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.001979, validLoss:0.001013, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.001995, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.001960, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.001949, validLoss:0.001002, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.001947, validLoss:0.001064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.001883, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.001986, validLoss:0.001174, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.001959, validLoss:0.001070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.001976, validLoss:0.001055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.001948, validLoss:0.001186, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.001940, validLoss:0.001125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.001925, validLoss:0.001088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.001922, validLoss:0.001048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.001925, validLoss:0.001133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.001931, validLoss:0.001148, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.001943, validLoss:0.001168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.001946, validLoss:0.001121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.001938, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.001934, validLoss:0.001125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.001893, validLoss:0.001022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.001892, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.001912, validLoss:0.001090, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.001901, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.001899, validLoss:0.001211, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.001910, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.001891, validLoss:0.001066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.001896, validLoss:0.001151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.001861, validLoss:0.001075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.001865, validLoss:0.001069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.001858, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.001882, validLoss:0.001015, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.001852, validLoss:0.001151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.001862, validLoss:0.001137, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.001853, validLoss:0.001094, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.001843, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.001842, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.001837, validLoss:0.001072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.001871, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.001864, validLoss:0.001189, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.001856, validLoss:0.001125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.001828, validLoss:0.001004, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.001826, validLoss:0.001094, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.001811, validLoss:0.001000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.001809, validLoss:0.001111, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:118, trainLoss:0.001804, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.001839, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.001826, validLoss:0.000969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.001786, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.001792, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.001840, validLoss:0.001069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.001800, validLoss:0.001076, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.001790, validLoss:0.001012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.001783, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.001769, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.001771, validLoss:0.001149, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.001787, validLoss:0.001086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.001753, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.001792, validLoss:0.001029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.001760, validLoss:0.001004, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.001725, validLoss:0.001059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.001772, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.001767, validLoss:0.001022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.001738, validLoss:0.001001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001765, validLoss:0.001165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001779, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001733, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001731, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001745, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001751, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001601, validLoss:0.001041, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001729, validLoss:0.001097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001701, validLoss:0.001003, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001717, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001709, validLoss:0.001007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001707, validLoss:0.001042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001709, validLoss:0.001100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001730, validLoss:0.001109, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001741, validLoss:0.001190, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001756, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001723, validLoss:0.001032, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001758, validLoss:0.001009, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001695, validLoss:0.000960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001706, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001727, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001698, validLoss:0.001024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001712, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001718, validLoss:0.000952, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001682, validLoss:0.000952, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001691, validLoss:0.001040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001727, validLoss:0.001224, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001717, validLoss:0.001090, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001744, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001690, validLoss:0.000970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001724, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001677, validLoss:0.000992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001677, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001653, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001719, validLoss:0.001029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001735, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001691, validLoss:0.001073, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001690, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001747, validLoss:0.001052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001722, validLoss:0.001050, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001696, validLoss:0.000966, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001726, validLoss:0.000999, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001685, validLoss:0.000964, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001661, validLoss:0.001127, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001702, validLoss:0.001009, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001690, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001688, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001692, validLoss:0.001037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001692, validLoss:0.000978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001659, validLoss:0.001023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001683, validLoss:0.001072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001665, validLoss:0.001066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001669, validLoss:0.001000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001716, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001649, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001658, validLoss:0.000888, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001693, validLoss:0.001048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001698, validLoss:0.001018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001668, validLoss:0.001070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001660, validLoss:0.001001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001697, validLoss:0.001112, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001684, validLoss:0.001012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001653, validLoss:0.001018, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.939810, g_loss:2.120692, d accuracy:0.531250, d AUC:0.533529, g accuracy:0.208333, rdf 0.000000\n",
      "Epoch:1, d_loss:1.665910, g_loss:0.376019, d accuracy:0.500000, d AUC:0.997721, g accuracy:0.000000, rdf 0.000000\n",
      "Epoch:2, d_loss:1.067593, g_loss:0.498127, d accuracy:0.843750, d AUC:1.000000, g accuracy:0.687500, rdf 0.000000\n",
      "Epoch:3, d_loss:0.468382, g_loss:0.989552, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:4, d_loss:0.160764, g_loss:3.430035, d accuracy:0.979167, d AUC:1.000000, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:5, d_loss:0.363823, g_loss:2.708961, d accuracy:0.953125, d AUC:0.999674, g accuracy:0.906250, rdf 0.000000\n",
      "Epoch:6, d_loss:0.179449, g_loss:5.624553, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.140541, g_loss:4.951396, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.073411, g_loss:7.945384, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.013240, g_loss:9.082632, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.016311, g_loss:6.943381, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11, d_loss:0.277752, g_loss:3.888912, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.051004, g_loss:6.080431, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.044681, g_loss:6.296758, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.025945, g_loss:5.682420, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.034953, g_loss:4.048847, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.031706, g_loss:3.826725, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.033265, g_loss:4.818995, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.019607, g_loss:5.497569, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:19, d_loss:0.154767, g_loss:5.342290, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.060471, g_loss:8.071304, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.022769, g_loss:6.830688, d accuracy:0.968750, d AUC:0.989258, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:22, d_loss:0.546607, g_loss:5.583476, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.056046, g_loss:7.336488, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.047252, g_loss:7.486311, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.030731, g_loss:6.317243, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.030556, g_loss:6.464342, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.043275, g_loss:5.339098, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.088828, g_loss:4.031033, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.106446, g_loss:6.089654, d accuracy:0.932292, d AUC:0.964518, g accuracy:0.864583, rdf 0.000000\n",
      "Epoch:30, d_loss:0.360595, g_loss:4.954951, d accuracy:0.953125, d AUC:0.962565, g accuracy:0.906250, rdf 0.000000\n",
      "Epoch:31, d_loss:0.374314, g_loss:2.749127, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.084088, g_loss:6.808595, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:33, d_loss:0.100975, g_loss:6.072624, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.058956, g_loss:5.111347, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.048243, g_loss:5.659327, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.085374, g_loss:4.552652, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.050127, g_loss:4.571193, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.050108, g_loss:4.238215, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.045132, g_loss:4.390993, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.057446, g_loss:4.285006, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:41, d_loss:0.100835, g_loss:3.615978, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.058748, g_loss:3.666693, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.052857, g_loss:3.823620, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.341578, g_loss:2.685033, d accuracy:0.489583, d AUC:0.414714, g accuracy:0.677083, rdf 0.000000\n",
      "Epoch:45, d_loss:0.817595, g_loss:4.388705, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.456892, g_loss:5.407837, d accuracy:0.895833, d AUC:0.933919, g accuracy:0.895833, rdf 0.000000\n",
      "Epoch:47, d_loss:0.389747, g_loss:3.893496, d accuracy:0.635417, d AUC:0.740885, g accuracy:0.708333, rdf 0.000000\n",
      "Epoch:48, d_loss:0.622870, g_loss:4.303771, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.298634, g_loss:4.559323, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.208446, validLoss:0.150997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.137438, validLoss:0.119281, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.104171, validLoss:0.075321, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.056336, validLoss:0.037607, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.036183, validLoss:0.023949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.025017, validLoss:0.015709, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.018133, validLoss:0.009615, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.014348, validLoss:0.007893, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.012065, validLoss:0.005097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.010829, validLoss:0.005246, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.010056, validLoss:0.004650, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.009766, validLoss:0.004435, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.009490, validLoss:0.004287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.009328, validLoss:0.004105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.009082, validLoss:0.004090, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.009058, validLoss:0.003940, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.008933, validLoss:0.002900, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.009004, validLoss:0.003924, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.008954, validLoss:0.003822, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.008923, validLoss:0.003830, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.008906, validLoss:0.002757, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.008774, validLoss:0.003688, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.008753, validLoss:0.003761, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.008851, validLoss:0.003685, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.008849, validLoss:0.003739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.008847, validLoss:0.003667, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.008826, validLoss:0.003700, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.008733, validLoss:0.003707, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.008728, validLoss:0.003689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.008835, validLoss:0.003675, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.008829, validLoss:0.003654, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.008832, validLoss:0.003764, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.008832, validLoss:0.003621, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.008824, validLoss:0.003715, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.008727, validLoss:0.003758, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.008837, validLoss:0.003651, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:36, trainLoss:0.008843, validLoss:0.003730, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.008582, validLoss:0.003762, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.008742, validLoss:0.003705, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.008837, validLoss:0.003718, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.008840, validLoss:0.003720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.008856, validLoss:0.003518, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.008796, validLoss:0.003720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.008636, validLoss:0.003721, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.008857, validLoss:0.003732, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.008722, validLoss:0.003714, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.008854, validLoss:0.002693, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.008851, validLoss:0.003810, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.008879, validLoss:0.003752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.008876, validLoss:0.003775, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.008862, validLoss:0.003774, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.008867, validLoss:0.002758, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.008868, validLoss:0.003718, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.008850, validLoss:0.002755, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.008817, validLoss:0.003733, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.008860, validLoss:0.003715, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.008555, validLoss:0.003814, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.008328, validLoss:0.003401, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.008224, validLoss:0.003352, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.008044, validLoss:0.003369, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.008142, validLoss:0.003398, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.008118, validLoss:0.003414, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.008130, validLoss:0.003361, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.008136, validLoss:0.003339, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.008123, validLoss:0.003337, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.008097, validLoss:0.003274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.007995, validLoss:0.003369, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.008101, validLoss:0.002237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.008090, validLoss:0.003342, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.008097, validLoss:0.003228, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.008085, validLoss:0.003339, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.008093, validLoss:0.003340, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.008093, validLoss:0.003341, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.008078, validLoss:0.003238, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.007971, validLoss:0.003260, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.008109, validLoss:0.003321, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.008085, validLoss:0.003330, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.008090, validLoss:0.003243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.008029, validLoss:0.003324, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.007990, validLoss:0.003313, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.008082, validLoss:0.003325, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.007953, validLoss:0.003330, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.007980, validLoss:0.003298, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.008053, validLoss:0.003468, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.008056, validLoss:0.003410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.008051, validLoss:0.003337, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.008042, validLoss:0.002225, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.008034, validLoss:0.003430, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.008042, validLoss:0.003370, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.008044, validLoss:0.003419, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.008022, validLoss:0.002323, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.008024, validLoss:0.003306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.008044, validLoss:0.003402, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.007997, validLoss:0.003292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.008000, validLoss:0.003305, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.008010, validLoss:0.003370, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.008003, validLoss:0.003374, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.007896, validLoss:0.003462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.007890, validLoss:0.003335, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.007988, validLoss:0.003355, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.007999, validLoss:0.003403, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.007974, validLoss:0.003307, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.007959, validLoss:0.002320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.007972, validLoss:0.003360, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.007976, validLoss:0.003293, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.007953, validLoss:0.003414, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.007975, validLoss:0.003394, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.007851, validLoss:0.003341, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.007946, validLoss:0.003287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.007839, validLoss:0.003327, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.007951, validLoss:0.003341, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.007754, validLoss:0.003383, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.007975, validLoss:0.003472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.007946, validLoss:0.003329, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.007939, validLoss:0.003294, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.007937, validLoss:0.003321, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.007920, validLoss:0.003306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.007823, validLoss:0.003364, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.007915, validLoss:0.003290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.007954, validLoss:0.003373, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.007932, validLoss:0.003240, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.007921, validLoss:0.003284, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.007923, validLoss:0.003405, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.007836, validLoss:0.003351, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.007919, validLoss:0.002281, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.007916, validLoss:0.003320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.007916, validLoss:0.003328, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.007895, validLoss:0.003379, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.007907, validLoss:0.003428, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.007796, validLoss:0.003356, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.007894, validLoss:0.003350, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.007824, validLoss:0.003317, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.007897, validLoss:0.003261, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.007467, validLoss:0.003158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.006807, validLoss:0.002843, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.004928, validLoss:0.001686, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.002746, validLoss:0.001302, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001947, validLoss:0.001378, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001941, validLoss:0.001279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001931, validLoss:0.001331, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001914, validLoss:0.001200, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001923, validLoss:0.001332, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001937, validLoss:0.001206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001787, validLoss:0.001271, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:144, trainLoss:0.001908, validLoss:0.001332, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001893, validLoss:0.001243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001910, validLoss:0.001292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001895, validLoss:0.001234, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001901, validLoss:0.001274, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001805, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001743, validLoss:0.001162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001724, validLoss:0.001294, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001729, validLoss:0.001140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001731, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001761, validLoss:0.001119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001722, validLoss:0.001135, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001764, validLoss:0.001176, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001725, validLoss:0.001140, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001706, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001710, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001717, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001684, validLoss:0.001000, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001693, validLoss:0.001083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001728, validLoss:0.001279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001735, validLoss:0.001126, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001742, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001687, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001732, validLoss:0.001083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001682, validLoss:0.001017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001680, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001651, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001730, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001742, validLoss:0.001123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001699, validLoss:0.001118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001686, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001745, validLoss:0.001098, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001723, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001699, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001730, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001691, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001661, validLoss:0.001158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001702, validLoss:0.001086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001690, validLoss:0.001118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001699, validLoss:0.001108, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001691, validLoss:0.001091, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001694, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001662, validLoss:0.001083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001688, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001667, validLoss:0.001111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001673, validLoss:0.001045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001706, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001649, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001659, validLoss:0.000922, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001695, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001699, validLoss:0.001059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001666, validLoss:0.001089, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001659, validLoss:0.001024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001701, validLoss:0.001157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001689, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001659, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.451410, g_loss:1.995057, d accuracy:0.541667, d AUC:0.984375, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.928060, g_loss:3.727794, d accuracy:0.692708, d AUC:0.910482, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:2, d_loss:0.697631, g_loss:6.692851, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:3, d_loss:0.218820, g_loss:8.666048, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:4, d_loss:0.102560, g_loss:9.873228, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:5, d_loss:0.073483, g_loss:10.783546, d accuracy:0.994792, d AUC:0.993815, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:6, d_loss:0.058138, g_loss:9.649854, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.032950, g_loss:9.309317, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.037798, g_loss:10.651545, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.025152, g_loss:10.792834, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.009750, g_loss:10.656007, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.005225, g_loss:8.609306, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.010920, g_loss:7.367128, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.021488, g_loss:8.181318, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.025745, g_loss:7.672707, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.012815, g_loss:8.424700, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.007352, g_loss:9.643494, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.009346, g_loss:10.689933, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.007417, g_loss:8.789497, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.012369, g_loss:10.481028, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.015617, g_loss:9.657660, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:21, d_loss:0.032884, g_loss:7.456130, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:22, d_loss:0.130339, g_loss:5.843391, d accuracy:0.979167, d AUC:1.000000, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:23, d_loss:0.344793, g_loss:4.862580, d accuracy:0.921875, d AUC:0.982422, g accuracy:0.854167, rdf 0.000000\n",
      "Epoch:24, d_loss:0.235918, g_loss:3.442409, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.167269, g_loss:3.426927, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.088694, g_loss:3.633876, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:27, d_loss:0.107926, g_loss:3.270738, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.096242, g_loss:3.661760, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.097781, g_loss:3.968281, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:30, d_loss:0.085997, g_loss:3.753270, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31, d_loss:0.053913, g_loss:5.060071, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.039511, g_loss:5.692999, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:33, d_loss:0.144244, g_loss:4.515380, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.081604, g_loss:4.839553, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.133032, g_loss:4.889900, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.039601, g_loss:6.952650, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.030330, g_loss:6.279296, d accuracy:0.994792, d AUC:0.999674, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:38, d_loss:0.066862, g_loss:4.240707, d accuracy:0.947917, d AUC:0.980794, g accuracy:0.895833, rdf 0.000000\n",
      "Epoch:39, d_loss:0.282460, g_loss:1.798832, d accuracy:0.973958, d AUC:0.996094, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:40, d_loss:0.377241, g_loss:3.562194, d accuracy:0.937500, d AUC:0.987630, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:41, d_loss:0.464977, g_loss:4.460939, d accuracy:0.901042, d AUC:0.956055, g accuracy:0.875000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.977488, g_loss:3.532306, d accuracy:0.526042, d AUC:0.599935, g accuracy:0.687500, rdf 0.000000\n",
      "Epoch:43, d_loss:0.267625, g_loss:7.689073, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.078701, g_loss:7.604444, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.054223, g_loss:6.497790, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.067417, g_loss:7.014408, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.039315, g_loss:6.712729, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.047830, g_loss:7.526964, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:49, d_loss:0.199337, g_loss:5.401796, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n",
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "0    353\n",
      "1    325\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.222375, validLoss:0.143123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.125740, validLoss:0.108776, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.097286, validLoss:0.082410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.078447, validLoss:0.069792, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.067911, validLoss:0.062164, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.061571, validLoss:0.057940, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.057894, validLoss:0.056995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.055707, validLoss:0.053772, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.054168, validLoss:0.054502, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.053498, validLoss:0.054131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.053024, validLoss:0.053528, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.052736, validLoss:0.052449, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.052310, validLoss:0.053834, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.052199, validLoss:0.051382, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.052048, validLoss:0.052080, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.051964, validLoss:0.051983, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.051887, validLoss:0.051908, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.051732, validLoss:0.051815, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.051784, validLoss:0.051791, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.051656, validLoss:0.050959, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.051621, validLoss:0.051653, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.051593, validLoss:0.050896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.051661, validLoss:0.051618, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.051642, validLoss:0.052400, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.051708, validLoss:0.051653, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.051519, validLoss:0.051595, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.051737, validLoss:0.051621, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.051708, validLoss:0.051614, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.051535, validLoss:0.051616, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.051597, validLoss:0.051637, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.051703, validLoss:0.052425, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.051606, validLoss:0.051558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.051519, validLoss:0.050811, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.051527, validLoss:0.053944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.051612, validLoss:0.052405, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.051626, validLoss:0.052418, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.051410, validLoss:0.051617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.051630, validLoss:0.050808, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.051546, validLoss:0.051591, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.051625, validLoss:0.051615, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.051545, validLoss:0.050793, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.051610, validLoss:0.050838, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.051559, validLoss:0.052353, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.051641, validLoss:0.051634, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.051550, validLoss:0.051642, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.051641, validLoss:0.050828, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.051573, validLoss:0.050889, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.051682, validLoss:0.052390, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.051678, validLoss:0.051648, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.051671, validLoss:0.051640, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.051707, validLoss:0.052423, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.051608, validLoss:0.050877, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:52, trainLoss:0.051673, validLoss:0.051639, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.051604, validLoss:0.050866, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.051709, validLoss:0.051623, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.051629, validLoss:0.051796, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.051702, validLoss:0.052474, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.051702, validLoss:0.052456, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.051604, validLoss:0.050875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.051607, validLoss:0.051679, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.051603, validLoss:0.052452, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.051618, validLoss:0.052469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.051632, validLoss:0.052446, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.051715, validLoss:0.053221, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.051621, validLoss:0.052430, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.051639, validLoss:0.051691, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.051727, validLoss:0.052442, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.051712, validLoss:0.051939, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.051712, validLoss:0.050844, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.051633, validLoss:0.050902, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.051729, validLoss:0.052470, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.051731, validLoss:0.051676, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.051635, validLoss:0.052367, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.051723, validLoss:0.052433, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.051660, validLoss:0.051678, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.051651, validLoss:0.050889, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.051658, validLoss:0.051615, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.051647, validLoss:0.051693, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.051602, validLoss:0.051707, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.051684, validLoss:0.050959, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.051668, validLoss:0.051673, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.051761, validLoss:0.051669, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.051664, validLoss:0.051745, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.051665, validLoss:0.050944, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.051743, validLoss:0.051708, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.051813, validLoss:0.050875, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.051733, validLoss:0.052516, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.051747, validLoss:0.050935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.051752, validLoss:0.051732, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.051666, validLoss:0.052486, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.051758, validLoss:0.053225, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.051790, validLoss:0.050972, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.051643, validLoss:0.052700, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.051733, validLoss:0.051689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.051681, validLoss:0.053254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.051668, validLoss:0.052496, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.051743, validLoss:0.052534, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.051680, validLoss:0.053540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.051673, validLoss:0.050900, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.051769, validLoss:0.053298, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.051659, validLoss:0.051977, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.051665, validLoss:0.051689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.051746, validLoss:0.050920, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.051771, validLoss:0.053524, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.051749, validLoss:0.051769, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.051682, validLoss:0.052551, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.051757, validLoss:0.050935, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.051662, validLoss:0.051663, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.051754, validLoss:0.051759, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.051673, validLoss:0.050925, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.051704, validLoss:0.053322, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.051778, validLoss:0.051820, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.051702, validLoss:0.051740, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.051767, validLoss:0.052462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.051685, validLoss:0.052524, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.051677, validLoss:0.050900, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.051759, validLoss:0.051720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.051677, validLoss:0.053543, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.051717, validLoss:0.052557, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.051868, validLoss:0.050855, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.051675, validLoss:0.051680, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.051684, validLoss:0.052515, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.051729, validLoss:0.050919, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.051702, validLoss:0.052518, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.051689, validLoss:0.052493, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.051778, validLoss:0.051740, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.051676, validLoss:0.051753, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.051684, validLoss:0.051787, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.051783, validLoss:0.051751, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.051674, validLoss:0.053342, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.051808, validLoss:0.052463, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.051766, validLoss:0.052472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.051821, validLoss:0.051735, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.051793, validLoss:0.050929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.051894, validLoss:0.050931, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.051676, validLoss:0.052460, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.051791, validLoss:0.051036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.051713, validLoss:0.051696, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.051688, validLoss:0.051775, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.051676, validLoss:0.051672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.051797, validLoss:0.051778, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.051781, validLoss:0.050937, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.051568, validLoss:0.051710, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.051770, validLoss:0.051773, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.051752, validLoss:0.051680, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.051683, validLoss:0.051720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.051762, validLoss:0.050909, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.051675, validLoss:0.051717, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.051756, validLoss:0.053299, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.051856, validLoss:0.050989, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.051720, validLoss:0.051845, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.051744, validLoss:0.051825, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.051799, validLoss:0.051747, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.051726, validLoss:0.050913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.051754, validLoss:0.051652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.051679, validLoss:0.051742, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.051706, validLoss:0.051765, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.051759, validLoss:0.052495, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.051853, validLoss:0.050921, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.051781, validLoss:0.051647, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.051755, validLoss:0.051941, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:161, trainLoss:0.051688, validLoss:0.051733, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.051802, validLoss:0.051872, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.051781, validLoss:0.052537, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.051809, validLoss:0.053557, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.051682, validLoss:0.050882, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.051790, validLoss:0.053297, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.051675, validLoss:0.051685, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.051844, validLoss:0.050948, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.051654, validLoss:0.052532, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.051719, validLoss:0.051740, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.051813, validLoss:0.050964, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.051770, validLoss:0.052540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.051691, validLoss:0.051758, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.051829, validLoss:0.051739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.051806, validLoss:0.050946, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.051694, validLoss:0.051930, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.038171, validLoss:0.009111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.009142, validLoss:0.006500, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.006280, validLoss:0.004770, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.005646, validLoss:0.004191, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.004956, validLoss:0.003464, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.003851, validLoss:0.002356, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.002785, validLoss:0.001684, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.002127, validLoss:0.001296, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001844, validLoss:0.001193, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001769, validLoss:0.001168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001731, validLoss:0.001161, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001708, validLoss:0.001054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001732, validLoss:0.001114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001674, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001672, validLoss:0.000907, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001705, validLoss:0.001094, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001708, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001671, validLoss:0.001088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001659, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001698, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001690, validLoss:0.001051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001653, validLoss:0.001037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001646, validLoss:0.001015, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.999051, g_loss:2.257905, d accuracy:0.833333, d AUC:0.923503, g accuracy:0.885417, rdf 0.000000\n",
      "Epoch:1, d_loss:0.924352, g_loss:1.686092, d accuracy:0.718750, d AUC:0.986654, g accuracy:0.437500, rdf 0.000000\n",
      "Epoch:2, d_loss:0.727636, g_loss:1.964998, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:3, d_loss:0.728120, g_loss:1.979841, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.674032, g_loss:1.806590, d accuracy:0.859375, d AUC:1.000000, g accuracy:0.718750, rdf 0.000000\n",
      "Epoch:5, d_loss:0.462779, g_loss:2.648921, d accuracy:0.973958, d AUC:0.999674, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:6, d_loss:0.228313, g_loss:4.169979, d accuracy:0.854167, d AUC:1.000000, g accuracy:0.708333, rdf 0.000000\n",
      "Epoch:7, d_loss:0.635854, g_loss:2.328527, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.132330, g_loss:3.141984, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.065944, g_loss:3.334143, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.038654, g_loss:3.910183, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.119856, g_loss:3.010560, d accuracy:0.895833, d AUC:1.000000, g accuracy:0.791667, rdf 0.000000\n",
      "Epoch:12, d_loss:0.132727, g_loss:4.830049, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.025642, g_loss:4.935998, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.031757, g_loss:4.148244, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.014199, g_loss:6.661115, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.009046, g_loss:5.974936, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.008070, g_loss:5.400975, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.135194, g_loss:3.570113, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.066036, g_loss:10.394709, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.015737, g_loss:9.544975, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.006646, g_loss:7.610658, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.014595, g_loss:7.531163, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:23, d_loss:0.095948, g_loss:7.512889, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.029928, g_loss:8.074952, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.023200, g_loss:8.976880, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.009788, g_loss:9.851593, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.015002, g_loss:9.065669, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.012141, g_loss:7.740351, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.010773, g_loss:11.846148, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:30, d_loss:0.027602, g_loss:15.016691, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:31, d_loss:0.026696, g_loss:7.390939, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:32, d_loss:0.062485, g_loss:12.102845, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.036512, g_loss:11.853117, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.019736, g_loss:11.148448, d accuracy:0.968750, d AUC:0.999349, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:35, d_loss:0.040984, g_loss:12.402700, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.048066, g_loss:10.279842, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.095720, g_loss:7.373529, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.081639, g_loss:8.128181, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:39, d_loss:0.046257, g_loss:9.867999, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:40, d_loss:0.045798, g_loss:8.339420, d accuracy:0.984375, d AUC:0.999349, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:41, d_loss:0.107171, g_loss:7.303965, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.108114, g_loss:6.801538, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.082230, g_loss:6.122149, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:44, d_loss:0.086296, g_loss:6.381982, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.105915, g_loss:5.750861, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.134101, g_loss:5.092553, d accuracy:0.973958, d AUC:1.000000, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:47, d_loss:0.152070, g_loss:4.757587, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:48, d_loss:0.337023, g_loss:2.969578, d accuracy:0.963542, d AUC:0.995443, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.259753, g_loss:3.183656, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.206528, validLoss:0.138945, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.114147, validLoss:0.087479, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.082301, validLoss:0.068951, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.065151, validLoss:0.056317, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.055025, validLoss:0.047637, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.049101, validLoss:0.044826, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.044851, validLoss:0.041916, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.042637, validLoss:0.040535, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.041421, validLoss:0.039983, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.040437, validLoss:0.037903, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.040094, validLoss:0.037376, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.039774, validLoss:0.036266, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.039532, validLoss:0.037100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.039517, validLoss:0.037080, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.039142, validLoss:0.035518, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.039025, validLoss:0.039047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.039017, validLoss:0.034262, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.039183, validLoss:0.036646, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.038919, validLoss:0.037691, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.039010, validLoss:0.036778, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.038968, validLoss:0.038535, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.038964, validLoss:0.035053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.038943, validLoss:0.037739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.038909, validLoss:0.037405, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.038932, validLoss:0.034416, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.038905, validLoss:0.036291, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.038760, validLoss:0.038624, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.038909, validLoss:0.036254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.038908, validLoss:0.037597, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.038905, validLoss:0.034970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.039009, validLoss:0.037587, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.038995, validLoss:0.036543, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.038901, validLoss:0.035375, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.038890, validLoss:0.037455, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.038793, validLoss:0.037675, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.038904, validLoss:0.036921, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.039029, validLoss:0.035482, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.038792, validLoss:0.037586, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.038914, validLoss:0.036426, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.038927, validLoss:0.036101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.038879, validLoss:0.035541, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.039029, validLoss:0.033321, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.038996, validLoss:0.038665, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.038909, validLoss:0.037410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.038912, validLoss:0.037122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.038999, validLoss:0.037501, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.038925, validLoss:0.035327, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.039033, validLoss:0.037739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.038935, validLoss:0.034115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.038834, validLoss:0.037607, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.038936, validLoss:0.035649, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.039040, validLoss:0.035506, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.038935, validLoss:0.038398, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.038928, validLoss:0.037592, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.038918, validLoss:0.037348, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.038964, validLoss:0.034896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.038958, validLoss:0.037717, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.039045, validLoss:0.036680, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.039050, validLoss:0.038589, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.038911, validLoss:0.034525, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.039005, validLoss:0.038615, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.038939, validLoss:0.035662, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.039066, validLoss:0.037752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.039069, validLoss:0.037597, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.039080, validLoss:0.037331, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.038990, validLoss:0.037394, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.038837, validLoss:0.034315, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.038926, validLoss:0.037376, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.039048, validLoss:0.036298, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.038931, validLoss:0.038044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.038849, validLoss:0.036311, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.039067, validLoss:0.036222, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.038959, validLoss:0.034307, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.038918, validLoss:0.034472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.039004, validLoss:0.036186, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.039047, validLoss:0.035349, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.038956, validLoss:0.037192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.038823, validLoss:0.036416, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.039031, validLoss:0.038603, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.038870, validLoss:0.035496, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.039069, validLoss:0.035340, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.038860, validLoss:0.037702, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:82, trainLoss:0.039068, validLoss:0.037396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.038953, validLoss:0.037558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.038941, validLoss:0.036587, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.038935, validLoss:0.038720, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.038957, validLoss:0.037423, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.039050, validLoss:0.035553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.039070, validLoss:0.035320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.038845, validLoss:0.036661, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.038953, validLoss:0.036475, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.038923, validLoss:0.035439, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.038852, validLoss:0.035601, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.039051, validLoss:0.036267, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.038836, validLoss:0.037237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.039027, validLoss:0.035537, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.039040, validLoss:0.037757, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.026221, validLoss:0.021540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.017849, validLoss:0.011010, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.009644, validLoss:0.007180, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.008591, validLoss:0.007570, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.008469, validLoss:0.007142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.008526, validLoss:0.007551, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.008498, validLoss:0.007435, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.008530, validLoss:0.007290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.008462, validLoss:0.007433, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.008508, validLoss:0.007494, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.008504, validLoss:0.007277, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.008479, validLoss:0.007022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.008498, validLoss:0.007401, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.008496, validLoss:0.006995, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.008504, validLoss:0.007133, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.008487, validLoss:0.006707, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.008483, validLoss:0.007339, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.008510, validLoss:0.006896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.008464, validLoss:0.006833, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.008432, validLoss:0.006918, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.008496, validLoss:0.007537, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.008460, validLoss:0.007119, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.008523, validLoss:0.007267, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.008487, validLoss:0.006117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.008462, validLoss:0.007290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.008495, validLoss:0.006683, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.008534, validLoss:0.007081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.008496, validLoss:0.007558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.008413, validLoss:0.007029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.008453, validLoss:0.007340, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.008469, validLoss:0.007573, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.008496, validLoss:0.007465, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.008511, validLoss:0.007500, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.008487, validLoss:0.007367, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.006770, validLoss:0.002588, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.003883, validLoss:0.002586, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.003412, validLoss:0.002556, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.003303, validLoss:0.002410, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.003178, validLoss:0.002287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.002937, validLoss:0.002039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.002783, validLoss:0.001941, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.002561, validLoss:0.001688, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.002350, validLoss:0.001621, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.002168, validLoss:0.001362, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.002041, validLoss:0.001362, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001963, validLoss:0.001191, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001750, validLoss:0.001163, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001814, validLoss:0.001203, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001769, validLoss:0.001069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001755, validLoss:0.001097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001734, validLoss:0.001016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001724, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001715, validLoss:0.001087, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001721, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001734, validLoss:0.001200, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001751, validLoss:0.001105, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001723, validLoss:0.001024, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001741, validLoss:0.001005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001681, validLoss:0.000953, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001687, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001713, validLoss:0.001077, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001677, validLoss:0.001008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001691, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001697, validLoss:0.000938, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001665, validLoss:0.000925, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001669, validLoss:0.001006, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001707, validLoss:0.001190, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001696, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001718, validLoss:0.000993, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001672, validLoss:0.000968, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001703, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001665, validLoss:0.000980, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001664, validLoss:0.001016, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001645, validLoss:0.001030, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001703, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001715, validLoss:0.001029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001676, validLoss:0.001037, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001662, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001736, validLoss:0.001052, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001702, validLoss:0.001033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001678, validLoss:0.000948, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001705, validLoss:0.000955, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:179, trainLoss:0.001665, validLoss:0.000923, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001646, validLoss:0.001086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001689, validLoss:0.001003, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001677, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001675, validLoss:0.001020, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001676, validLoss:0.001019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001687, validLoss:0.000952, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001650, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001669, validLoss:0.001036, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001652, validLoss:0.001046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001649, validLoss:0.000964, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001693, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001650, validLoss:0.000996, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001649, validLoss:0.000848, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001676, validLoss:0.001023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001686, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001661, validLoss:0.001022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001648, validLoss:0.000961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001689, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001682, validLoss:0.001012, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001646, validLoss:0.001013, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:0.759227, g_loss:2.957772, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.604623, g_loss:2.593568, d accuracy:0.973958, d AUC:0.999349, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:2, d_loss:0.977405, g_loss:2.400215, d accuracy:0.427083, d AUC:0.425781, g accuracy:0.375000, rdf 0.000000\n",
      "Epoch:3, d_loss:0.608584, g_loss:3.785826, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.467712, g_loss:2.636742, d accuracy:0.562500, d AUC:0.957031, g accuracy:0.125000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.710842, g_loss:1.351675, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.455007, g_loss:1.183264, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:7, d_loss:0.152579, g_loss:3.884232, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:8, d_loss:0.150229, g_loss:4.764014, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.098216, g_loss:4.828340, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.282968, g_loss:3.710578, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.133412, g_loss:3.271935, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.064469, g_loss:4.226728, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.055406, g_loss:4.754708, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.022847, g_loss:5.677983, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.016916, g_loss:5.072893, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.018637, g_loss:5.549313, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.006081, g_loss:6.643855, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.008402, g_loss:5.350429, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.040327, g_loss:4.390621, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.063219, g_loss:5.835013, d accuracy:0.932292, d AUC:0.999023, g accuracy:0.864583, rdf 0.000000\n",
      "Epoch:21, d_loss:0.130440, g_loss:8.826157, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:22, d_loss:0.030582, g_loss:7.056090, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.027008, g_loss:6.384267, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.054742, g_loss:7.460466, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.019294, g_loss:8.243805, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:26, d_loss:0.153356, g_loss:10.536873, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:27, d_loss:0.018338, g_loss:7.784273, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.058018, g_loss:7.325912, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:29, d_loss:0.077890, g_loss:5.544981, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:30, d_loss:0.133669, g_loss:6.368560, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.060492, g_loss:4.818887, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.053203, g_loss:5.697029, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.076210, g_loss:4.799174, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:34, d_loss:0.077672, g_loss:3.876540, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.045972, g_loss:4.249543, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.038103, g_loss:4.405432, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.033401, g_loss:4.377775, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.035769, g_loss:4.038933, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.042760, g_loss:3.909713, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.033838, g_loss:4.416446, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.050461, g_loss:3.937770, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.042582, g_loss:4.496636, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.026688, g_loss:4.558532, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.158127, g_loss:3.435708, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.714595, g_loss:8.050459, d accuracy:0.661458, d AUC:0.789062, g accuracy:0.822917, rdf 0.000000\n",
      "Epoch:46, d_loss:0.552102, g_loss:6.368489, d accuracy:0.973958, d AUC:0.991536, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:47, d_loss:0.124138, g_loss:5.362485, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.103190, g_loss:7.371657, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.427278, g_loss:5.461101, d accuracy:0.963542, d AUC:0.998047, g accuracy:0.927083, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.199720, validLoss:0.149943, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.131981, validLoss:0.119405, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.110017, validLoss:0.103525, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.079021, validLoss:0.048338, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.039627, validLoss:0.023896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.023614, validLoss:0.018328, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.018747, validLoss:0.015100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.015699, validLoss:0.012226, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.013260, validLoss:0.010179, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.011396, validLoss:0.008473, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.010084, validLoss:0.007704, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.009077, validLoss:0.006781, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.008404, validLoss:0.006182, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.007966, validLoss:0.005847, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.007633, validLoss:0.005960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.007405, validLoss:0.005647, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.007291, validLoss:0.005630, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.007232, validLoss:0.005581, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.007135, validLoss:0.005429, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.007065, validLoss:0.005216, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.006712, validLoss:0.004970, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.006271, validLoss:0.004943, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.006115, validLoss:0.004588, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.006031, validLoss:0.004757, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.005946, validLoss:0.004609, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.005862, validLoss:0.004404, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.005819, validLoss:0.004510, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.005805, validLoss:0.004466, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.005770, validLoss:0.004635, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.005745, validLoss:0.004491, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.005716, validLoss:0.004475, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.005701, validLoss:0.004451, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.005666, validLoss:0.004094, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.005653, validLoss:0.004496, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.005668, validLoss:0.004492, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.005669, validLoss:0.004388, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.005672, validLoss:0.004399, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.005472, validLoss:0.004484, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.005667, validLoss:0.004400, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.005661, validLoss:0.004401, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.005663, validLoss:0.004454, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.005673, validLoss:0.003999, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.005645, validLoss:0.004446, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.005654, validLoss:0.004471, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.005674, validLoss:0.004652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.005645, validLoss:0.004452, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.005677, validLoss:0.004569, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.005663, validLoss:0.004412, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.005682, validLoss:0.004448, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.005690, validLoss:0.004429, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.005665, validLoss:0.004607, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.005662, validLoss:0.004652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.005684, validLoss:0.004432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.005675, validLoss:0.004670, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.005630, validLoss:0.004482, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.005677, validLoss:0.004641, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.005679, validLoss:0.004675, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.005674, validLoss:0.004569, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.005686, validLoss:0.004437, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.005696, validLoss:0.003941, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.005680, validLoss:0.004613, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.005677, validLoss:0.004021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.005708, validLoss:0.004530, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.005713, validLoss:0.004372, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.005710, validLoss:0.004500, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.005695, validLoss:0.004357, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.005713, validLoss:0.004258, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.005698, validLoss:0.004447, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.005712, validLoss:0.004569, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.005724, validLoss:0.004389, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.005723, validLoss:0.004243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.005714, validLoss:0.004630, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.005744, validLoss:0.004396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.005726, validLoss:0.003620, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.005734, validLoss:0.004153, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.005749, validLoss:0.004469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.005741, validLoss:0.004488, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.005761, validLoss:0.003894, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.005727, validLoss:0.004215, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.005578, validLoss:0.004685, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.005759, validLoss:0.004357, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.005766, validLoss:0.004374, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.005774, validLoss:0.004194, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.005764, validLoss:0.004309, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.005768, validLoss:0.004711, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.005751, validLoss:0.004517, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.005758, validLoss:0.004381, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.005737, validLoss:0.004480, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.005763, validLoss:0.004613, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.005775, validLoss:0.004649, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.005743, validLoss:0.004378, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.005747, validLoss:0.004285, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.005779, validLoss:0.004656, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.005734, validLoss:0.004455, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.005757, validLoss:0.004461, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.005774, validLoss:0.004280, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.005784, validLoss:0.004503, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.005767, validLoss:0.004723, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.005788, validLoss:0.004684, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.005783, validLoss:0.004633, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.005803, validLoss:0.004745, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:101, trainLoss:0.005727, validLoss:0.004630, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.005771, validLoss:0.004481, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.005782, validLoss:0.004441, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.005798, validLoss:0.004652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.005790, validLoss:0.004758, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.005770, validLoss:0.004430, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.005770, validLoss:0.004560, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.005739, validLoss:0.004502, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.005789, validLoss:0.004432, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.005792, validLoss:0.004551, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.005810, validLoss:0.004713, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.005804, validLoss:0.003876, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.005758, validLoss:0.004411, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.005784, validLoss:0.004449, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.005806, validLoss:0.004553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.005737, validLoss:0.004652, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.005798, validLoss:0.004590, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.005786, validLoss:0.004670, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.005790, validLoss:0.004689, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.005770, validLoss:0.003812, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.005798, validLoss:0.004438, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.005792, validLoss:0.004029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.005833, validLoss:0.004712, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.005800, validLoss:0.004585, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.005800, validLoss:0.004682, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.005813, validLoss:0.004646, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.005798, validLoss:0.004532, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.005818, validLoss:0.004701, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.005807, validLoss:0.004660, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.005804, validLoss:0.004381, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.005821, validLoss:0.003861, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.005799, validLoss:0.004542, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.005753, validLoss:0.004552, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.005811, validLoss:0.004380, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.005831, validLoss:0.004512, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.005814, validLoss:0.004493, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.005827, validLoss:0.004604, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.005850, validLoss:0.004078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.005823, validLoss:0.004600, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.005793, validLoss:0.004387, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.005797, validLoss:0.004634, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.005841, validLoss:0.004406, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.005641, validLoss:0.004528, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.005795, validLoss:0.004544, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.005747, validLoss:0.004663, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.005808, validLoss:0.003803, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.005806, validLoss:0.004357, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.005725, validLoss:0.004320, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.005808, validLoss:0.004635, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.005827, validLoss:0.004680, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.005812, validLoss:0.004486, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.005843, validLoss:0.004464, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.005845, validLoss:0.004515, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.005868, validLoss:0.004337, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.005729, validLoss:0.003729, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.005645, validLoss:0.004369, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.005745, validLoss:0.004377, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.005606, validLoss:0.004515, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.005629, validLoss:0.004523, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.005636, validLoss:0.003779, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.005622, validLoss:0.004382, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.005602, validLoss:0.003693, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.005610, validLoss:0.004595, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.005579, validLoss:0.004151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.005642, validLoss:0.004298, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.005611, validLoss:0.003390, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.005643, validLoss:0.004333, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.005616, validLoss:0.004429, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.005583, validLoss:0.004492, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.005601, validLoss:0.004227, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.005648, validLoss:0.004158, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.005663, validLoss:0.004455, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.005631, validLoss:0.004356, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.005620, validLoss:0.004196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.005672, validLoss:0.004362, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.005636, validLoss:0.004484, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.005578, validLoss:0.004294, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.005660, validLoss:0.004276, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.005630, validLoss:0.004291, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.005598, validLoss:0.004152, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.005096, validLoss:0.002955, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.003719, validLoss:0.002874, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.003371, validLoss:0.002716, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.003152, validLoss:0.002264, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.002919, validLoss:0.001947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.002519, validLoss:0.001625, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.002273, validLoss:0.001403, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001972, validLoss:0.001302, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001778, validLoss:0.001063, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001744, validLoss:0.001042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001647, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001640, validLoss:0.000879, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001667, validLoss:0.001070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001675, validLoss:0.000994, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001653, validLoss:0.001047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001621, validLoss:0.000958, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001667, validLoss:0.001068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001656, validLoss:0.001014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001627, validLoss:0.001018, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, d_loss:1.045927, g_loss:2.186377, d accuracy:0.864583, d AUC:0.947917, g accuracy:0.875000, rdf 0.000000\n",
      "Epoch:1, d_loss:1.052791, g_loss:1.579165, d accuracy:0.781250, d AUC:0.970052, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:2, d_loss:0.748573, g_loss:3.102814, d accuracy:0.822917, d AUC:0.871745, g accuracy:0.729167, rdf 0.000000\n",
      "Epoch:3, d_loss:0.347847, g_loss:4.680413, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.173770, g_loss:4.143151, d accuracy:0.765625, d AUC:0.996419, g accuracy:0.531250, rdf 0.000000\n",
      "Epoch:5, d_loss:0.855706, g_loss:1.691173, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.417279, g_loss:1.584004, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.190224, g_loss:2.072069, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.084789, g_loss:2.799406, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.056967, g_loss:3.262200, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.062925, g_loss:3.142241, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.022486, g_loss:4.893450, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.014208, g_loss:4.758201, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.011484, g_loss:4.899357, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.009717, g_loss:5.052551, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.008505, g_loss:5.213637, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.007206, g_loss:5.363652, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.005709, g_loss:5.514506, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.049670, g_loss:4.074300, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.005856, g_loss:9.753446, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.166484, g_loss:6.947036, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.044839, g_loss:7.966580, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.055303, g_loss:8.928517, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.022836, g_loss:7.751896, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.016999, g_loss:7.805600, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.009336, g_loss:7.410976, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.027025, g_loss:6.114586, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.051083, g_loss:6.671876, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.055360, g_loss:5.631144, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.026179, g_loss:6.412280, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.018471, g_loss:5.447480, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.021993, g_loss:4.849563, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.010224, g_loss:6.030873, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.011541, g_loss:5.156503, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.011710, g_loss:5.502450, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.006886, g_loss:6.118620, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.007202, g_loss:5.967339, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.031287, g_loss:5.275306, d accuracy:0.973958, d AUC:1.000000, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:38, d_loss:0.114494, g_loss:5.850690, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.102353, g_loss:7.822154, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.107716, g_loss:10.272573, d accuracy:0.958333, d AUC:0.993490, g accuracy:0.916667, rdf 0.000000\n",
      "Epoch:41, d_loss:0.101013, g_loss:8.918512, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.042363, g_loss:9.332937, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:43, d_loss:0.013932, g_loss:12.289059, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.008303, g_loss:11.037817, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.007488, g_loss:8.482965, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.010688, g_loss:6.897170, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:47, d_loss:0.060145, g_loss:4.627974, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.034186, g_loss:6.408814, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.011728, g_loss:6.086339, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n",
      "0      8\n",
      "1      4\n",
      "2      4\n",
      "3      8\n",
      "4      8\n",
      "      ..\n",
      "673    4\n",
      "674    8\n",
      "675    3\n",
      "676    8\n",
      "677    4\n",
      "Name: ethnicity, Length: 678, dtype: int32\n",
      "1    373\n",
      "0    305\n",
      "Name: approved, dtype: int64\n",
      "265 265 413 413\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.170986, validLoss:0.128008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.121164, validLoss:0.112421, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.107363, validLoss:0.103621, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.097957, validLoss:0.094243, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.090867, validLoss:0.086785, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.086120, validLoss:0.079988, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.083228, validLoss:0.082060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.081741, validLoss:0.081345, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.080676, validLoss:0.080161, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.080206, validLoss:0.080093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.080004, validLoss:0.077637, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.079835, validLoss:0.078573, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:12, trainLoss:0.079765, validLoss:0.077501, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.079527, validLoss:0.076241, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.079545, validLoss:0.079772, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.079548, validLoss:0.076114, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.079525, validLoss:0.077472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.079384, validLoss:0.079534, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.079364, validLoss:0.077346, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.079550, validLoss:0.079231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.079408, validLoss:0.076976, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.079297, validLoss:0.077196, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.079250, validLoss:0.080246, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.079392, validLoss:0.074078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.079364, validLoss:0.077862, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.079218, validLoss:0.079292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.079257, validLoss:0.077986, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.079367, validLoss:0.078426, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.079360, validLoss:0.077748, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.079356, validLoss:0.079299, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.079457, validLoss:0.077173, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.079417, validLoss:0.076793, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.079212, validLoss:0.078176, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.079353, validLoss:0.077373, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.079238, validLoss:0.079601, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.079481, validLoss:0.076085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.079284, validLoss:0.079352, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.079366, validLoss:0.077092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.079479, validLoss:0.077821, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.079324, validLoss:0.076257, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.079471, validLoss:0.074595, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.079556, validLoss:0.080416, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.079444, validLoss:0.078146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.079258, validLoss:0.079034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.079543, validLoss:0.079238, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.079261, validLoss:0.077192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.079372, validLoss:0.077292, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.079364, validLoss:0.074790, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.079273, validLoss:0.078254, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.079468, validLoss:0.076453, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.079453, validLoss:0.076396, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.079369, validLoss:0.081148, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.079355, validLoss:0.078493, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.079404, validLoss:0.079123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.079383, validLoss:0.077864, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.079370, validLoss:0.079626, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.079465, validLoss:0.077434, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.079465, validLoss:0.080234, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.079350, validLoss:0.075735, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.079422, validLoss:0.079458, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.079473, validLoss:0.076924, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.079389, validLoss:0.077409, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.079387, validLoss:0.079218, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.079397, validLoss:0.078064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.079521, validLoss:0.079043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.079267, validLoss:0.075812, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.079329, validLoss:0.079103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.078984, validLoss:0.073918, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.075218, validLoss:0.076646, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.074292, validLoss:0.074346, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.074391, validLoss:0.075484, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.073999, validLoss:0.069929, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.073850, validLoss:0.071862, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.073804, validLoss:0.074421, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.073413, validLoss:0.070328, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.073120, validLoss:0.073237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.072670, validLoss:0.072552, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.072954, validLoss:0.072808, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.072502, validLoss:0.071144, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.072753, validLoss:0.071886, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.072521, validLoss:0.071818, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.072624, validLoss:0.072678, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.072591, validLoss:0.071746, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.072593, validLoss:0.070059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.072569, validLoss:0.074060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.072458, validLoss:0.073927, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.072650, validLoss:0.068795, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.072783, validLoss:0.072042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.072567, validLoss:0.073124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.072665, validLoss:0.070749, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.072439, validLoss:0.071719, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.072354, validLoss:0.069985, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.072747, validLoss:0.071890, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.072542, validLoss:0.071867, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.072567, validLoss:0.069609, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.072570, validLoss:0.070870, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.072329, validLoss:0.071092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.072347, validLoss:0.073128, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.072666, validLoss:0.070997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.072481, validLoss:0.070074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.072501, validLoss:0.071031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.072326, validLoss:0.070847, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.072547, validLoss:0.071861, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.072467, validLoss:0.069981, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.072445, validLoss:0.073216, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.072533, validLoss:0.071865, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.072430, validLoss:0.072014, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.072401, validLoss:0.072989, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.072444, validLoss:0.073978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.072554, validLoss:0.074051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.072696, validLoss:0.070040, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.072466, validLoss:0.070277, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.065754, validLoss:0.049239, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.045667, validLoss:0.037533, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.038838, validLoss:0.035927, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.037233, validLoss:0.034475, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.036600, validLoss:0.034950, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.036397, validLoss:0.032724, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:118, trainLoss:0.036303, validLoss:0.034726, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.036157, validLoss:0.033874, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.036143, validLoss:0.033452, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.036131, validLoss:0.032994, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.036177, validLoss:0.033673, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.036113, validLoss:0.033556, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.036000, validLoss:0.033623, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.036239, validLoss:0.034663, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.036110, validLoss:0.032495, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.036019, validLoss:0.035744, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.036236, validLoss:0.034660, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.035999, validLoss:0.033369, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.036238, validLoss:0.031776, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.036096, validLoss:0.033551, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.036044, validLoss:0.034545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.036107, validLoss:0.035427, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.036230, validLoss:0.035589, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.036104, validLoss:0.031348, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.036125, validLoss:0.034577, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.028023, validLoss:0.021662, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.019910, validLoss:0.017379, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.014720, validLoss:0.009732, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.008304, validLoss:0.005173, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.006057, validLoss:0.004417, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.005547, validLoss:0.004428, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.005676, validLoss:0.004418, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.005618, validLoss:0.004510, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.005669, validLoss:0.003690, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.005684, validLoss:0.004223, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.005600, validLoss:0.004181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.005661, validLoss:0.004503, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.005679, validLoss:0.004517, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.005666, validLoss:0.004285, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.005723, validLoss:0.004288, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.005683, validLoss:0.004387, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.005735, validLoss:0.004185, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.005663, validLoss:0.003738, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.005661, validLoss:0.004449, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.005703, validLoss:0.004363, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.005630, validLoss:0.004504, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.005658, validLoss:0.004511, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.005653, validLoss:0.003809, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.005640, validLoss:0.004375, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.005619, validLoss:0.003719, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.005641, validLoss:0.004647, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.005282, validLoss:0.003767, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.003855, validLoss:0.002698, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.003361, validLoss:0.002306, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.003262, validLoss:0.002421, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.003134, validLoss:0.002302, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.002982, validLoss:0.002181, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.002753, validLoss:0.002066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.002623, validLoss:0.001686, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.002423, validLoss:0.001658, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.002219, validLoss:0.001483, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.002078, validLoss:0.001383, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.002046, validLoss:0.001326, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001941, validLoss:0.001253, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001861, validLoss:0.001117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001835, validLoss:0.001078, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001772, validLoss:0.001017, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001725, validLoss:0.001147, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001754, validLoss:0.001023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001733, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001712, validLoss:0.001066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001710, validLoss:0.001073, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001707, validLoss:0.001005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001681, validLoss:0.001057, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001698, validLoss:0.001085, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001678, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001666, validLoss:0.000997, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001703, validLoss:0.001071, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001662, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001651, validLoss:0.000874, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001701, validLoss:0.001097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001707, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001673, validLoss:0.001069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001652, validLoss:0.000979, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001686, validLoss:0.001092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001691, validLoss:0.001029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001652, validLoss:0.001031, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001638, validLoss:0.000997, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.475957, g_loss:2.331908, d accuracy:0.500000, d AUC:0.919271, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:1, d_loss:1.029256, g_loss:3.425711, d accuracy:0.781250, d AUC:0.996745, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:2, d_loss:0.421184, g_loss:5.060020, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:3, d_loss:0.167670, g_loss:6.781253, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:4, d_loss:0.143246, g_loss:8.827054, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:5, d_loss:0.116156, g_loss:11.362754, d accuracy:0.984375, d AUC:0.999349, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:6, d_loss:0.075739, g_loss:10.627333, d accuracy:0.984375, d AUC:0.989583, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:7, d_loss:0.064932, g_loss:14.909329, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.028825, g_loss:17.338074, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.019878, g_loss:15.487020, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10, d_loss:0.019290, g_loss:9.848271, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.021189, g_loss:7.418612, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.013227, g_loss:8.897989, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.009038, g_loss:10.865882, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.005445, g_loss:9.387035, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.005235, g_loss:8.479593, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.007961, g_loss:7.207644, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:17, d_loss:0.009758, g_loss:6.952896, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:18, d_loss:0.010696, g_loss:6.315267, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.043661, g_loss:9.656864, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.099681, g_loss:6.951900, d accuracy:0.984375, d AUC:0.999349, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:21, d_loss:0.724425, g_loss:1.619850, d accuracy:0.921875, d AUC:0.990560, g accuracy:0.895833, rdf 0.000000\n",
      "Epoch:22, d_loss:0.299487, g_loss:2.298226, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:23, d_loss:0.186685, g_loss:3.190405, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:24, d_loss:0.068437, g_loss:4.277840, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.037449, g_loss:4.112913, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.038688, g_loss:4.844302, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.077916, g_loss:5.263625, d accuracy:0.973958, d AUC:1.000000, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:28, d_loss:0.208067, g_loss:3.791775, d accuracy:0.989583, d AUC:0.999674, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:29, d_loss:0.275129, g_loss:3.536235, d accuracy:0.901042, d AUC:0.931641, g accuracy:0.812500, rdf 0.000000\n",
      "Epoch:30, d_loss:0.313916, g_loss:4.887759, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.352518, g_loss:4.777211, d accuracy:0.937500, d AUC:0.969401, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:32, d_loss:0.286606, g_loss:5.180418, d accuracy:0.906250, d AUC:0.939128, g accuracy:0.843750, rdf 0.000000\n",
      "Epoch:33, d_loss:0.445609, g_loss:5.005943, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:34, d_loss:0.139171, g_loss:6.177536, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:35, d_loss:0.163834, g_loss:6.204577, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.193316, g_loss:6.416691, d accuracy:0.979167, d AUC:0.995117, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:37, d_loss:0.146627, g_loss:5.945776, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:38, d_loss:0.249699, g_loss:3.757610, d accuracy:0.927083, d AUC:0.972656, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:39, d_loss:0.291318, g_loss:2.839471, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:40, d_loss:0.164806, g_loss:2.841608, d accuracy:0.984375, d AUC:0.999674, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:41, d_loss:0.256640, g_loss:2.476973, d accuracy:0.973958, d AUC:0.999349, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:42, d_loss:0.272835, g_loss:3.094123, d accuracy:0.973958, d AUC:0.995443, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:43, d_loss:0.366548, g_loss:3.237584, d accuracy:0.875000, d AUC:0.977865, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:44, d_loss:0.434401, g_loss:2.856818, d accuracy:0.973958, d AUC:0.998372, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:45, d_loss:0.317048, g_loss:2.559376, d accuracy:0.968750, d AUC:0.999349, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:46, d_loss:0.325556, g_loss:2.505366, d accuracy:0.984375, d AUC:0.996094, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:47, d_loss:0.313781, g_loss:3.118903, d accuracy:0.927083, d AUC:0.997396, g accuracy:0.864583, rdf 0.000000\n",
      "Epoch:48, d_loss:0.362406, g_loss:2.634914, d accuracy:0.953125, d AUC:0.991211, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:49, d_loss:0.339409, g_loss:3.092581, d accuracy:0.984375, d AUC:0.999674, g accuracy:0.989583, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.8545994065281899\n",
      "PRDC: recall 0.5663716814159292\n",
      "PRDC: density 0.7456973293768546\n",
      "PRDC: coverage 0.39233038348082594\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.230750, validLoss:0.158739, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.143762, validLoss:0.125389, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.100339, validLoss:0.083302, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.082005, validLoss:0.075206, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.074941, validLoss:0.070634, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.070545, validLoss:0.067683, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.066769, validLoss:0.062124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.064292, validLoss:0.062138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.062430, validLoss:0.059774, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.061104, validLoss:0.058838, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.060428, validLoss:0.058398, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.060155, validLoss:0.057987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.059854, validLoss:0.057859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.059780, validLoss:0.057713, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.059182, validLoss:0.056625, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.059253, validLoss:0.058738, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.058902, validLoss:0.053464, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.059384, validLoss:0.056572, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.059030, validLoss:0.059387, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.059191, validLoss:0.057395, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.059165, validLoss:0.056541, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.059010, validLoss:0.057122, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.058303, validLoss:0.059385, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.057118, validLoss:0.057815, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.055811, validLoss:0.054859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.054253, validLoss:0.056347, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.053152, validLoss:0.056269, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.053070, validLoss:0.055987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.053000, validLoss:0.056097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.052873, validLoss:0.054932, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.053048, validLoss:0.056127, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.052944, validLoss:0.055066, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.052952, validLoss:0.053001, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.052925, validLoss:0.056044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.052714, validLoss:0.055138, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.052934, validLoss:0.056984, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:36, trainLoss:0.044203, validLoss:0.042399, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.034665, validLoss:0.034131, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.028622, validLoss:0.029802, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.023707, validLoss:0.026113, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.022595, validLoss:0.025978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.022516, validLoss:0.026808, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.022499, validLoss:0.025934, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.022519, validLoss:0.026818, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.022502, validLoss:0.026834, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.022485, validLoss:0.025054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:46, trainLoss:0.022503, validLoss:0.024998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.022401, validLoss:0.025956, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.022528, validLoss:0.025908, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.022511, validLoss:0.023139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.022398, validLoss:0.025907, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.022507, validLoss:0.025915, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.022500, validLoss:0.026803, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.022496, validLoss:0.024092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.022508, validLoss:0.025896, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.022421, validLoss:0.025869, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.022414, validLoss:0.026053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.022505, validLoss:0.026852, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.022514, validLoss:0.025901, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.022499, validLoss:0.026808, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.022408, validLoss:0.024976, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.022390, validLoss:0.026797, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.022317, validLoss:0.024978, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.022426, validLoss:0.026836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.022521, validLoss:0.024968, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.022502, validLoss:0.026814, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.022517, validLoss:0.026845, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.022524, validLoss:0.025873, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.022503, validLoss:0.024021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.022520, validLoss:0.024939, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.022538, validLoss:0.026830, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.022528, validLoss:0.025905, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.022439, validLoss:0.024974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.022518, validLoss:0.025824, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.022515, validLoss:0.026800, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.022447, validLoss:0.025913, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.022541, validLoss:0.024038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.022430, validLoss:0.024899, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.022541, validLoss:0.026828, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.022379, validLoss:0.025947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.022559, validLoss:0.025049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.022554, validLoss:0.024982, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.022558, validLoss:0.025887, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.022449, validLoss:0.025953, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.022443, validLoss:0.025048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.022547, validLoss:0.025906, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.022430, validLoss:0.025870, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.022422, validLoss:0.025947, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.022526, validLoss:0.025909, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.022550, validLoss:0.025029, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.022449, validLoss:0.025925, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.022455, validLoss:0.026797, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.022547, validLoss:0.025968, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.022421, validLoss:0.025859, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.022523, validLoss:0.025890, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.022543, validLoss:0.025912, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.022528, validLoss:0.025908, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.021865, validLoss:0.011259, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.007310, validLoss:0.006318, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.005117, validLoss:0.004272, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.003936, validLoss:0.002955, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.003039, validLoss:0.002060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.002429, validLoss:0.001622, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.002102, validLoss:0.001381, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.001981, validLoss:0.001290, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.001892, validLoss:0.001272, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.001864, validLoss:0.001287, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.001835, validLoss:0.001242, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.001818, validLoss:0.001146, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.001820, validLoss:0.001233, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.001793, validLoss:0.001157, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.001815, validLoss:0.001222, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.001802, validLoss:0.001249, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.001796, validLoss:0.001164, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.001761, validLoss:0.001068, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.001761, validLoss:0.001142, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.001740, validLoss:0.001055, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.001738, validLoss:0.001132, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.001734, validLoss:0.001088, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.001770, validLoss:0.001139, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.001758, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.001725, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.001726, validLoss:0.001104, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.001768, validLoss:0.001073, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.001739, validLoss:0.001097, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.001731, validLoss:0.001092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.001738, validLoss:0.001101, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.001712, validLoss:0.001110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.001725, validLoss:0.001117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.001738, validLoss:0.001107, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.001713, validLoss:0.001145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.001758, validLoss:0.001034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.001721, validLoss:0.001004, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.001668, validLoss:0.001067, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:134, trainLoss:0.001731, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.001715, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.001696, validLoss:0.001019, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.001727, validLoss:0.001145, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.001730, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.001709, validLoss:0.001123, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.001708, validLoss:0.001007, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.001707, validLoss:0.001110, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.001713, validLoss:0.001033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.001580, validLoss:0.001035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:144, trainLoss:0.001698, validLoss:0.001102, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.001682, validLoss:0.001011, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.001684, validLoss:0.001046, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.001683, validLoss:0.001008, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.001692, validLoss:0.001045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.001686, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.001707, validLoss:0.001092, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.001719, validLoss:0.001184, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.001748, validLoss:0.001125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.001724, validLoss:0.001075, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.001738, validLoss:0.001023, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.001678, validLoss:0.000960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.001684, validLoss:0.001093, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.001718, validLoss:0.001086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.001680, validLoss:0.001035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.001697, validLoss:0.001034, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.001706, validLoss:0.000961, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.001670, validLoss:0.000975, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.001686, validLoss:0.001053, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.001722, validLoss:0.001212, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.001710, validLoss:0.001086, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.001737, validLoss:0.001043, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.001675, validLoss:0.000969, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.001698, validLoss:0.001047, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.001669, validLoss:0.000990, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.001670, validLoss:0.001039, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.001650, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.001720, validLoss:0.001060, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.001732, validLoss:0.001065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001690, validLoss:0.001074, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001685, validLoss:0.001058, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001741, validLoss:0.001059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001723, validLoss:0.001042, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001708, validLoss:0.000987, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001715, validLoss:0.001005, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001686, validLoss:0.000949, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001656, validLoss:0.001121, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001704, validLoss:0.001018, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001701, validLoss:0.001044, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001689, validLoss:0.001048, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001692, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001696, validLoss:0.000984, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001672, validLoss:0.001028, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001687, validLoss:0.001061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001659, validLoss:0.001062, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001659, validLoss:0.000998, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001704, validLoss:0.001070, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001660, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001659, validLoss:0.000892, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001696, validLoss:0.001065, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001709, validLoss:0.001026, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001674, validLoss:0.001027, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001653, validLoss:0.000979, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001693, validLoss:0.001115, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001690, validLoss:0.001025, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001652, validLoss:0.001019, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.398440, g_loss:2.028996, d accuracy:0.510417, d AUC:0.997070, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.903793, g_loss:3.035874, d accuracy:0.859375, d AUC:0.989583, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:2, d_loss:0.478551, g_loss:5.494063, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:3, d_loss:0.172095, g_loss:7.470659, d accuracy:0.979167, d AUC:0.998372, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:4, d_loss:0.183355, g_loss:9.318102, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:5, d_loss:0.086833, g_loss:11.274073, d accuracy:0.989583, d AUC:0.999674, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:6, d_loss:0.052231, g_loss:10.519382, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.097062, g_loss:10.503824, d accuracy:0.968750, d AUC:1.000000, g accuracy:0.937500, rdf 0.000000\n",
      "Epoch:8, d_loss:0.094951, g_loss:9.541107, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.053448, g_loss:8.284857, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.034209, g_loss:8.766761, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.014955, g_loss:10.361342, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.006810, g_loss:10.245657, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.016061, g_loss:9.178731, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.022031, g_loss:10.321046, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.007038, g_loss:8.407239, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.026856, g_loss:6.530226, d accuracy:0.963542, d AUC:1.000000, g accuracy:0.927083, rdf 0.000000\n",
      "Epoch:17, d_loss:0.364573, g_loss:2.815714, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:18, d_loss:0.189645, g_loss:2.963647, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:19, d_loss:0.089621, g_loss:3.180651, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.056479, g_loss:3.837968, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.171165, g_loss:3.148473, d accuracy:0.833333, d AUC:0.998698, g accuracy:0.666667, rdf 0.000000\n",
      "Epoch:22, d_loss:0.183355, g_loss:4.517444, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:23, d_loss:0.118515, g_loss:4.591879, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:24, d_loss:0.126427, g_loss:3.894455, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:25, d_loss:0.039732, g_loss:5.096885, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:26, d_loss:0.051746, g_loss:4.598079, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.028987, g_loss:6.279026, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.029386, g_loss:6.660316, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.141170, g_loss:6.159739, d accuracy:0.979167, d AUC:0.983073, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:30, d_loss:0.116761, g_loss:7.355930, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:31, d_loss:0.164257, g_loss:6.302264, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:32, d_loss:0.027733, g_loss:7.275012, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:33, d_loss:0.146839, g_loss:4.596588, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:34, d_loss:0.104059, g_loss:4.773438, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.061934, g_loss:5.786395, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:36, d_loss:0.184893, g_loss:4.080236, d accuracy:0.979167, d AUC:0.987956, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:37, d_loss:0.366044, g_loss:5.089974, d accuracy:0.812500, d AUC:0.882487, g accuracy:0.666667, rdf 0.000000\n",
      "Epoch:38, d_loss:0.217204, g_loss:7.550611, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:39, d_loss:0.043547, g_loss:5.889592, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:40, d_loss:0.039481, g_loss:6.844108, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:41, d_loss:0.091465, g_loss:6.068542, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:42, d_loss:0.187433, g_loss:4.023214, d accuracy:0.994792, d AUC:0.994792, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:43, d_loss:0.200212, g_loss:3.473270, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:44, d_loss:0.135898, g_loss:3.536573, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:45, d_loss:0.187726, g_loss:3.167817, d accuracy:0.963542, d AUC:0.998047, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:46, d_loss:0.210202, g_loss:3.338310, d accuracy:0.979167, d AUC:0.999349, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:47, d_loss:0.167892, g_loss:3.782022, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:48, d_loss:0.113548, g_loss:3.669186, d accuracy:0.984375, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.156043, g_loss:3.247948, d accuracy:0.937500, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "adult.npy\n",
      "Pretrain_Epoch:0, trainLoss:0.237013, validLoss:0.210879, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:1, trainLoss:0.203075, validLoss:0.188154, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:2, trainLoss:0.187804, validLoss:0.178201, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:3, trainLoss:0.178918, validLoss:0.173192, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:4, trainLoss:0.171200, validLoss:0.166382, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:5, trainLoss:0.166363, validLoss:0.162769, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:6, trainLoss:0.162774, validLoss:0.161484, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:7, trainLoss:0.160644, validLoss:0.159610, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:8, trainLoss:0.159594, validLoss:0.158162, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:9, trainLoss:0.158651, validLoss:0.155231, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:10, trainLoss:0.158398, validLoss:0.155904, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:11, trainLoss:0.158169, validLoss:0.156394, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:12, trainLoss:0.158011, validLoss:0.155738, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:13, trainLoss:0.158155, validLoss:0.155644, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:14, trainLoss:0.157579, validLoss:0.155629, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:15, trainLoss:0.157539, validLoss:0.158118, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:16, trainLoss:0.157430, validLoss:0.152863, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:17, trainLoss:0.157967, validLoss:0.157033, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:18, trainLoss:0.157562, validLoss:0.158125, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:19, trainLoss:0.157848, validLoss:0.156536, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:20, trainLoss:0.157613, validLoss:0.157469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:21, trainLoss:0.157709, validLoss:0.155989, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:22, trainLoss:0.157761, validLoss:0.159151, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:23, trainLoss:0.157754, validLoss:0.157495, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:24, trainLoss:0.157814, validLoss:0.154992, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:25, trainLoss:0.157750, validLoss:0.154925, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:26, trainLoss:0.157521, validLoss:0.159022, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:27, trainLoss:0.157937, validLoss:0.157069, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:28, trainLoss:0.157727, validLoss:0.156488, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:29, trainLoss:0.157664, validLoss:0.153897, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:30, trainLoss:0.157930, validLoss:0.155389, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:31, trainLoss:0.157876, validLoss:0.157561, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:32, trainLoss:0.157616, validLoss:0.153724, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:33, trainLoss:0.158048, validLoss:0.158565, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:34, trainLoss:0.157661, validLoss:0.159061, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:35, trainLoss:0.157846, validLoss:0.157003, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:36, trainLoss:0.157855, validLoss:0.156497, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:37, trainLoss:0.157596, validLoss:0.157425, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:38, trainLoss:0.157855, validLoss:0.157064, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:39, trainLoss:0.157620, validLoss:0.155458, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:40, trainLoss:0.157841, validLoss:0.154406, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:41, trainLoss:0.157849, validLoss:0.152647, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:42, trainLoss:0.157995, validLoss:0.158591, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:43, trainLoss:0.157640, validLoss:0.160187, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:44, trainLoss:0.157959, validLoss:0.157067, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:45, trainLoss:0.157835, validLoss:0.156963, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:46, trainLoss:0.157850, validLoss:0.153836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:47, trainLoss:0.158006, validLoss:0.158629, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:48, trainLoss:0.157769, validLoss:0.156499, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:49, trainLoss:0.157660, validLoss:0.155815, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:50, trainLoss:0.157584, validLoss:0.155464, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:51, trainLoss:0.157867, validLoss:0.157565, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:52, trainLoss:0.157861, validLoss:0.156035, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:53, trainLoss:0.157848, validLoss:0.156404, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:54, trainLoss:0.157602, validLoss:0.157549, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:55, trainLoss:0.157703, validLoss:0.156472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:56, trainLoss:0.157682, validLoss:0.157540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:57, trainLoss:0.157847, validLoss:0.156045, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:58, trainLoss:0.157842, validLoss:0.156472, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:59, trainLoss:0.157851, validLoss:0.153752, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:60, trainLoss:0.157908, validLoss:0.156960, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:61, trainLoss:0.157570, validLoss:0.156854, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:62, trainLoss:0.157857, validLoss:0.159083, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:63, trainLoss:0.157917, validLoss:0.159165, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:64, trainLoss:0.158090, validLoss:0.158003, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:65, trainLoss:0.157822, validLoss:0.159188, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:66, trainLoss:0.157644, validLoss:0.152934, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:67, trainLoss:0.157858, validLoss:0.156509, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:68, trainLoss:0.158078, validLoss:0.156370, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:69, trainLoss:0.153271, validLoss:0.141540, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:70, trainLoss:0.134342, validLoss:0.127617, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:71, trainLoss:0.126157, validLoss:0.124836, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:72, trainLoss:0.124564, validLoss:0.119530, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:73, trainLoss:0.123614, validLoss:0.121237, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:74, trainLoss:0.123245, validLoss:0.124054, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:75, trainLoss:0.123170, validLoss:0.120279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:76, trainLoss:0.122956, validLoss:0.124241, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:77, trainLoss:0.122853, validLoss:0.122449, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:78, trainLoss:0.121709, validLoss:0.109974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:79, trainLoss:0.108684, validLoss:0.108729, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:80, trainLoss:0.105002, validLoss:0.103143, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:81, trainLoss:0.099491, validLoss:0.096443, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:82, trainLoss:0.094248, validLoss:0.093447, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:83, trainLoss:0.081206, validLoss:0.058168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:84, trainLoss:0.059123, validLoss:0.053796, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:85, trainLoss:0.056023, validLoss:0.052059, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:86, trainLoss:0.055518, validLoss:0.051853, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:87, trainLoss:0.055312, validLoss:0.051804, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:88, trainLoss:0.055085, validLoss:0.051709, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:89, trainLoss:0.055053, validLoss:0.051644, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:90, trainLoss:0.054981, validLoss:0.053722, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:91, trainLoss:0.054943, validLoss:0.052556, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:92, trainLoss:0.055009, validLoss:0.051588, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:93, trainLoss:0.054902, validLoss:0.053611, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:94, trainLoss:0.054983, validLoss:0.052589, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:95, trainLoss:0.055100, validLoss:0.051523, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:96, trainLoss:0.054968, validLoss:0.051554, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:97, trainLoss:0.055067, validLoss:0.051531, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:98, trainLoss:0.054972, validLoss:0.053573, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:99, trainLoss:0.054855, validLoss:0.053572, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:100, trainLoss:0.054967, validLoss:0.052595, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:101, trainLoss:0.054820, validLoss:0.053605, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:102, trainLoss:0.054931, validLoss:0.051533, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:103, trainLoss:0.054931, validLoss:0.051512, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:104, trainLoss:0.054967, validLoss:0.054626, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:105, trainLoss:0.054803, validLoss:0.052524, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:106, trainLoss:0.054784, validLoss:0.052530, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:107, trainLoss:0.054896, validLoss:0.052553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:108, trainLoss:0.054897, validLoss:0.053568, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:109, trainLoss:0.054890, validLoss:0.051481, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:110, trainLoss:0.054799, validLoss:0.052461, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:111, trainLoss:0.054806, validLoss:0.052592, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:112, trainLoss:0.054930, validLoss:0.051374, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:113, trainLoss:0.054804, validLoss:0.052582, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:114, trainLoss:0.054802, validLoss:0.052462, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:115, trainLoss:0.054897, validLoss:0.052598, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:116, trainLoss:0.054763, validLoss:0.051441, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:117, trainLoss:0.054768, validLoss:0.053553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:118, trainLoss:0.054855, validLoss:0.053510, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:119, trainLoss:0.054892, validLoss:0.053583, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:120, trainLoss:0.054909, validLoss:0.051213, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:121, trainLoss:0.054886, validLoss:0.052490, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:122, trainLoss:0.054775, validLoss:0.051279, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:123, trainLoss:0.054938, validLoss:0.052497, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:124, trainLoss:0.054796, validLoss:0.052548, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:125, trainLoss:0.054854, validLoss:0.051476, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:126, trainLoss:0.054780, validLoss:0.051490, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:127, trainLoss:0.054746, validLoss:0.053596, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:128, trainLoss:0.054782, validLoss:0.051485, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:129, trainLoss:0.054903, validLoss:0.051494, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:130, trainLoss:0.054761, validLoss:0.051530, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:131, trainLoss:0.054798, validLoss:0.051236, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:132, trainLoss:0.054792, validLoss:0.052433, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:133, trainLoss:0.054741, validLoss:0.053545, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:134, trainLoss:0.054796, validLoss:0.051438, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:135, trainLoss:0.054766, validLoss:0.051469, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:136, trainLoss:0.054865, validLoss:0.052470, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:137, trainLoss:0.054896, validLoss:0.052549, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:138, trainLoss:0.054777, validLoss:0.052467, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:139, trainLoss:0.054874, validLoss:0.051390, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:140, trainLoss:0.054768, validLoss:0.052445, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:141, trainLoss:0.054777, validLoss:0.052563, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:142, trainLoss:0.053687, validLoss:0.029858, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:143, trainLoss:0.019128, validLoss:0.013082, validReverseLoss:0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain_Epoch:144, trainLoss:0.013596, validLoss:0.012732, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:145, trainLoss:0.011543, validLoss:0.009640, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:146, trainLoss:0.009722, validLoss:0.008379, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:147, trainLoss:0.007271, validLoss:0.005565, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:148, trainLoss:0.004895, validLoss:0.003117, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:149, trainLoss:0.003382, validLoss:0.002103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:150, trainLoss:0.002929, validLoss:0.001672, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:151, trainLoss:0.002804, validLoss:0.001693, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:152, trainLoss:0.002797, validLoss:0.001593, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:153, trainLoss:0.002740, validLoss:0.001575, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:154, trainLoss:0.002761, validLoss:0.001511, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:155, trainLoss:0.002729, validLoss:0.001294, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:156, trainLoss:0.002720, validLoss:0.001494, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:157, trainLoss:0.002726, validLoss:0.001563, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:158, trainLoss:0.002702, validLoss:0.001560, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:159, trainLoss:0.002732, validLoss:0.001581, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:160, trainLoss:0.002746, validLoss:0.001300, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:161, trainLoss:0.002696, validLoss:0.001416, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:162, trainLoss:0.002699, validLoss:0.001319, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:163, trainLoss:0.002724, validLoss:0.001544, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:164, trainLoss:0.002700, validLoss:0.001553, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:165, trainLoss:0.002723, validLoss:0.001558, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:166, trainLoss:0.002690, validLoss:0.001299, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:167, trainLoss:0.002697, validLoss:0.001562, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:168, trainLoss:0.002683, validLoss:0.001483, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:169, trainLoss:0.002684, validLoss:0.001572, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:170, trainLoss:0.002220, validLoss:0.001180, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:171, trainLoss:0.002092, validLoss:0.001189, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:172, trainLoss:0.002028, validLoss:0.001164, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:173, trainLoss:0.001970, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:174, trainLoss:0.001952, validLoss:0.001072, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:175, trainLoss:0.001995, validLoss:0.001124, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:176, trainLoss:0.001988, validLoss:0.001168, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:177, trainLoss:0.001951, validLoss:0.001021, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:178, trainLoss:0.001990, validLoss:0.001103, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:179, trainLoss:0.001940, validLoss:0.000974, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:180, trainLoss:0.001910, validLoss:0.001245, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:181, trainLoss:0.001993, validLoss:0.001049, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:182, trainLoss:0.001979, validLoss:0.001100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:183, trainLoss:0.001957, validLoss:0.001106, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:184, trainLoss:0.001943, validLoss:0.001081, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:185, trainLoss:0.001946, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:186, trainLoss:0.001907, validLoss:0.001100, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:187, trainLoss:0.001925, validLoss:0.001130, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:188, trainLoss:0.001905, validLoss:0.001129, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:189, trainLoss:0.001907, validLoss:0.001051, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:190, trainLoss:0.001936, validLoss:0.001111, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:191, trainLoss:0.001892, validLoss:0.001056, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:192, trainLoss:0.001888, validLoss:0.000927, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:193, trainLoss:0.001923, validLoss:0.001148, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:194, trainLoss:0.001942, validLoss:0.001057, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:195, trainLoss:0.001908, validLoss:0.001099, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:196, trainLoss:0.001876, validLoss:0.001038, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:197, trainLoss:0.001916, validLoss:0.001200, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:198, trainLoss:0.001915, validLoss:0.001079, validReverseLoss:0.000000\n",
      "Pretrain_Epoch:199, trainLoss:0.001881, validLoss:0.001067, validReverseLoss:0.000000\n",
      "Epoch:0, d_loss:1.221010, g_loss:2.375778, d accuracy:0.734375, d AUC:0.998372, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:1, d_loss:0.726644, g_loss:3.677927, d accuracy:0.953125, d AUC:0.988932, g accuracy:0.958333, rdf 0.000000\n",
      "Epoch:2, d_loss:0.390706, g_loss:5.478307, d accuracy:0.968750, d AUC:0.993490, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:3, d_loss:0.304423, g_loss:5.519562, d accuracy:0.953125, d AUC:0.956380, g accuracy:0.906250, rdf 0.000000\n",
      "Epoch:4, d_loss:0.299600, g_loss:7.456102, d accuracy:0.984375, d AUC:0.999674, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:5, d_loss:0.143257, g_loss:8.522251, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:6, d_loss:0.054718, g_loss:8.409334, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:7, d_loss:0.074067, g_loss:9.744103, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:8, d_loss:0.031641, g_loss:10.388699, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:9, d_loss:0.017946, g_loss:10.488464, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:10, d_loss:0.028782, g_loss:10.229040, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:11, d_loss:0.018142, g_loss:10.443649, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:12, d_loss:0.005031, g_loss:9.817348, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:13, d_loss:0.006787, g_loss:8.163347, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:14, d_loss:0.016178, g_loss:7.283085, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:15, d_loss:0.005781, g_loss:8.609037, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:16, d_loss:0.042024, g_loss:6.979151, d accuracy:0.994792, d AUC:1.000000, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:17, d_loss:0.060943, g_loss:4.749996, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:18, d_loss:0.170155, g_loss:2.635988, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:19, d_loss:0.151407, g_loss:2.899975, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:20, d_loss:0.094314, g_loss:4.456471, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:21, d_loss:0.053454, g_loss:4.335553, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:22, d_loss:0.033785, g_loss:5.318794, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:23, d_loss:0.063674, g_loss:4.925002, d accuracy:0.973958, d AUC:1.000000, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:24, d_loss:0.102542, g_loss:4.010091, d accuracy:0.989583, d AUC:1.000000, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:25, d_loss:0.086878, g_loss:6.820716, d accuracy:0.989583, d AUC:0.998372, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:26, d_loss:0.075189, g_loss:6.737625, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:27, d_loss:0.036918, g_loss:6.201643, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:28, d_loss:0.022085, g_loss:6.308276, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:29, d_loss:0.030675, g_loss:5.950699, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:30, d_loss:0.030039, g_loss:5.672143, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:31, d_loss:0.205807, g_loss:4.761577, d accuracy:0.875000, d AUC:0.941732, g accuracy:0.833333, rdf 0.000000\n",
      "Epoch:32, d_loss:0.325021, g_loss:4.299798, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:33, d_loss:0.407348, g_loss:3.614558, d accuracy:0.901042, d AUC:0.955404, g accuracy:0.802083, rdf 0.000000\n",
      "Epoch:34, d_loss:0.262942, g_loss:5.050725, d accuracy:0.989583, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:35, d_loss:0.211948, g_loss:5.368535, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:36, d_loss:0.127487, g_loss:5.317667, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:37, d_loss:0.103250, g_loss:5.108406, d accuracy:0.994792, d AUC:0.999674, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:38, d_loss:0.406062, g_loss:3.161747, d accuracy:0.776042, d AUC:0.842448, g accuracy:0.729167, rdf 0.000000\n",
      "Epoch:39, d_loss:0.251657, g_loss:5.866656, d accuracy:0.984375, d AUC:1.000000, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:40, d_loss:0.146371, g_loss:4.602602, d accuracy:0.973958, d AUC:0.999349, g accuracy:0.947917, rdf 0.000000\n",
      "Epoch:41, d_loss:0.169506, g_loss:4.071496, d accuracy:0.989583, d AUC:0.999023, g accuracy:0.979167, rdf 0.000000\n",
      "Epoch:42, d_loss:0.866665, g_loss:2.387045, d accuracy:0.140625, d AUC:0.020182, g accuracy:0.031250, rdf 0.000000\n",
      "Epoch:43, d_loss:1.365686, g_loss:2.559669, d accuracy:1.000000, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:44, d_loss:0.288743, g_loss:4.337140, d accuracy:0.984375, d AUC:0.997396, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:45, d_loss:0.291918, g_loss:4.330287, d accuracy:0.994792, d AUC:1.000000, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:46, d_loss:0.165474, g_loss:4.659062, d accuracy:0.984375, d AUC:0.998698, g accuracy:0.989583, rdf 0.000000\n",
      "Epoch:47, d_loss:0.223621, g_loss:4.465631, d accuracy:0.979167, d AUC:0.999349, g accuracy:0.968750, rdf 0.000000\n",
      "Epoch:48, d_loss:0.335724, g_loss:2.977779, d accuracy:0.942708, d AUC:0.996094, g accuracy:1.000000, rdf 0.000000\n",
      "Epoch:49, d_loss:0.326107, g_loss:2.639314, d accuracy:0.942708, d AUC:0.997070, g accuracy:0.906250, rdf 0.000000\n",
      "INFO:tensorflow:fair_unfair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:fair-49 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "fair-49\n",
      "INFO:tensorflow:Restoring parameters from fair-399\n",
      "burning in\n",
      "generating\n",
      "synth before: (672, 15) (672,)\n",
      "synth after: (672, 16)\n",
      "LOGGGING 265 265\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.5085106382978724\n",
      "PRDC: recall 0.28679245283018867\n",
      "PRDC: density 0.52\n",
      "PRDC: coverage 0.3471698113207547\n",
      "LOGGGING 413 413\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0]\n",
      "Computing neg\n",
      "(674, 16)\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6973293768545994\n",
      "PRDC: recall 0.0\n",
      "PRDC: density 0.6973293768545994\n",
      "PRDC: coverage 0.012106537530266344\n",
      "LOGGGING 678 678\n",
      "0.0\n",
      "nan\n",
      "Feature Importance =  [-0.033962264150943396, 0.0, -0.01327433628318584, -0.07924528301886792, 0.0, -0.030973451327433628, -0.018867924528301886, 0.0, -0.007374631268436578]\n",
      "#####################!OC model not defined !##################\n",
      "Computing metrics for no additional OneClass embedding\n",
      "Start computing P&R and D&C\n",
      "PRDC: precision 0.6201780415430267\n",
      "PRDC: recall 0.3938053097345133\n",
      "PRDC: density 0.5225519287833829\n",
      "PRDC: coverage 0.28761061946902655\n",
      "fairgan 0.0 0.0 -0.02 0.024 -1.0 0.0\n",
      "fairgan&$0.635\\pm0.11$&$0.246\\pm0.192$&$0.0\\pm0.0$&$-0.02\\pm0.024$&$0.777\\pm0.055$\\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "p_idx = 6\n",
    "p_attr = 'ethnicity'\n",
    "\n",
    "for p in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
    "#for p in [0, 0.2, 0.4]:\n",
    "    column_names = ['male', 'age', 'debt', 'married', 'bankcustomer', 'educationlevel', 'ethnicity', 'yearsemployed',\n",
    "               'priordefault', 'employed', 'creditscore', 'driverslicense', 'citizen', 'zip', 'income', 'approved']\n",
    "    data = pd.read_csv('data/crx.data', header=None,  names=column_names)\n",
    "    data.reset_index(drop=True, inplace=True) \n",
    "    data = data.dropna(how = 'all')\n",
    "\n",
    "    data = data[data.age != '?']\n",
    "    data.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    for feat in ['male', 'married','bankcustomer', 'educationlevel', 'ethnicity','priordefault', 'employed', 'driverslicense', 'citizen', 'zip', 'approved']:\n",
    "        data[feat] = preprocessing.LabelEncoder().fit_transform(data[feat])\n",
    "        \n",
    "    data['age'] = pd.to_numeric(data['age'],errors='coerce')\n",
    "\n",
    "    print(data['ethnicity'])\n",
    "\n",
    "    data.loc[data['ethnicity'] <= 4, 'ethnicity'] = 0\n",
    "    data.loc[data['ethnicity'] > 4, 'ethnicity']= 1\n",
    "\n",
    "\n",
    "    data.loc[data['ethnicity'] ==1 , 'employed'] =  1\n",
    "\n",
    "    biased_data = data.copy()\n",
    "\n",
    "    bias = p\n",
    "    biased_data.loc[biased_data['ethnicity'] == 1, 'approved'] = np.logical_and(biased_data.loc[biased_data['ethnicity'] == 1, 'approved'].values, np.random.binomial(1, bias, len(biased_data.loc[biased_data['ethnicity'] == 1, 'approved']))).astype(int)\n",
    "        \n",
    "    print(biased_data['approved'].value_counts())\n",
    "    thresh = 0.8\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data[data.columns] = scaler.fit_transform(data)\n",
    "    biased_data[biased_data.columns] = scaler.transform(biased_data)\n",
    "    \n",
    "    import pickle \n",
    "    p_idx = 6\n",
    "    p_attr = 'ethnicity'\n",
    "\n",
    "    view_stats_new(['fairgan'], biased_data, protected = p_attr, remove_protected = False,\n",
    "               orig_data = data ,protected_idx = p_idx, bias_dict ={})\n",
    "\n",
    "    #view_stats_new(['gan', 'wgan', 'adsgan'], biased_data, protected = p_attr, remove_protected = False,\n",
    "    #           orig_data = data ,protected_idx = p_idx, bias_dict ={})\n",
    "    \n",
    "#     view_stats_new(['DECAF'], biased_data, protected = p_attr, remove_protected = False,\n",
    "#            orig_data = data ,protected_idx = p_idx, bias_dict ={})\n",
    "\n",
    "#     view_stats_new(['DECAF-FTU1'], biased_data, protected = p_attr, remove_protected = False,\n",
    "#                orig_data = data ,protected_idx = p_idx, bias_dict ={15:[6]})\n",
    "    \n",
    "#     view_stats_new(['DECAF-FTU2'], biased_data, protected = p_attr, remove_protected = False,\n",
    "#                orig_data = data ,protected_idx = p_idx, bias_dict ={15:[6]}, surrogate = True,)\n",
    "\n",
    "#     view_stats_new(['DECAF-DP'], biased_data, protected = p_attr, remove_protected = False,\n",
    "#                orig_data = data ,protected_idx = p_idx, bias_dict ={15:[6,9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 237.6x237.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 237.6x237.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 237.6x237.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 237.6x237.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 237.6x237.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "path = 'logs' # use your path\n",
    "all_files = glob.glob(path + \"/*.log\")\n",
    "\n",
    "li = []\n",
    "\n",
    "\n",
    "methods = ['Benchmark', 'bias', 'precision', 'recall', 'density', 'coverage', 'dp', 'ftu', 'auc']\n",
    "\n",
    "\n",
    "\n",
    "def read_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False, header=0, names = methods)\n",
    "    df['ftu'] = abs(df['ftu'])\n",
    "    df['dp'] = abs(df['dp'])\n",
    "    df.loc[df.Benchmark == 'fairgan', 'Benchmark'] = 'FairGAN'\n",
    "    df.loc[df.Benchmark == 'DECAF', 'Benchmark'] = 'DECAF-ND'\n",
    "    df = df[df.Benchmark != 'adsgan']\n",
    "    \n",
    "    df = df[df.Benchmark != 'adsgan-pr']\n",
    "    df = df[df.Benchmark != 'gan']\n",
    "    df = df[df.Benchmark != 'gan-pr']\n",
    "    df = df[df.Benchmark != 'wgan']\n",
    "    df = df[df.Benchmark != 'wgan-pr']\n",
    "    df.bias = 1 - df.bias\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_df2(filename):\n",
    "    df = pd.read_csv(filename, index_col=False, header=0, names = methods)\n",
    "    df['ftu'] = abs(df['ftu'])\n",
    "    df['dp'] = df['dp'] - np.min(df['dp'])\n",
    "    df.loc[df.Benchmark == 'fairgan', 'Benchmark'] = 'FairGAN'\n",
    "    df.loc[df.Benchmark == 'DECAF', 'Benchmark'] = 'DECAF-ND'\n",
    "    df = df[df.Benchmark != 'adsgan']\n",
    "    \n",
    "    df = df[df.Benchmark != 'adsgan-pr']\n",
    "    df = df[df.Benchmark != 'gan']\n",
    "    df = df[df.Benchmark != 'gan-pr']\n",
    "    df = df[df.Benchmark != 'wgan']\n",
    "    df = df[df.Benchmark != 'wgan-pr']\n",
    "    df = df[df.Benchmark != 'FairGAN']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_df3(filename):\n",
    "    df = pd.read_csv(filename, index_col=False, header=0, names = methods)\n",
    "    df['ftu'] = abs(df['ftu'])\n",
    "    df['dp'] = abs(df['dp'])\n",
    "    df.loc[df.Benchmark == 'fairgan', 'Benchmark'] = 'FairGAN'\n",
    "    df.loc[df.Benchmark == 'DECAF', 'Benchmark'] = 'DECAF-ND'\n",
    "    df.loc[df.Benchmark == 'DECAF-FTU', 'Benchmark'] = 'DECAF-FTU1'\n",
    "    df = df[df.Benchmark != 'adsgan']\n",
    "    \n",
    "    df = df[df.Benchmark != 'adsgan-pr']\n",
    "    df = df[df.Benchmark != 'gan']\n",
    "    df = df[df.Benchmark != 'gan-pr']\n",
    "    df = df[df.Benchmark != 'wgan']\n",
    "    df = df[df.Benchmark != 'wgan-pr']\n",
    "    df = df[df.Benchmark != 'FairGAN']\n",
    "    df.bias = 1 - df.bias\n",
    "    return df\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "df = read_df('plots_surrogate_confounder_both.csv')\n",
    "dims = (3.3,3.3)\n",
    "plt.figure(figsize = dims)\n",
    "\n",
    "fig = sns.lineplot(data=df, x='bias', y = 'auc', hue = 'Benchmark', err_style = \"bars\")\n",
    "\n",
    "\n",
    "plt.xlabel(r\"Bias $\\beta$\", fontsize=14)\n",
    "plt.ylabel(\"AUROC\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_auroc2.pdf')\n",
    "\n",
    "plt.figure(figsize = dims)\n",
    "sns.lineplot(data=df, x='bias', y = 'dp', hue = 'Benchmark',err_style=\"bars\")\n",
    "plt.xlabel(r\"Bias $\\beta$\", fontsize=14)\n",
    "plt.ylabel(\"DP\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_dp2.pdf')\n",
    "\n",
    "\n",
    "plt.figure(figsize = dims)\n",
    "sns.lineplot(data=df, x='bias', y = 'ftu', hue = 'Benchmark', err_style=\"bars\")\n",
    "plt.xlabel(r\"Bias $\\beta$\", fontsize=14)\n",
    "plt.ylabel(\"FTU\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_ftu2.pdf')\n",
    "\n",
    "plt.figure(figsize = dims)\n",
    "sns.lineplot(data=df, x='bias', y = 'precision', hue = 'Benchmark',err_style=\"bars\")\n",
    "plt.xlabel(r\"Bias $\\beta$\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_prec2.pdf')\n",
    "\n",
    "\n",
    "plt.figure(figsize = dims)\n",
    "sns.lineplot(data=df, x='bias', y = 'recall', hue = 'Benchmark',err_style=\"bars\" )\n",
    "plt.xlabel(r\"Bias $\\beta$\", fontsize=14)\n",
    "plt.ylabel(\"Recall\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_recall2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
