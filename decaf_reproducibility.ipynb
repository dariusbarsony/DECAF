{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "S1t5-cjhja30",
    "outputId": "540e7a09-91a3-47b0-ef80-939dc75e8a5f"
   },
   "outputs": [],
   "source": [
    "#Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pytest\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add files to sys \n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset (adult.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "veyREY-tlrmH"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xdSEw1G4lwzJ"
   },
   "outputs": [],
   "source": [
    "#adult_dir = '../gdrive/MyDrive/adult.data'\n",
    "adult_dir = 'data/adult.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RPBUIw5jcKu",
    "outputId": "9756042c-364f-4525-8f57-801f06541e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL DATAPOINTS AFTER CLEANING: 30162\n"
     ]
    }
   ],
   "source": [
    "names = [\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"label\",\n",
    "    ]\n",
    "df = pd.read_csv(adult_dir, names=names, index_col=False)\n",
    "df = df.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == \"object\":\n",
    "        df = df[df[col] != \"?\"]\n",
    "\n",
    "replace = [\n",
    "    [\n",
    "        \"Private\",\n",
    "        \"Self-emp-not-inc\",\n",
    "        \"Self-emp-inc\",\n",
    "        \"Federal-gov\",\n",
    "        \"Local-gov\",\n",
    "        \"State-gov\",\n",
    "        \"Without-pay\",\n",
    "        \"Never-worked\",\n",
    "    ],\n",
    "    [\n",
    "        \"Bachelors\",\n",
    "        \"Some-college\",\n",
    "        \"11th\",\n",
    "        \"HS-grad\",\n",
    "        \"Prof-school\",\n",
    "        \"Assoc-acdm\",\n",
    "        \"Assoc-voc\",\n",
    "        \"9th\",\n",
    "        \"7th-8th\",\n",
    "        \"12th\",\n",
    "        \"Masters\",\n",
    "        \"1st-4th\",\n",
    "        \"10th\",\n",
    "        \"Doctorate\",\n",
    "        \"5th-6th\",\n",
    "        \"Preschool\",\n",
    "    ],\n",
    "    [\n",
    "        \"Married-civ-spouse\",\n",
    "        \"Divorced\",\n",
    "        \"Never-married\",\n",
    "        \"Separated\",\n",
    "        \"Widowed\",\n",
    "        \"Married-spouse-absent\",\n",
    "        \"Married-AF-spouse\",\n",
    "    ],\n",
    "    [\n",
    "        \"Tech-support\",\n",
    "        \"Craft-repair\",\n",
    "        \"Other-service\",\n",
    "        \"Sales\",\n",
    "        \"Exec-managerial\",\n",
    "        \"Prof-specialty\",\n",
    "        \"Handlers-cleaners\",\n",
    "        \"Machine-op-inspct\",\n",
    "        \"Adm-clerical\",\n",
    "        \"Farming-fishing\",\n",
    "        \"Transport-moving\",\n",
    "        \"Priv-house-serv\",\n",
    "        \"Protective-serv\",\n",
    "        \"Armed-Forces\",\n",
    "    ],\n",
    "    [\n",
    "        \"Wife\",\n",
    "        \"Own-child\",\n",
    "        \"Husband\",\n",
    "        \"Not-in-family\",\n",
    "        \"Other-relative\",\n",
    "        \"Unmarried\",\n",
    "    ],\n",
    "    [\"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"],\n",
    "    [\"Female\", \"Male\"],\n",
    "    [\n",
    "        \"United-States\",\n",
    "        \"Cambodia\",\n",
    "        \"England\",\n",
    "        \"Puerto-Rico\",\n",
    "        \"Canada\",\n",
    "        \"Germany\",\n",
    "        \"Outlying-US(Guam-USVI-etc)\",\n",
    "        \"India\",\n",
    "        \"Japan\",\n",
    "        \"Greece\",\n",
    "        \"South\",\n",
    "        \"China\",\n",
    "        \"Cuba\",\n",
    "        \"Iran\",\n",
    "        \"Honduras\",\n",
    "        \"Philippines\",\n",
    "        \"Italy\",\n",
    "        \"Poland\",\n",
    "        \"Jamaica\",\n",
    "        \"Vietnam\",\n",
    "        \"Mexico\",\n",
    "        \"Portugal\",\n",
    "        \"Ireland\",\n",
    "        \"France\",\n",
    "        \"Dominican-Republic\",\n",
    "        \"Laos\",\n",
    "        \"Ecuador\",\n",
    "        \"Taiwan\",\n",
    "        \"Haiti\",\n",
    "        \"Columbia\",\n",
    "        \"Hungary\",\n",
    "        \"Guatemala\",\n",
    "        \"Nicaragua\",\n",
    "        \"Scotland\",\n",
    "        \"Thailand\",\n",
    "        \"Yugoslavia\",\n",
    "        \"El-Salvador\",\n",
    "        \"Trinadad&Tobago\",\n",
    "        \"Peru\",\n",
    "        \"Hong\",\n",
    "        \"Holand-Netherlands\",\n",
    "    ],\n",
    "    [\">50K\", \"<=50K\"],\n",
    "]\n",
    "\n",
    "for row in replace:\n",
    "    df = df.replace(row, range(len(row)))\n",
    "\n",
    "index = df.index\n",
    "print('TOTAL DATAPOINTS AFTER CLEANING:',len(index))\n",
    "\n",
    "# Split the data into train,test\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "X_train = train.loc[:, train.columns != 'label']\n",
    "y_train = train['label']\n",
    "X_test = test.loc[:, test.columns != 'label']\n",
    "y_test = test['label']\n",
    "\n",
    "df = df.values\n",
    "X = df[:, :14].astype(np.uint32)\n",
    "y = df[:, 14].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECAF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decaf.DECAF import DECAF\n",
    "from decaf.data import DataModule\n",
    "from tests.utils import gen_data_nonlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baseline(size: int = 100) -> Tuple[torch.Tensor, DataModule, list, dict]:\n",
    "    # causal structure is in dag_seed\n",
    "    dag_seed = [\n",
    "        [1, 2],\n",
    "        [1, 3],\n",
    "        [1, 4],\n",
    "        [2, 5],\n",
    "        [2, 0],\n",
    "        [3, 0],\n",
    "        [3, 6],\n",
    "        [3, 7],\n",
    "        [6, 9],\n",
    "        [0, 8],\n",
    "        [0, 9],\n",
    "    ]\n",
    "    # edge removal dictionary\n",
    "    bias_dict = {6: [3]}  # This removes the edge into 6 from 3.\n",
    "\n",
    "    # DATA SETUP according to dag_seed\n",
    "    G = nx.DiGraph(dag_seed)\n",
    "    data = gen_data_nonlinear(G, SIZE=size)\n",
    "    dm = DataModule(data.values)\n",
    "\n",
    "    return torch.Tensor(np.asarray(data)), dm, dag_seed, bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "def test_sanity_params() -> None:\n",
    "    _, dummy_dm, seed, _ = generate_baseline()\n",
    "\n",
    "    model = DECAF(\n",
    "        dummy_dm.dims[0],\n",
    "        dag_seed=seed,\n",
    "    )\n",
    "\n",
    "    assert model.generator is not None\n",
    "    assert model.discriminator is not None\n",
    "    assert model.x_dim == dummy_dm.dims[0]\n",
    "    assert model.z_dim == dummy_dm.dims[0]\n",
    "    print('pass')\n",
    "\n",
    "test_sanity_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\space\\anaconda3\\envs\\factai\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:99: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\n",
      "  rank_zero_warn(f\"you passed in a {loader_name} but have no {step_name}. Skipping {stage} loop\")\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | generator     | Generator_causal | 106 K \n",
      "1 | discriminator | Discriminator    | 42.6 K\n",
      "---------------------------------------------------\n",
      "149 K     Trainable params\n",
      "0         Non-trainable params\n",
      "149 K     Total params\n",
      "0.596     Total estimated model params size (MB)\n",
      "C:\\Users\\space\\anaconda3\\envs\\factai\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0fd10e3f584becac82f1edeb21ebe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "def test_sanity_train() -> None:\n",
    "    _, dummy_dm, seed, _ = generate_baseline()\n",
    "\n",
    "    model = DECAF(\n",
    "        dummy_dm.dims[0],\n",
    "        dag_seed=seed,\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=2, logger=False)\n",
    "\n",
    "    trainer.fit(model, dummy_dm)\n",
    "    print('pass')\n",
    "\n",
    "test_sanity_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | generator     | Generator_causal | 106 K \n",
      "1 | discriminator | Discriminator    | 42.6 K\n",
      "---------------------------------------------------\n",
      "149 K     Trainable params\n",
      "0         Non-trainable params\n",
      "149 K     Total params\n",
      "0.596     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd956499f2cb464fb02f20f67e5b6b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "def test_sanity_generate() -> None:\n",
    "    raw_data, dummy_dm, seed, bias_dict = generate_baseline(size=10)\n",
    "\n",
    "    model = DECAF(\n",
    "        dummy_dm.dims[0],\n",
    "        dag_seed=seed,\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=2, logger=False)\n",
    "\n",
    "    trainer.fit(model, dummy_dm)\n",
    "\n",
    "    synth_data = (\n",
    "        model.gen_synthetic(\n",
    "            raw_data, gen_order=model.get_gen_order(), biased_edges=bias_dict\n",
    "        )\n",
    "            .detach()\n",
    "            .numpy()\n",
    "    )\n",
    "    assert synth_data.shape[0] == 10\n",
    "    print('pass')\n",
    "\n",
    "test_sanity_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\space\\anaconda3\\envs\\factai\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\space\\anaconda3\\envs\\factai\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:99: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\n",
      "  rank_zero_warn(f\"you passed in a {loader_name} but have no {step_name}. Skipping {stage} loop\")\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | generator     | Generator_causal | 128 K \n",
      "1 | discriminator | Discriminator    | 43.4 K\n",
      "---------------------------------------------------\n",
      "171 K     Trainable params\n",
      "196       Non-trainable params\n",
      "171 K     Total params\n",
      "0.686     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline scores 0.8882623224728488 0.9386863247108679 0.7911998485568191\n",
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\space\\anaconda3\\envs\\factai\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c063b7dd2e84021b4e1e07d8d2fc04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10947564 -0.50399894  0.85820274 ... -0.21858598  3.2612238\n",
      "  -0.26867354]\n",
      " [-0.64242537  2.91624496 -0.45902768 ... -0.21858598 -0.07773411\n",
      "   0.97224522]\n",
      " [-1.1753751  -0.50399894 -0.63910132 ... -0.21858598 -0.91247359\n",
      "  -0.26867354]\n",
      " ...\n",
      " [ 1.48937355 -0.50399894 -0.3585745  ... -0.21858598 -0.07773411\n",
      "  -0.26867354]\n",
      " [-1.25151078 -0.50399894  0.11070545 ... -0.21858598 -1.74721307\n",
      "  -0.26867354]\n",
      " [ 1.0325595   0.86409862  0.92884082 ... -0.21858598 -0.07773411\n",
      "  -0.26867354]]\n",
      "[[8.9023031e-22 2.2188302e-11 9.9923933e-01 ... 7.9098110e-11\n",
      "  9.9997890e-01 5.5946163e-09]\n",
      " [9.9999321e-01 1.4558826e-11 1.1332482e-22 ... 1.8286881e-09\n",
      "  0.0000000e+00 9.9206576e-10]\n",
      " [1.4122944e-21 1.2257043e-12 2.9353224e-36 ... 2.4023030e-09\n",
      "  6.7028543e-17 5.0735791e-11]\n",
      " ...\n",
      " [9.9433451e-34 5.3437502e-12 2.9736013e-25 ... 1.2563774e-09\n",
      "  9.9998760e-01 4.1445078e-10]\n",
      " [1.8389986e-25 1.0675898e-10 2.2447413e-01 ... 2.3216279e-10\n",
      "  9.9979824e-01 5.8968852e-10]\n",
      " [2.8678443e-18 2.1347746e-09 0.0000000e+00 ... 7.1072543e-09\n",
      "  9.9979597e-01 1.9887474e-09]]\n",
      "[1 1 1 ... 1 1 1]\n",
      "y_synth unique? [False False]\n",
      "synth scores 0.9153350401695065 0.7520127656487996 0.5054499686049253\n"
     ]
    }
   ],
   "source": [
    "def test_run_experiments(X: pd.DataFrame, y: pd.DataFrame) -> None:\n",
    "    \"\"\"Normalize X\"\"\"\n",
    "    X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "    baseline_clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
    "                                 learning_rate='constant', learning_rate_init=0.001)\n",
    "    baseline_clf.fit(X_normalized, y)\n",
    "\n",
    "    y_pred = baseline_clf.predict(X_normalized)\n",
    "\n",
    "    print(\n",
    "        \"baseline scores\",\n",
    "        precision_score(y, y_pred),\n",
    "        recall_score(y, y_pred),\n",
    "        roc_auc_score(y, y_pred),\n",
    "    )\n",
    "\n",
    "    dm = DataModule(X_normalized)\n",
    "\n",
    "    # causal structure is in dag_seed\n",
    "    dag_seed = [\n",
    "        [0, 6],\n",
    "        [0, 12],\n",
    "        [0, 1],\n",
    "        [0, 5],\n",
    "        [0, 3],\n",
    "        [3, 6],\n",
    "        [3, 12],\n",
    "        [3, 1],\n",
    "        [3, 7],\n",
    "        [5, 6],\n",
    "        [5, 12],\n",
    "        [5, 1],\n",
    "        [5, 7],\n",
    "        [5, 3],\n",
    "        [8, 6],\n",
    "        [8, 12],\n",
    "        [8, 3],\n",
    "        [8, 5],\n",
    "        [9, 6],\n",
    "        [9, 5],\n",
    "        [9, 12],\n",
    "        [9, 1],\n",
    "        [9, 3],\n",
    "        [9, 7],\n",
    "        [13, 5],\n",
    "        [13, 12],\n",
    "        [13, 3],\n",
    "        [13, 1],\n",
    "        [13, 7],\n",
    "    ]\n",
    "    # edge removal dictionary\n",
    "    bias_dict = {}\n",
    "\n",
    "    # bias_dict = {6: [9],\n",
    "    #              5: [9],\n",
    "    #              12: [9],\n",
    "    #              1: [9],\n",
    "    #              3: [9],\n",
    "    #              7: [9],\n",
    "    #              }\n",
    "\n",
    "    model = DECAF(\n",
    "        dm.dims[0],\n",
    "        dag_seed=dag_seed,\n",
    "        use_mask=True,\n",
    "        grad_dag_loss=False,\n",
    "        lambda_privacy=0,\n",
    "        lambda_gp=10,\n",
    "        weight_decay=1e-2,\n",
    "        l1_g=0,\n",
    "        p_gen=-1,\n",
    "        batch_size=100,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=10, logger=False)\n",
    "\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    X_synth = (\n",
    "        model.gen_synthetic(\n",
    "            dm.dataset.x,\n",
    "            gen_order=model.get_gen_order(), biased_edges=bias_dict\n",
    "        )\n",
    "            .detach()\n",
    "            .numpy()\n",
    "    )\n",
    "\n",
    "    y_synth = baseline_clf.predict(X_synth)\n",
    "\n",
    "    print(y_synth)\n",
    "    print('y_synth unique?', np.unique(y_synth) > 1)\n",
    "\n",
    "    synth_clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
    "                              learning_rate='constant', learning_rate_init=0.001)\n",
    "    synth_clf.fit(X_synth, y_synth)\n",
    "    y_pred = synth_clf.predict(X_synth)\n",
    "\n",
    "    print(\n",
    "        \"synth scores\",\n",
    "        precision_score(y_synth, y),\n",
    "        recall_score(y_synth, y),\n",
    "        roc_auc_score(y_synth, y),\n",
    "    )\n",
    "\n",
    "test_run_experiments(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark results for comparison - CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "QOa1glz3wCe-",
    "outputId": "696b2f1b-b3e2-48d9-9f58-cf11be291cb9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ctgan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b270760ab295>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtable_evaluator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTableEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mctgan\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCTGANSynthesizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ctgan'"
     ]
    }
   ],
   "source": [
    "from table_evaluator import TableEvaluator\n",
    "from ctgan import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAMirvKwmm3H",
    "outputId": "44473c3e-aa10-4c04-cfcf-f41473c00a7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discrete_columns = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "dfc = df.copy()\n",
    "\n",
    "#ctgan = CTGANSynthesizer(epochs=10,verbose=True)\n",
    "#ctgan.fit(dfc, discrete_columns)\n",
    "\n",
    "# Generate the exact same amount\n",
    "#df1 = ctgan.sample(len(dfc.index))\n",
    "\n",
    "# fig, ax = plt.subplots(5, 3, figsize = (30, 20))\n",
    "# fig.tight_layout(pad = 2.0)\n",
    "\n",
    "# ax[0,0].hist(df1['age'])\n",
    "# ax[0,0].set_title('age')\n",
    "# ax[0,1].hist(df1['workclass'])\n",
    "# ax[0,1].set_title('workclass')\n",
    "# ax[0,2].hist(df1['fnlwgt'])\n",
    "# ax[0,2].set_title('fnlwgt')\n",
    "\n",
    "# ax[1,0].hist(df1['education'])\n",
    "# ax[1,0].set_title('education')\n",
    "# ax[1,1].hist(df1['education-num'])\n",
    "# ax[1,1].set_title('education-num')\n",
    "# ax[1,2].hist(df1['marital-status'])\n",
    "# ax[1,2].set_title('marital-status')\n",
    "\n",
    "# ax[2,0].hist(df1['occupation'])\n",
    "# ax[2,0].set_title('occupation')\n",
    "# ax[2,1].hist(df1['relationship'])\n",
    "# ax[2,1].set_title('relationship')\n",
    "# ax[2,2].hist(df1['race'])\n",
    "# ax[2,2].set_title('race')\n",
    "\n",
    "# ax[3,0].hist(df1['sex'])\n",
    "# ax[3,0].set_title('sex')\n",
    "# ax[3,1].hist(df1['capital-gain'])\n",
    "# ax[3,1].set_title('capital-gain')\n",
    "# ax[3,2].hist(df1['capital-loss'])\n",
    "# ax[3,2].set_title('capital-loss')\n",
    "\n",
    "# ax[4,0].hist(df1['hours-per-week'])\n",
    "# ax[4,0].set_title('hours-per-week')\n",
    "# ax[4,1].hist(df1['native-country'])\n",
    "# ax[4,1].set_title('native-country')\n",
    "# ax[4,2].hist(df1['label'])\n",
    "# ax[4,2].set_title('label')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j987Rrbs9Lb"
   },
   "outputs": [],
   "source": [
    "# indexc = dfc.index\n",
    "# print('TOTAL DATAPOINTS AFTER CLEANING:',len(indexc))\n",
    "# index1 = df1.index\n",
    "# print('TOTAL DATAPOINTS AFTER CLEANING:',len(index1))\n",
    "# table_evaluator = TableEvaluator(dfc,df1)\n",
    "# table_evaluator.visual_evaluation() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_columns = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a dataframe to store the synthetic data\n",
    "df = df[['race','age','sex','native-country','marital-status','education','occupation','hours-per-week','workclass','relationship','label']]\n",
    "\n",
    "# Node order contains the order in which to generate the data, starting with the root nodes\n",
    "node_order = [['race','age','sex','native-country'],['marital-status'],['education'],['occupation','hours-per-week','workclass','relationship'],['label']]\n",
    "node_order_nl = ['race','age','sex','native-country','marital-status','education','occupation','hours-per-week','workclass','relationship','label']\n",
    "\n",
    "# List of connections; key is receiving node\n",
    "node_connections_normal = {'label':['occupation','race','hours-per-week','age','marital-status','education','sex','workclass','native-country','relatinship'],\n",
    "                    'occupation':['race','age','sex','marital-status','education'],\n",
    "                    'hours-per-week':['race','age','marital-status','native-country','education','sex'],\n",
    "                    'workclass':['age','marital-status','sex','education','native-country'],\n",
    "                    'relationship':['marital-status','education','age','sex','native-country'],\n",
    "                    'education':['race','age','marital-status','sex','native-country'],\n",
    "                    'marital-status':['race','age','sex','native-country']\n",
    "                    }\n",
    "\n",
    "'''\n",
    "Connections are removed according to the privacy criterion\n",
    "'''\n",
    "node_connections_FTU = {'label':['occupation','race','hours-per-week','age','marital-status','education','workclass','native-country','relationship'],\n",
    "                    'occupation':['race','age','sex','marital-status','education'],\n",
    "                    'hours-per-week':['race','age','marital-status','native-country','education','sex'],\n",
    "                    'workclass':['age','marital-status','sex','education','native-country'],\n",
    "                    'relationship':['marital-status','education','age','sex','native-country'],\n",
    "                    'education':['race','age','marital-status','sex','native-country'],\n",
    "                    'marital-status':['race','age','sex','native-country']\n",
    "                    }\n",
    "\n",
    "node_connections_DP = {'label':['race','age','native-country'],\n",
    "                    'occupation':['race','age','sex','marital-status','education'],\n",
    "                    'hours-per-week':['race','age','marital-status','native-country','education','sex'],\n",
    "                    'workclass':['age','marital-status','sex','education','native-country'],\n",
    "                    'relationship':['marital-status','education','age','sex','native-country'],\n",
    "                    'education':['race','age','marital-status','sex','native-country'],\n",
    "                    'marital-status':['race','age','sex','native-country']\n",
    "                    }\n",
    "\n",
    "node_connections_CF = {'label':['occupation','race','hours-per-week','age','education','workclass','native-country',],\n",
    "                    'occupation':['race','age','sex','marital-status','education'],\n",
    "                    'hours-per-week':['race','age','marital-status','native-country','education','sex'],\n",
    "                    'workclass':['age','marital-status','sex','education','native-country'],\n",
    "                    'relationship':['marital-status','education','age','sex','native-country'],\n",
    "                    'education':['race','age','marital-status','sex','native-country'],\n",
    "                    'marital-status':['race','age','sex','native-country']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "gvyTSr449x0G",
    "outputId": "ddf5fd93-9ce1-4b69-c6a2-55ba59bd0ed6"
   },
   "outputs": [],
   "source": [
    "ctgan = CTGANSynthesizer(epochs=10)\n",
    "def generate_data(mode):\n",
    "    \n",
    "    # Define the privacy measure\n",
    "    if mode == 'FTU':\n",
    "        node_connections = node_connections_FTU\n",
    "    elif mode == 'DP':\n",
    "        node_connections = node_connections_DP\n",
    "    elif mode == 'CF':\n",
    "        node_connections = node_connections_CF\n",
    "    else:\n",
    "        print('Mode is not correct!')\n",
    "        \n",
    "    # DF to fit the first model on\n",
    "    start_df = df[['race','age','sex','native-country']]\n",
    "    \n",
    "    # Generate the initial nodes\n",
    "    temp_discrete = ['race','age','sex','native-country']\n",
    "    ctgan.fit(start_df, temp_discrete)\n",
    "    synth_df = ctgan.sample(len(start_df.index))\n",
    "    print('Done generating the root nodes.')\n",
    "    \n",
    "    \n",
    "    # Iteratively generate the data\n",
    "    for node in node_order_nl:\n",
    "        \n",
    "        # If the node has not been generated yet\n",
    "        if node not in synth_df.columns:\n",
    "            \n",
    "            # Grab the old data\n",
    "            empty_df = df[[node]]\n",
    "\n",
    "            # Grab the attributes that need to be looked at when generating data\n",
    "            attributes = node_connections[node]\n",
    "            \n",
    "            # Grab the attributes from the final df\n",
    "            gen_df = synth_df.loc[:,synth_df.columns.isin(attributes)]\n",
    "        \n",
    "            # Add the old attribute to the current dataframe\n",
    "            at = df[attributes]\n",
    "            empty_df = empty_df.join(at)\n",
    "            \n",
    "            temp_discrete = []\n",
    "            for d in discrete_columns:\n",
    "                if d in gen_df.columns:\n",
    "                    temp_discrete.append(d)\n",
    "                    \n",
    "            print('Started training node',node)\n",
    "            ctgan.fit(empty_df, temp_discrete)\n",
    "            generated_data = ctgan.sample(len(synth_df.index))\n",
    "            \n",
    "            # Check if synth_df needs the current attribute (shouldn't, but just to be sure)\n",
    "            for attribute in attributes:\n",
    "                if attribute not in synth_df.columns:\n",
    "                    synth_df[attribute] = generated_data[attribute].values\n",
    "            print('Finished training node',node)\n",
    "    \n",
    "    synth_df = synth_df.join(generated_data[['label']])\n",
    "    return synth_df\n",
    "\n",
    "synthetic = generate_data('FTU')\n",
    "print(synthetic.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FACT GAN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
